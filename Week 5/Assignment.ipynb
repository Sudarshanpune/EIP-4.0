{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVqJaz4q79mx",
        "colab_type": "text"
      },
      "source": [
        "1. ResNet50 @ 100epoc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "9e9f6910-7ff0-42d5-9a14-21c51d46228b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWQ8DU0OavsB",
        "colab_type": "code",
        "outputId": "1230b412-be08-4488-d7a4-cb241f65bccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05c215fa-c655-44cf-f9eb-a18442cebf29"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16,InceptionResNetV2,ResNet50\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "4413d07f-3801-4453-eca9-3ecff8fd79e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "31bb3d70-4a07-4eec-99ed-9fd7481d9cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "a5bc5096-c2b2-42e2-8e55-10000d646203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15,random_state=1)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "a0070bf5-4e2d-4b1e-df05-537100d3481b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resized/59.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>resized/2107.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5206</th>\n",
              "      <td>resized/5207.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>resized/1164.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>resized/13536.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "58        resized/59.jpg              0  ...                        1              0\n",
              "2106    resized/2107.jpg              1  ...                        1              0\n",
              "5206    resized/5207.jpg              0  ...                        1              0\n",
              "1163    resized/1164.jpg              1  ...                        1              0\n",
              "13534  resized/13536.jpg              1  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=ImageDataGenerator(\n",
        "                                                                           horizontal_flip=False, \n",
        "                                                                           vertical_flip=True\n",
        "                                                                           ))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "46c3fa0b-4f93-4184-d527-f427e4059977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "f6802cc6-902c-4ed6-8a8c-10cc5780c02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "backbone = ResNet50(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 4s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "3c1b07dc-5321-4db6-912a-b7dda3fa9ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.fit(train_df, validation_data=(val_df), batch_size=64, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nv8fdaSbkVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency.\n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored\n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or\n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "\n",
        "    # Example for CIFAR-10 w/ batch size 100:\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    # References\n",
        "\n",
        "      - [Cyclical Learning Rates for Training Neural Networks](\n",
        "      https://arxiv.org/abs/1506.01186)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            base_lr=0.001,\n",
        "            max_lr=0.006,\n",
        "            step_size=2000.,\n",
        "            mode='triangular',\n",
        "            gamma=1.,\n",
        "            scale_fn=None,\n",
        "            scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2',\n",
        "                        'exp_range']:\n",
        "            raise KeyError(\"mode must be one of 'triangular', \"\n",
        "                           \"'triangular2', or 'exp_range'\")\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** x\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr is not None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr is not None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size is not None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "        self.history.setdefault(\n",
        "            'lr', []).append(\n",
        "            K.get_value(\n",
        "                self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EveN99bbmCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_LR=1e-5\n",
        "MAX_LR=1e-2\n",
        "\n",
        "STEP_SIZE=8\n",
        "CLR_METHOD='triangular'\n",
        "NUM_EPOCHS=96\n",
        "\n",
        "\n",
        "clr=CyclicLR(\n",
        "            base_lr=MIN_LR,\n",
        "            max_lr=MAX_LR,\n",
        "            step_size=STEP_SIZE * (train_df.shape[0] // 32),\n",
        "            mode=CLR_METHOD\n",
        ")\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtbJYVhL4QZp",
        "colab_type": "code",
        "outputId": "4de68bf0-6abf-4282-9497-fe0a067ba83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = '/content/drive/My Drive/m.h5'\n",
        "ckpt =ModelCheckpoint(filepath, verbose=2, monitor='val_loss', save_best_only=True,save_weights_only=False, mode='auto')\n",
        "callback = [ckpt,clr,lr_reducer]\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=callback\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 8.3544 - gender_output_loss: 0.6226 - image_quality_output_loss: 1.0869 - age_output_loss: 1.5658 - weight_output_loss: 1.1154 - bag_output_loss: 0.9698 - footwear_output_loss: 1.0098 - pose_output_loss: 0.8840 - emotion_output_loss: 1.1001 - gender_output_acc: 0.6774 - image_quality_output_acc: 0.5041 - age_output_acc: 0.3535 - weight_output_acc: 0.5927 - bag_output_acc: 0.5497 - footwear_output_acc: 0.5447 - pose_output_acc: 0.6341 - emotion_output_acc: 0.6681 - val_loss: 7.5516 - val_gender_output_loss: 0.4925 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4528 - val_weight_output_loss: 1.0215 - val_bag_output_loss: 0.9206 - val_footwear_output_loss: 0.8515 - val_pose_output_loss: 0.8127 - val_emotion_output_loss: 1.0281 - val_gender_output_acc: 0.7495 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3589 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.6522 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.55163, saving model to /content/drive/My Drive/bestmodel5.h5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 7.6479 - gender_output_loss: 0.5570 - image_quality_output_loss: 0.9975 - age_output_loss: 1.4698 - weight_output_loss: 1.0423 - bag_output_loss: 0.9227 - footwear_output_loss: 0.9123 - pose_output_loss: 0.7899 - emotion_output_loss: 0.9564 - gender_output_acc: 0.7078 - image_quality_output_acc: 0.5309 - age_output_acc: 0.3777 - weight_output_acc: 0.6247 - bag_output_acc: 0.5764 - footwear_output_acc: 0.5861 - pose_output_acc: 0.6801 - emotion_output_acc: 0.7080 - val_loss: 7.7927 - val_gender_output_loss: 0.5948 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.4498 - val_weight_output_loss: 1.0208 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9404 - val_pose_output_loss: 0.8610 - val_emotion_output_loss: 1.0042 - val_gender_output_acc: 0.6784 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5706 - val_pose_output_acc: 0.6492 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 7.55163\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.5543 - gender_output_loss: 0.5580 - image_quality_output_loss: 0.9801 - age_output_loss: 1.4579 - weight_output_loss: 1.0171 - bag_output_loss: 0.9129 - footwear_output_loss: 0.9245 - pose_output_loss: 0.7899 - emotion_output_loss: 0.9139 - gender_output_acc: 0.6978 - image_quality_output_acc: 0.5485 - age_output_acc: 0.3901 - weight_output_acc: 0.6285 - bag_output_acc: 0.5794 - footwear_output_acc: 0.5827 - pose_output_acc: 0.6806 - emotion_output_acc: 0.7095 - val_loss: 7.6832 - val_gender_output_loss: 0.6426 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4573 - val_weight_output_loss: 1.0002 - val_bag_output_loss: 0.9325 - val_footwear_output_loss: 0.9426 - val_pose_output_loss: 0.7664 - val_emotion_output_loss: 0.9587 - val_gender_output_acc: 0.6245 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3513 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.5549 - val_footwear_output_acc: 0.5801 - val_pose_output_acc: 0.6920 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 7.55163\n",
            "Epoch 4/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.4319 - gender_output_loss: 0.5589 - image_quality_output_loss: 0.9736 - age_output_loss: 1.4333 - weight_output_loss: 0.9920 - bag_output_loss: 0.8915 - footwear_output_loss: 0.9075 - pose_output_loss: 0.7731 - emotion_output_loss: 0.9021 - gender_output_acc: 0.6848 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3971 - weight_output_acc: 0.6302 - bag_output_acc: 0.5873 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6862 - emotion_output_acc: 0.7102\n",
            "Epoch 00003: val_loss did not improve from 7.55163\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.4322 - gender_output_loss: 0.5592 - image_quality_output_loss: 0.9733 - age_output_loss: 1.4333 - weight_output_loss: 0.9919 - bag_output_loss: 0.8916 - footwear_output_loss: 0.9075 - pose_output_loss: 0.7741 - emotion_output_loss: 0.9013 - gender_output_acc: 0.6845 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3972 - weight_output_acc: 0.6300 - bag_output_acc: 0.5870 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6859 - emotion_output_acc: 0.7105 - val_loss: 7.9999 - val_gender_output_loss: 0.6269 - val_image_quality_output_loss: 1.0340 - val_age_output_loss: 1.4906 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 1.0180 - val_footwear_output_loss: 0.9746 - val_pose_output_loss: 0.8764 - val_emotion_output_loss: 0.9894 - val_gender_output_acc: 0.7006 - val_image_quality_output_acc: 0.5348 - val_age_output_acc: 0.3624 - val_weight_output_acc: 0.6341 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.5685 - val_pose_output_acc: 0.6361 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.55163\n",
            "Epoch 5/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.3772 - gender_output_loss: 0.5402 - image_quality_output_loss: 0.9630 - age_output_loss: 1.4283 - weight_output_loss: 0.9951 - bag_output_loss: 0.8880 - footwear_output_loss: 0.9104 - pose_output_loss: 0.7641 - emotion_output_loss: 0.8881 - gender_output_acc: 0.7027 - image_quality_output_acc: 0.5601 - age_output_acc: 0.4013 - weight_output_acc: 0.6307 - bag_output_acc: 0.5912 - footwear_output_acc: 0.5845 - pose_output_acc: 0.6950 - emotion_output_acc: 0.7122\n",
            "Epoch 00004: val_loss did not improve from 7.55163\n",
            "\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 7.3783 - gender_output_loss: 0.5397 - image_quality_output_loss: 0.9628 - age_output_loss: 1.4286 - weight_output_loss: 0.9955 - bag_output_loss: 0.8880 - footwear_output_loss: 0.9104 - pose_output_loss: 0.7647 - emotion_output_loss: 0.8886 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.5602 - age_output_acc: 0.4010 - weight_output_acc: 0.6306 - bag_output_acc: 0.5912 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6948 - emotion_output_acc: 0.7121 - val_loss: 7.6646 - val_gender_output_loss: 0.6069 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.4303 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.9389 - val_footwear_output_loss: 0.9520 - val_pose_output_loss: 0.8031 - val_emotion_output_loss: 0.9702 - val_gender_output_acc: 0.750 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5640 - val_footwear_output_acc: 0.5358 - val_pose_output_acc: 0.6583 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.55163\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.2137 - gender_output_loss: 0.4982 - image_quality_output_loss: 0.9623 - age_output_loss: 1.4167 - weight_output_loss: 0.9889 - bag_output_loss: 0.8750 - footwear_output_loss: 0.8930 - pose_output_loss: 0.7054 - emotion_output_loss: 0.8742 - gender_output_acc: 0.7480 - image_quality_output_acc: 0.5608 - age_output_acc: 0.4054 - weight_output_acc: 0.6346 - bag_output_acc: 0.5990 - footwear_output_acc: 0.5884 - pose_output_acc: 0.7180 - emotion_output_acc: 0.7161 - val_loss: 7.5165 - val_gender_output_loss: 0.5594 - val_image_quality_output_loss: 0.9809 - val_age_output_loss: 1.4218 - val_weight_output_loss: 0.9997 - val_bag_output_loss: 0.9201 - val_footwear_output_loss: 0.8980 - val_pose_output_loss: 0.7876 - val_emotion_output_loss: 0.9489 - val_gender_output_acc: 0.7248 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5640 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6638 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.55163 to 7.51648, saving model to /content/drive/My Drive/bestmodel5.h5\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.0740 - gender_output_loss: 0.4637 - image_quality_output_loss: 0.9660 - age_output_loss: 1.4042 - weight_output_loss: 0.9774 - bag_output_loss: 0.8703 - footwear_output_loss: 0.8670 - pose_output_loss: 0.6601 - emotion_output_loss: 0.8652 - gender_output_acc: 0.7653 - image_quality_output_acc: 0.5641 - age_output_acc: 0.4076 - weight_output_acc: 0.6326 - bag_output_acc: 0.6062 - footwear_output_acc: 0.6011 - pose_output_acc: 0.7398 - emotion_output_acc: 0.7154 - val_loss: 7.7324 - val_gender_output_loss: 0.7321 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9320 - val_footwear_output_loss: 0.8910 - val_pose_output_loss: 0.8319 - val_emotion_output_loss: 0.9509 - val_gender_output_acc: 0.6925 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.6033 - val_pose_output_acc: 0.6487 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 7.51648\n",
            "Epoch 8/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.0522 - gender_output_loss: 0.4626 - image_quality_output_loss: 0.9629 - age_output_loss: 1.4020 - weight_output_loss: 0.9758 - bag_output_loss: 0.8785 - footwear_output_loss: 0.8751 - pose_output_loss: 0.6254 - emotion_output_loss: 0.8700 - gender_output_acc: 0.7770 - image_quality_output_acc: 0.5697 - age_output_acc: 0.4075 - weight_output_acc: 0.6349 - bag_output_acc: 0.6063 - footwear_output_acc: 0.6029 - pose_output_acc: 0.7505 - emotion_output_acc: 0.7143\n",
            "Epoch 00007: val_loss did not improve from 7.51648\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 7.0511 - gender_output_loss: 0.4623 - image_quality_output_loss: 0.9632 - age_output_loss: 1.4016 - weight_output_loss: 0.9756 - bag_output_loss: 0.8784 - footwear_output_loss: 0.8753 - pose_output_loss: 0.6249 - emotion_output_loss: 0.8699 - gender_output_acc: 0.7773 - image_quality_output_acc: 0.5694 - age_output_acc: 0.4077 - weight_output_acc: 0.6350 - bag_output_acc: 0.6062 - footwear_output_acc: 0.6027 - pose_output_acc: 0.7506 - emotion_output_acc: 0.7143 - val_loss: 7.5884 - val_gender_output_loss: 0.5460 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4276 - val_weight_output_loss: 0.9939 - val_bag_output_loss: 0.9174 - val_footwear_output_loss: 0.9400 - val_pose_output_loss: 0.8326 - val_emotion_output_loss: 0.9531 - val_gender_output_acc: 0.7475 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3816 - val_weight_output_acc: 0.6351 - val_bag_output_acc: 0.5640 - val_footwear_output_acc: 0.5721 - val_pose_output_acc: 0.6825 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.51648\n",
            "Epoch 9/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.9750 - gender_output_loss: 0.4406 - image_quality_output_loss: 0.9667 - age_output_loss: 1.3994 - weight_output_loss: 0.9791 - bag_output_loss: 0.8667 - footwear_output_loss: 0.8454 - pose_output_loss: 0.6069 - emotion_output_loss: 0.8703 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4126 - weight_output_acc: 0.6361 - bag_output_acc: 0.6080 - footwear_output_acc: 0.6156 - pose_output_acc: 0.7709 - emotion_output_acc: 0.7154Epoch 9/50\n",
            "\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 6.9729 - gender_output_loss: 0.4402 - image_quality_output_loss: 0.9669 - age_output_loss: 1.3991 - weight_output_loss: 0.9784 - bag_output_loss: 0.8663 - footwear_output_loss: 0.8454 - pose_output_loss: 0.6067 - emotion_output_loss: 0.8699 - gender_output_acc: 0.7816 - image_quality_output_acc: 0.5683 - age_output_acc: 0.4126 - weight_output_acc: 0.6365 - bag_output_acc: 0.6078 - footwear_output_acc: 0.6156 - pose_output_acc: 0.7707 - emotion_output_acc: 0.7155 - val_loss: 7.4727 - val_gender_output_loss: 0.5227 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.4346 - val_weight_output_loss: 0.9903 - val_bag_output_loss: 0.9429 - val_footwear_output_loss: 0.8699 - val_pose_output_loss: 0.7868 - val_emotion_output_loss: 0.9440 - val_gender_output_acc: 0.7697 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.3609 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5489 - val_footwear_output_acc: 0.6331 - val_pose_output_acc: 0.6930 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.51648 to 7.47272, saving model to /content/drive/My Drive/bestmodel5.h5\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 6.8220 - gender_output_loss: 0.4071 - image_quality_output_loss: 0.9450 - age_output_loss: 1.4050 - weight_output_loss: 0.9692 - bag_output_loss: 0.8556 - footwear_output_loss: 0.8198 - pose_output_loss: 0.5583 - emotion_output_loss: 0.8620 - gender_output_acc: 0.8106 - image_quality_output_acc: 0.5730 - age_output_acc: 0.4095 - weight_output_acc: 0.6391 - bag_output_acc: 0.6070 - footwear_output_acc: 0.6365 - pose_output_acc: 0.7907 - emotion_output_acc: 0.7161 - val_loss: 7.7864 - val_gender_output_loss: 0.7051 - val_image_quality_output_loss: 0.9809 - val_age_output_loss: 1.4422 - val_weight_output_loss: 0.9850 - val_bag_output_loss: 0.9141 - val_footwear_output_loss: 0.8473 - val_pose_output_loss: 0.9595 - val_emotion_output_loss: 0.9524 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5519 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.6643 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.47272\n",
            "Epoch 11/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.5849 - gender_output_loss: 0.3643 - image_quality_output_loss: 0.9256 - age_output_loss: 1.3830 - weight_output_loss: 0.9577 - bag_output_loss: 0.8388 - footwear_output_loss: 0.7696 - pose_output_loss: 0.4941 - emotion_output_loss: 0.8519 - gender_output_acc: 0.8241 - image_quality_output_acc: 0.5834 - age_output_acc: 0.4144 - weight_output_acc: 0.6406 - bag_output_acc: 0.6245 - footwear_output_acc: 0.6576 - pose_output_acc: 0.8123 - emotion_output_acc: 0.7195\n",
            "Epoch 00010: val_loss did not improve from 7.47272\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 6.5842 - gender_output_loss: 0.3647 - image_quality_output_loss: 0.9258 - age_output_loss: 1.3837 - weight_output_loss: 0.9575 - bag_output_loss: 0.8380 - footwear_output_loss: 0.7690 - pose_output_loss: 0.4942 - emotion_output_loss: 0.8514 - gender_output_acc: 0.8238 - image_quality_output_acc: 0.5833 - age_output_acc: 0.4143 - weight_output_acc: 0.6408 - bag_output_acc: 0.6249 - footwear_output_acc: 0.6580 - pose_output_acc: 0.8124 - emotion_output_acc: 0.7198 - val_loss: 7.9821 - val_gender_output_loss: 0.6571 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.4244 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.9511 - val_footwear_output_loss: 0.9430 - val_pose_output_loss: 1.0957 - val_emotion_output_loss: 0.9550 - val_gender_output_acc: 0.7490 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.6316 - val_pose_output_acc: 0.6714 - val_emotion_output_acc: 0.6759\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.47272\n",
            "Epoch 12/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.4143 - gender_output_loss: 0.3300 - image_quality_output_loss: 0.9093 - age_output_loss: 1.3640 - weight_output_loss: 0.9527 - bag_output_loss: 0.8173 - footwear_output_loss: 0.7401 - pose_output_loss: 0.4611 - emotion_output_loss: 0.8397 - gender_output_acc: 0.8406 - image_quality_output_acc: 0.5942 - age_output_acc: 0.4230 - weight_output_acc: 0.6448 - bag_output_acc: 0.6300 - footwear_output_acc: 0.6730 - pose_output_acc: 0.8291 - emotion_output_acc: 0.7194\n",
            "Epoch 00011: val_loss did not improve from 7.47272\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 6.4109 - gender_output_loss: 0.3296 - image_quality_output_loss: 0.9092 - age_output_loss: 1.3636 - weight_output_loss: 0.9522 - bag_output_loss: 0.8171 - footwear_output_loss: 0.7397 - pose_output_loss: 0.4606 - emotion_output_loss: 0.8388 - gender_output_acc: 0.8407 - image_quality_output_acc: 0.5941 - age_output_acc: 0.4232 - weight_output_acc: 0.6451 - bag_output_acc: 0.6301 - footwear_output_acc: 0.6731 - pose_output_acc: 0.8292 - emotion_output_acc: 0.7198 - val_loss: 7.5393 - val_gender_output_loss: 0.6342 - val_image_quality_output_loss: 0.9679 - val_age_output_loss: 1.4133 - val_weight_output_loss: 0.9724 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 0.9244 - val_pose_output_loss: 0.7588 - val_emotion_output_loss: 0.9415 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.6966 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 7.47272\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 6.2066 - gender_output_loss: 0.2963 - image_quality_output_loss: 0.8836 - age_output_loss: 1.3403 - weight_output_loss: 0.9344 - bag_output_loss: 0.8045 - footwear_output_loss: 0.7044 - pose_output_loss: 0.4117 - emotion_output_loss: 0.8314 - gender_output_acc: 0.8555 - image_quality_output_acc: 0.6016 - age_output_acc: 0.4317 - weight_output_acc: 0.6477 - bag_output_acc: 0.6376 - footwear_output_acc: 0.6868 - pose_output_acc: 0.8475 - emotion_output_acc: 0.7205 - val_loss: 7.3264 - val_gender_output_loss: 0.4984 - val_image_quality_output_loss: 0.9668 - val_age_output_loss: 1.4247 - val_weight_output_loss: 0.9712 - val_bag_output_loss: 0.9020 - val_footwear_output_loss: 0.8984 - val_pose_output_loss: 0.7225 - val_emotion_output_loss: 0.9425 - val_gender_output_acc: 0.7949 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5791 - val_footwear_output_acc: 0.6316 - val_pose_output_acc: 0.7167 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.47272 to 7.32643, saving model to /content/drive/My Drive/bestmodel5.h5\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.9995 - gender_output_loss: 0.2658 - image_quality_output_loss: 0.8560 - age_output_loss: 1.3254 - weight_output_loss: 0.9214 - bag_output_loss: 0.7777 - footwear_output_loss: 0.6718 - pose_output_loss: 0.3675 - emotion_output_loss: 0.8139 - gender_output_acc: 0.8719 - image_quality_output_acc: 0.6168 - age_output_acc: 0.4339 - weight_output_acc: 0.6530 - bag_output_acc: 0.6451 - footwear_output_acc: 0.6968 - pose_output_acc: 0.8627 - emotion_output_acc: 0.7263 - val_loss: 7.7399 - val_gender_output_loss: 0.7555 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.4234 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.9232 - val_footwear_output_loss: 0.9258 - val_pose_output_loss: 0.8250 - val_emotion_output_loss: 0.9417 - val_gender_output_acc: 0.750 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5721 - val_footwear_output_acc: 0.6361 - val_pose_output_acc: 0.6951 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.32643\n",
            "Epoch 15/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8131 - gender_output_loss: 0.2368 - image_quality_output_loss: 0.8302 - age_output_loss: 1.3035 - weight_output_loss: 0.9096 - bag_output_loss: 0.7565 - footwear_output_loss: 0.6390 - pose_output_loss: 0.3397 - emotion_output_loss: 0.7979 - gender_output_acc: 0.8843 - image_quality_output_acc: 0.6301 - age_output_acc: 0.4389 - weight_output_acc: 0.6573 - bag_output_acc: 0.6555 - footwear_output_acc: 0.7100 - pose_output_acc: 0.8720 - emotion_output_acc: 0.7295\n",
            "Epoch 00014: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.8154 - gender_output_loss: 0.2367 - image_quality_output_loss: 0.8304 - age_output_loss: 1.3036 - weight_output_loss: 0.9101 - bag_output_loss: 0.7571 - footwear_output_loss: 0.6389 - pose_output_loss: 0.3400 - emotion_output_loss: 0.7986 - gender_output_acc: 0.8843 - image_quality_output_acc: 0.6301 - age_output_acc: 0.4388 - weight_output_acc: 0.6569 - bag_output_acc: 0.6550 - footwear_output_acc: 0.7100 - pose_output_acc: 0.8720 - emotion_output_acc: 0.7293 - val_loss: 7.6387 - val_gender_output_loss: 0.6848 - val_image_quality_output_loss: 0.9672 - val_age_output_loss: 1.4210 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.9221 - val_footwear_output_loss: 0.9329 - val_pose_output_loss: 0.8008 - val_emotion_output_loss: 0.9342 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.6310 - val_pose_output_acc: 0.7092 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.32643\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.6749 - gender_output_loss: 0.2232 - image_quality_output_loss: 0.8138 - age_output_loss: 1.2823 - weight_output_loss: 0.8963 - bag_output_loss: 0.7436 - footwear_output_loss: 0.6212 - pose_output_loss: 0.3034 - emotion_output_loss: 0.7910 - gender_output_acc: 0.8928 - image_quality_output_acc: 0.6398 - age_output_acc: 0.4508 - weight_output_acc: 0.6599 - bag_output_acc: 0.6606 - footwear_output_acc: 0.7234 - pose_output_acc: 0.8852 - emotion_output_acc: 0.7295 - val_loss: 7.7740 - val_gender_output_loss: 0.7721 - val_image_quality_output_loss: 0.9611 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.9312 - val_footwear_output_loss: 0.9740 - val_pose_output_loss: 0.8129 - val_emotion_output_loss: 0.9313 - val_gender_output_acc: 0.7172 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5766 - val_footwear_output_acc: 0.6371 - val_pose_output_acc: 0.7193 - val_emotion_output_acc: 0.6840\n",
            "Epoch 16/50\n",
            "Epoch 00016: val_loss did not improve from 7.32643\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.6222 - gender_output_loss: 0.2220 - image_quality_output_loss: 0.7943 - age_output_loss: 1.2766 - weight_output_loss: 0.8860 - bag_output_loss: 0.7336 - footwear_output_loss: 0.6130 - pose_output_loss: 0.3074 - emotion_output_loss: 0.7893 - gender_output_acc: 0.8911 - image_quality_output_acc: 0.6410 - age_output_acc: 0.4543 - weight_output_acc: 0.6645 - bag_output_acc: 0.6674 - footwear_output_acc: 0.7206 - pose_output_acc: 0.8835 - emotion_output_acc: 0.7309 - val_loss: 7.8651 - val_gender_output_loss: 0.7540 - val_image_quality_output_loss: 0.9638 - val_age_output_loss: 1.4281 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9325 - val_footwear_output_loss: 0.9974 - val_pose_output_loss: 0.8684 - val_emotion_output_loss: 0.9378 - val_gender_output_acc: 0.7188 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.3684 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5771 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.7097 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.32643\n",
            "Epoch 18/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6371 - gender_output_loss: 0.2200 - image_quality_output_loss: 0.7976 - age_output_loss: 1.2746 - weight_output_loss: 0.8926 - bag_output_loss: 0.7415 - footwear_output_loss: 0.6176 - pose_output_loss: 0.3051 - emotion_output_loss: 0.7880 - gender_output_acc: 0.8901 - image_quality_output_acc: 0.6412 - age_output_acc: 0.4493 - weight_output_acc: 0.6636 - bag_output_acc: 0.6589 - footwear_output_acc: 0.7190 - pose_output_acc: 0.8861 - emotion_output_acc: 0.7302\n",
            "360/360 [==============================] - 42s 115ms/step - loss: 5.6391 - gender_output_loss: 0.2201 - image_quality_output_loss: 0.7979 - age_output_loss: 1.2752 - weight_output_loss: 0.8927 - bag_output_loss: 0.7419 - footwear_output_loss: 0.6180 - pose_output_loss: 0.3059 - emotion_output_loss: 0.7875 - gender_output_acc: 0.8902 - image_quality_output_acc: 0.6412 - age_output_acc: 0.4491 - weight_output_acc: 0.6636 - bag_output_acc: 0.6587 - footwear_output_acc: 0.7187 - pose_output_acc: 0.8859 - emotion_output_acc: 0.7303 - val_loss: 7.6275 - val_gender_output_loss: 0.6211 - val_image_quality_output_loss: 0.9611 - val_age_output_loss: 1.4215 - val_weight_output_loss: 0.9732 - val_bag_output_loss: 0.9241 - val_footwear_output_loss: 0.9824 - val_pose_output_loss: 0.8050 - val_emotion_output_loss: 0.9392 - val_gender_output_acc: 0.7591 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5731 - val_footwear_output_acc: 0.6351 - val_pose_output_acc: 0.7142 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.32643\n",
            "Epoch 19/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6647 - gender_output_loss: 0.2241 - image_quality_output_loss: 0.7992 - age_output_loss: 1.2796 - weight_output_loss: 0.8945 - bag_output_loss: 0.7378 - footwear_output_loss: 0.6180 - pose_output_loss: 0.3175 - emotion_output_loss: 0.7939 - gender_output_acc: 0.8896 - image_quality_output_acc: 0.6381 - age_output_acc: 0.4481 - weight_output_acc: 0.6588 - bag_output_acc: 0.6678 - footwear_output_acc: 0.7221 - pose_output_acc: 0.8824 - emotion_output_acc: 0.7295Epoch 19/50\n",
            "\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.6641 - gender_output_loss: 0.2239 - image_quality_output_loss: 0.7991 - age_output_loss: 1.2792 - weight_output_loss: 0.8945 - bag_output_loss: 0.7376 - footwear_output_loss: 0.6187 - pose_output_loss: 0.3175 - emotion_output_loss: 0.7938 - gender_output_acc: 0.8898 - image_quality_output_acc: 0.6381 - age_output_acc: 0.4483 - weight_output_acc: 0.6589 - bag_output_acc: 0.6681 - footwear_output_acc: 0.7215 - pose_output_acc: 0.8823 - emotion_output_acc: 0.7296 - val_loss: 7.5827 - val_gender_output_loss: 0.6091 - val_image_quality_output_loss: 0.9598 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9719 - val_bag_output_loss: 0.9075 - val_footwear_output_loss: 0.9588 - val_pose_output_loss: 0.8211 - val_emotion_output_loss: 0.9375 - val_gender_output_acc: 0.7651 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7067 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.32643\n",
            "Epoch 20/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6772 - gender_output_loss: 0.2323 - image_quality_output_loss: 0.7906 - age_output_loss: 1.2885 - weight_output_loss: 0.8903 - bag_output_loss: 0.7403 - footwear_output_loss: 0.6282 - pose_output_loss: 0.3095 - emotion_output_loss: 0.7976 - gender_output_acc: 0.8886 - image_quality_output_acc: 0.6502 - age_output_acc: 0.4476 - weight_output_acc: 0.6631 - bag_output_acc: 0.6660 - footwear_output_acc: 0.7183 - pose_output_acc: 0.8860 - emotion_output_acc: 0.7269\n",
            "Epoch 00019: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.6775 - gender_output_loss: 0.2324 - image_quality_output_loss: 0.7902 - age_output_loss: 1.2883 - weight_output_loss: 0.8906 - bag_output_loss: 0.7401 - footwear_output_loss: 0.6281 - pose_output_loss: 0.3093 - emotion_output_loss: 0.7984 - gender_output_acc: 0.8886 - image_quality_output_acc: 0.6505 - age_output_acc: 0.4477 - weight_output_acc: 0.6629 - bag_output_acc: 0.6659 - footwear_output_acc: 0.7182 - pose_output_acc: 0.8861 - emotion_output_acc: 0.7265 - val_loss: 8.0234 - val_gender_output_loss: 0.7831 - val_image_quality_output_loss: 0.9930 - val_age_output_loss: 1.4237 - val_weight_output_loss: 1.0168 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 0.9508 - val_pose_output_loss: 0.9708 - val_emotion_output_loss: 0.9486 - val_gender_output_acc: 0.6875 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7036 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 7.32643\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.7483 - gender_output_loss: 0.2457 - image_quality_output_loss: 0.7931 - age_output_loss: 1.2920 - weight_output_loss: 0.8972 - bag_output_loss: 0.7501 - footwear_output_loss: 0.6345 - pose_output_loss: 0.3331 - emotion_output_loss: 0.8026 - gender_output_acc: 0.8823 - image_quality_output_acc: 0.6457 - age_output_acc: 0.4450 - weight_output_acc: 0.6559 - bag_output_acc: 0.6609 - footwear_output_acc: 0.7163 - pose_output_acc: 0.8797 - emotion_output_acc: 0.7273 - val_loss: 7.5698 - val_gender_output_loss: 0.5272 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.4314 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9348 - val_footwear_output_loss: 0.9962 - val_pose_output_loss: 0.7987 - val_emotion_output_loss: 0.9394 - val_gender_output_acc: 0.7540 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5756 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.7223 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 7.32643Epoch 21/50\n",
            "\n",
            "Epoch 22/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8440 - gender_output_loss: 0.2511 - image_quality_output_loss: 0.8121 - age_output_loss: 1.3019 - weight_output_loss: 0.9046 - bag_output_loss: 0.7571 - footwear_output_loss: 0.6579 - pose_output_loss: 0.3459 - emotion_output_loss: 0.8133 - gender_output_acc: 0.8798 - image_quality_output_acc: 0.6431 - age_output_acc: 0.4425 - weight_output_acc: 0.6630 - bag_output_acc: 0.6587 - footwear_output_acc: 0.7087 - pose_output_acc: 0.8744 - emotion_output_acc: 0.7258\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.8454 - gender_output_loss: 0.2513 - image_quality_output_loss: 0.8118 - age_output_loss: 1.3019 - weight_output_loss: 0.9046 - bag_output_loss: 0.7569 - footwear_output_loss: 0.6593 - pose_output_loss: 0.3458 - emotion_output_loss: 0.8138 - gender_output_acc: 0.8798 - image_quality_output_acc: 0.6431 - age_output_acc: 0.4427 - weight_output_acc: 0.6628 - bag_output_acc: 0.6588 - footwear_output_acc: 0.7082 - pose_output_acc: 0.8745 - emotion_output_acc: 0.7257 - val_loss: 8.0502 - val_gender_output_loss: 0.5862 - val_image_quality_output_loss: 0.9704 - val_age_output_loss: 1.4399 - val_weight_output_loss: 1.0101 - val_bag_output_loss: 1.0277 - val_footwear_output_loss: 0.9570 - val_pose_output_loss: 1.1190 - val_emotion_output_loss: 0.9399 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5696 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.6754 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 7.32643\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.9563 - gender_output_loss: 0.2691 - image_quality_output_loss: 0.8356 - age_output_loss: 1.3227 - weight_output_loss: 0.9061 - bag_output_loss: 0.7725 - footwear_output_loss: 0.6733 - pose_output_loss: 0.3559 - emotion_output_loss: 0.8211 - gender_output_acc: 0.8754 - image_quality_output_acc: 0.6369 - age_output_acc: 0.4352 - weight_output_acc: 0.6589 - bag_output_acc: 0.6525 - footwear_output_acc: 0.7097 - pose_output_acc: 0.8763 - emotion_output_acc: 0.7253 - val_loss: 8.0998 - val_gender_output_loss: 0.5845 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.4377 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 1.0802 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 1.0218 - val_emotion_output_loss: 0.9545 - val_gender_output_acc: 0.7697 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3468 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5534 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.6890 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 7.32643\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 6.1218 - gender_output_loss: 0.3054 - image_quality_output_loss: 0.8300 - age_output_loss: 1.3276 - weight_output_loss: 0.9304 - bag_output_loss: 0.7907 - footwear_output_loss: 0.6911 - pose_output_loss: 0.4130 - emotion_output_loss: 0.8336 - gender_output_acc: 0.8636 - image_quality_output_acc: 0.6404 - age_output_acc: 0.4404 - weight_output_acc: 0.6519 - bag_output_acc: 0.6453 - footwear_output_acc: 0.7019 - pose_output_acc: 0.8588 - emotion_output_acc: 0.7236 - val_loss: 7.5372 - val_gender_output_loss: 0.4908 - val_image_quality_output_loss: 0.9899 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9077 - val_footwear_output_loss: 0.9892 - val_pose_output_loss: 0.7875 - val_emotion_output_loss: 0.9514 - val_gender_output_acc: 0.7732 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.7036 - val_emotion_output_acc: 0.6845\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.32643\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 25/50\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 6.1966 - gender_output_loss: 0.3052 - image_quality_output_loss: 0.8420 - age_output_loss: 1.3570 - weight_output_loss: 0.9381 - bag_output_loss: 0.7967 - footwear_output_loss: 0.6922 - pose_output_loss: 0.4208 - emotion_output_loss: 0.8446 - gender_output_acc: 0.8657 - image_quality_output_acc: 0.6406 - age_output_acc: 0.4306 - weight_output_acc: 0.6518 - bag_output_acc: 0.6300 - footwear_output_acc: 0.6993 - pose_output_acc: 0.8574 - emotion_output_acc: 0.7229 - val_loss: 7.8862 - val_gender_output_loss: 0.6055 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9307 - val_footwear_output_loss: 1.0235 - val_pose_output_loss: 0.9696 - val_emotion_output_loss: 0.9515 - val_gender_output_acc: 0.7505 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.6099 - val_pose_output_acc: 0.6875 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 7.32643\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 6.0660 - gender_output_loss: 0.2779 - image_quality_output_loss: 0.8124 - age_output_loss: 1.3407 - weight_output_loss: 0.9255 - bag_output_loss: 0.7866 - footwear_output_loss: 0.6873 - pose_output_loss: 0.3914 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8717 - image_quality_output_acc: 0.6517 - age_output_acc: 0.4337 - weight_output_acc: 0.6582 - bag_output_acc: 0.6403 - footwear_output_acc: 0.7029 - pose_output_acc: 0.8679 - emotion_output_acc: 0.7221 - val_loss: 7.5145 - val_gender_output_loss: 0.5071 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4416 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9109 - val_footwear_output_loss: 0.9807 - val_pose_output_loss: 0.7522 - val_emotion_output_loss: 0.9518 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.7147 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 7.32643\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 6.0660 - gender_output_loss: 0.2779 - image_quality_output_loss: 0.8124 - age_output_loss: 1.3407 - weight_output_loss: 0.9255 - bag_output_loss: 0.7866 - footwear_output_loss: 0.6873 - pose_output_loss: 0.3914 - emotion_output_loss: 0.8442 - gender_output_acc: 0.8717 - image_quality_output_acc: 0.6517 - age_output_acc: 0.4337 - weight_output_acc: 0.6582 - bag_output_acc: 0.6403 - footwear_output_acc: 0.7029 - pose_output_acc: 0.8679 - emotion_output_acc: 0.7221 - val_loss: 7.5145 - val_gender_output_loss: 0.5071 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4416 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9109 - val_footwear_output_loss: 0.9807 - val_pose_output_loss: 0.7522 - val_emotion_output_loss: 0.9518 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.7147 - val_emotion_output_acc: 0.6825\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.8277 - gender_output_loss: 0.2592 - image_quality_output_loss: 0.7796 - age_output_loss: 1.3099 - weight_output_loss: 0.9061 - bag_output_loss: 0.7650 - footwear_output_loss: 0.6448 - pose_output_loss: 0.3447 - emotion_output_loss: 0.8183 - gender_output_acc: 0.8818 - image_quality_output_acc: 0.6694 - age_output_acc: 0.4435 - weight_output_acc: 0.6600 - bag_output_acc: 0.6441 - footwear_output_acc: 0.7152 - pose_output_acc: 0.8828 - emotion_output_acc: 0.7256 - val_loss: 7.8254 - val_gender_output_loss: 0.5287 - val_image_quality_output_loss: 1.0094 - val_age_output_loss: 1.4451 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9615 - val_footwear_output_loss: 1.1840 - val_pose_output_loss: 0.7679 - val_emotion_output_loss: 0.9496 - val_gender_output_acc: 0.7737 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5635 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7021 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 7.32643\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 42s 115ms/step - loss: 5.6084 - gender_output_loss: 0.2155 - image_quality_output_loss: 0.7527 - age_output_loss: 1.2815 - weight_output_loss: 0.8863 - bag_output_loss: 0.7466 - footwear_output_loss: 0.6173 - pose_output_loss: 0.3008 - emotion_output_loss: 0.8076 - gender_output_acc: 0.8999 - image_quality_output_acc: 0.6800 - age_output_acc: 0.4530 - weight_output_acc: 0.6684 - bag_output_acc: 0.6556 - footwear_output_acc: 0.7273 - pose_output_acc: 0.8960 - emotion_output_acc: 0.7274 - val_loss: 7.6981 - val_gender_output_loss: 0.6776 - val_image_quality_output_loss: 1.0080 - val_age_output_loss: 1.4235 - val_weight_output_loss: 0.9718 - val_bag_output_loss: 0.9630 - val_footwear_output_loss: 0.9327 - val_pose_output_loss: 0.7717 - val_emotion_output_loss: 0.9498 - val_gender_output_acc: 0.7797 - val_image_quality_output_acc: 0.5474 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.7011 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 7.32643\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.4061 - gender_output_loss: 0.2075 - image_quality_output_loss: 0.7203 - age_output_loss: 1.2473 - weight_output_loss: 0.8528 - bag_output_loss: 0.7118 - footwear_output_loss: 0.5863 - pose_output_loss: 0.2836 - emotion_output_loss: 0.7964 - gender_output_acc: 0.9045 - image_quality_output_acc: 0.6991 - age_output_acc: 0.4639 - weight_output_acc: 0.6758 - bag_output_acc: 0.6648 - footwear_output_acc: 0.7341 - pose_output_acc: 0.9024 - emotion_output_acc: 0.7285 - val_loss: 7.7992 - val_gender_output_loss: 0.6756 - val_image_quality_output_loss: 1.0053 - val_age_output_loss: 1.4266 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9450 - val_footwear_output_loss: 1.0470 - val_pose_output_loss: 0.7728 - val_emotion_output_loss: 0.9441 - val_gender_output_acc: 0.7717 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.7157 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 7.32643\n",
            "Epoch 30/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.2706 - gender_output_loss: 0.1943 - image_quality_output_loss: 0.6975 - age_output_loss: 1.2331 - weight_output_loss: 0.8284 - bag_output_loss: 0.6960 - footwear_output_loss: 0.5678 - pose_output_loss: 0.2692 - emotion_output_loss: 0.7843 - gender_output_acc: 0.9069 - image_quality_output_acc: 0.7116 - age_output_acc: 0.4652 - weight_output_acc: 0.6861 - bag_output_acc: 0.6683 - footwear_output_acc: 0.7466 - pose_output_acc: 0.9076 - emotion_output_acc: 0.7329\n",
            "Epoch 00029: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.2722 - gender_output_loss: 0.1942 - image_quality_output_loss: 0.6977 - age_output_loss: 1.2329 - weight_output_loss: 0.8293 - bag_output_loss: 0.6964 - footwear_output_loss: 0.5677 - pose_output_loss: 0.2691 - emotion_output_loss: 0.7850 - gender_output_acc: 0.9070 - image_quality_output_acc: 0.7115 - age_output_acc: 0.4650 - weight_output_acc: 0.6858 - bag_output_acc: 0.6681 - footwear_output_acc: 0.7468 - pose_output_acc: 0.9077 - emotion_output_acc: 0.7326 - val_loss: 7.8005 - val_gender_output_loss: 0.6342 - val_image_quality_output_loss: 1.0217 - val_age_output_loss: 1.4209 - val_weight_output_loss: 0.9675 - val_bag_output_loss: 0.9472 - val_footwear_output_loss: 1.0264 - val_pose_output_loss: 0.8397 - val_emotion_output_loss: 0.9430 - val_gender_output_acc: 0.7747 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6336 - val_pose_output_acc: 0.6915 - val_emotion_output_acc: 0.6820\n",
            "Epoch 30/50\n",
            "Epoch 00030: val_loss did not improve from 7.32643\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 42s 115ms/step - loss: 5.0799 - gender_output_loss: 0.1718 - image_quality_output_loss: 0.6671 - age_output_loss: 1.1941 - weight_output_loss: 0.8063 - bag_output_loss: 0.6794 - footwear_output_loss: 0.5473 - pose_output_loss: 0.2463 - emotion_output_loss: 0.7675 - gender_output_acc: 0.9178 - image_quality_output_acc: 0.7245 - age_output_acc: 0.4838 - weight_output_acc: 0.6963 - bag_output_acc: 0.6793 - footwear_output_acc: 0.7520 - pose_output_acc: 0.9135 - emotion_output_acc: 0.7353 - val_loss: 7.9014 - val_gender_output_loss: 0.7289 - val_image_quality_output_loss: 1.0295 - val_age_output_loss: 1.4219 - val_weight_output_loss: 0.9664 - val_bag_output_loss: 0.9625 - val_footwear_output_loss: 1.0571 - val_pose_output_loss: 0.7933 - val_emotion_output_loss: 0.9418 - val_gender_output_acc: 0.7631 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5781 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.7102 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 7.32643\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 7.32643\n",
            "Epoch 31/50\n",
            "Epoch 32/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.9850 - gender_output_loss: 0.1670 - image_quality_output_loss: 0.6513 - age_output_loss: 1.1810 - weight_output_loss: 0.7913 - bag_output_loss: 0.6696 - footwear_output_loss: 0.5377 - pose_output_loss: 0.2333 - emotion_output_loss: 0.7538 - gender_output_acc: 0.9163 - image_quality_output_acc: 0.7323 - age_output_acc: 0.4860 - weight_output_acc: 0.6973 - bag_output_acc: 0.6774 - footwear_output_acc: 0.7552 - pose_output_acc: 0.9162 - emotion_output_acc: 0.7389\n",
            "Epoch 00031: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.9868 - gender_output_loss: 0.1672 - image_quality_output_loss: 0.6514 - age_output_loss: 1.1808 - weight_output_loss: 0.7913 - bag_output_loss: 0.6700 - footwear_output_loss: 0.5383 - pose_output_loss: 0.2331 - emotion_output_loss: 0.7546 - gender_output_acc: 0.9162 - image_quality_output_acc: 0.7319 - age_output_acc: 0.4861 - weight_output_acc: 0.6973 - bag_output_acc: 0.6773 - footwear_output_acc: 0.7548 - pose_output_acc: 0.9161 - emotion_output_acc: 0.7387 - val_loss: 7.9112 - val_gender_output_loss: 0.6822 - val_image_quality_output_loss: 1.0576 - val_age_output_loss: 1.4210 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.9835 - val_footwear_output_loss: 1.0588 - val_pose_output_loss: 0.7999 - val_emotion_output_loss: 0.9386 - val_gender_output_acc: 0.7692 - val_image_quality_output_acc: 0.5388 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7167 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 7.32643\n",
            "Epoch 33/50\n",
            "360/360 [==============================]Epoch 33/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.9297 - gender_output_loss: 0.1660 - image_quality_output_loss: 0.6473 - age_output_loss: 1.1647 - weight_output_loss: 0.7878 - bag_output_loss: 0.6577 - footwear_output_loss: 0.5243 - pose_output_loss: 0.2291 - emotion_output_loss: 0.7527 - gender_output_acc: 0.9187 - image_quality_output_acc: 0.7333 - age_output_acc: 0.4951 - weight_output_acc: 0.6990 - bag_output_acc: 0.6867 - footwear_output_acc: 0.7574 - pose_output_acc: 0.9162 - emotion_output_acc: 0.7401 - val_loss: 7.7819 - val_gender_output_loss: 0.6054 - val_image_quality_output_loss: 1.0267 - val_age_output_loss: 1.4193 - val_weight_output_loss: 0.9691 - val_bag_output_loss: 0.9528 - val_footwear_output_loss: 1.0536 - val_pose_output_loss: 0.8147 - val_emotion_output_loss: 0.9404 - val_gender_output_acc: 0.7888 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.7026 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.32643\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.9482 - gender_output_loss: 0.1663 - image_quality_output_loss: 0.6502 - age_output_loss: 1.1724 - weight_output_loss: 0.7942 - bag_output_loss: 0.6618 - footwear_output_loss: 0.5252 - pose_output_loss: 0.2273 - emotion_output_loss: 0.7510 - gender_output_acc: 0.9166 - image_quality_output_acc: 0.7299 - age_output_acc: 0.4915 - weight_output_acc: 0.6954 - bag_output_acc: 0.6835 - footwear_output_acc: 0.7579 - pose_output_acc: 0.9168 - emotion_output_acc: 0.7401 - val_loss: 7.9067 - val_gender_output_loss: 0.7150 - val_image_quality_output_loss: 1.0258 - val_age_output_loss: 1.4242 - val_weight_output_loss: 0.9673 - val_bag_output_loss: 0.9825 - val_footwear_output_loss: 1.0835 - val_pose_output_loss: 0.7696 - val_emotion_output_loss: 0.9387 - val_gender_output_acc: 0.7762 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.6406 - val_pose_output_acc: 0.7233 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.32643\n",
            "Epoch 35/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.9929 - gender_output_loss: 0.1698 - image_quality_output_loss: 0.6569 - age_output_loss: 1.1822 - weight_output_loss: 0.8070 - bag_output_loss: 0.6549 - footwear_output_loss: 0.5359 - pose_output_loss: 0.2321 - emotion_output_loss: 0.7540 - gender_output_acc: 0.9201 - image_quality_output_acc: 0.7259 - age_output_acc: 0.4871 - weight_output_acc: 0.6943 - bag_output_acc: 0.6904 - footwear_output_acc: 0.7509 - pose_output_acc: 0.9185 - emotion_output_acc: 0.7382\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.9482 - gender_output_loss: 0.1663 - image_quality_output_loss: 0.6502 - age_output_loss: 1.1724 - weight_output_loss: 0.7942 - bag_output_loss: 0.6618 - footwear_output_loss: 0.5252 - pose_output_loss: 0.2273 - emotion_output_loss: 0.7510 - gender_output_acc: 0.9166 - image_quality_output_acc: 0.7299 - age_output_acc: 0.4915 - weight_output_acc: 0.6954 - bag_output_acc: 0.6835 - footwear_output_acc: 0.7579 - pose_output_acc: 0.9168 - emotion_output_acc: 0.7401 - val_loss: 7.9067 - val_gender_output_loss: 0.7150 - val_image_quality_output_loss: 1.0258 - val_age_output_loss: 1.4242 - val_weight_output_loss: 0.9673 - val_bag_output_loss: 0.9825 - val_footwear_output_loss: 1.0835 - val_pose_output_loss: 0.7696 - val_emotion_output_loss: 0.9387 - val_gender_output_acc: 0.7762 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.6406 - val_pose_output_acc: 0.7233 - val_emotion_output_acc: 0.6850\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.9938 - gender_output_loss: 0.1697 - image_quality_output_loss: 0.6570 - age_output_loss: 1.1820 - weight_output_loss: 0.8074 - bag_output_loss: 0.6554 - footwear_output_loss: 0.5359 - pose_output_loss: 0.2316 - emotion_output_loss: 0.7548 - gender_output_acc: 0.9201 - image_quality_output_acc: 0.7260 - age_output_acc: 0.4876 - weight_output_acc: 0.6942 - bag_output_acc: 0.6904 - footwear_output_acc: 0.7510 - pose_output_acc: 0.9187 - emotion_output_acc: 0.7378 - val_loss: 8.0988 - val_gender_output_loss: 0.6494 - val_image_quality_output_loss: 1.0731 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9748 - val_bag_output_loss: 1.0149 - val_footwear_output_loss: 1.2039 - val_pose_output_loss: 0.8206 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.7636 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5837 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7152 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.32643\n",
            "Epoch 36/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.0346 - gender_output_loss: 0.1747 - image_quality_output_loss: 0.6606 - age_output_loss: 1.1957 - weight_output_loss: 0.8040 - bag_output_loss: 0.6685 - footwear_output_loss: 0.5345 - pose_output_loss: 0.2372 - emotion_output_loss: 0.7594 - gender_output_acc: 0.9165 - image_quality_output_acc: 0.7256 - age_output_acc: 0.4801 - weight_output_acc: 0.6946 - bag_output_acc: 0.6793 - footwear_output_acc: 0.7544 - pose_output_acc: 0.9169 - emotion_output_acc: 0.7389Epoch 36/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.0352 - gender_output_loss: 0.1749 - image_quality_output_loss: 0.6606 - age_output_loss: 1.1958 - weight_output_loss: 0.8038 - bag_output_loss: 0.6685 - footwear_output_loss: 0.5346 - pose_output_loss: 0.2370 - emotion_output_loss: 0.7599 - gender_output_acc: 0.9163 - image_quality_output_acc: 0.7257 - age_output_acc: 0.4803 - weight_output_acc: 0.6945 - bag_output_acc: 0.6793 - footwear_output_acc: 0.7545 - pose_output_acc: 0.9170 - emotion_output_acc: 0.7387 - val_loss: 7.8618 - val_gender_output_loss: 0.5716 - val_image_quality_output_loss: 1.0190 - val_age_output_loss: 1.4544 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 1.0307 - val_footwear_output_loss: 1.0620 - val_pose_output_loss: 0.8080 - val_emotion_output_loss: 0.9401 - val_gender_output_acc: 0.7707 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5459 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7132 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 7.32643\n",
            "Epoch 37/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.1233 - gender_output_loss: 0.1875 - image_quality_output_loss: 0.6575 - age_output_loss: 1.2020 - weight_output_loss: 0.8106 - bag_output_loss: 0.6737 - footwear_output_loss: 0.5647 - pose_output_loss: 0.2524 - emotion_output_loss: 0.7748 - gender_output_acc: 0.9136 - image_quality_output_acc: 0.7315 - age_output_acc: 0.4842 - weight_output_acc: 0.6925 - bag_output_acc: 0.6860 - footwear_output_acc: 0.7460 - pose_output_acc: 0.9111 - emotion_output_acc: 0.7360\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.1257 - gender_output_loss: 0.1876 - image_quality_output_loss: 0.6583 - age_output_loss: 1.2023 - weight_output_loss: 0.8116 - bag_output_loss: 0.6736 - footwear_output_loss: 0.5653 - pose_output_loss: 0.2521 - emotion_output_loss: 0.7749 - gender_output_acc: 0.9135 - image_quality_output_acc: 0.7310 - age_output_acc: 0.4838 - weight_output_acc: 0.6921 - bag_output_acc: 0.6860 - footwear_output_acc: 0.7456 - pose_output_acc: 0.9113 - emotion_output_acc: 0.7359 - val_loss: 8.3073 - val_gender_output_loss: 0.8093 - val_image_quality_output_loss: 1.0722 - val_age_output_loss: 1.4697 - val_weight_output_loss: 1.0133 - val_bag_output_loss: 1.0162 - val_footwear_output_loss: 1.0816 - val_pose_output_loss: 0.8966 - val_emotion_output_loss: 0.9484 - val_gender_output_acc: 0.7661 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5711 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.6925 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 7.32643\n",
            "Epoch 37/50\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.2323 - gender_output_loss: 0.2037 - image_quality_output_loss: 0.6718 - age_output_loss: 1.2203 - weight_output_loss: 0.8315 - bag_output_loss: 0.6844 - footwear_output_loss: 0.5727 - pose_output_loss: 0.2696 - emotion_output_loss: 0.7784 - gender_output_acc: 0.9014 - image_quality_output_acc: 0.7240 - age_output_acc: 0.4781 - weight_output_acc: 0.6865 - bag_output_acc: 0.6872 - footwear_output_acc: 0.7407 - pose_output_acc: 0.9108 - emotion_output_acc: 0.7341 - val_loss: 8.2272 - val_gender_output_loss: 0.6151 - val_image_quality_output_loss: 1.0905 - val_age_output_loss: 1.4577 - val_weight_output_loss: 0.9730 - val_bag_output_loss: 0.9729 - val_footwear_output_loss: 1.1826 - val_pose_output_loss: 0.9814 - val_emotion_output_loss: 0.9539 - val_gender_output_acc: 0.7828 - val_image_quality_output_acc: 0.5494 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.6310 - val_pose_output_acc: 0.6976 - val_emotion_output_acc: 0.6789\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 7.32643\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.4107 - gender_output_loss: 0.2195 - image_quality_output_loss: 0.7019 - age_output_loss: 1.2552 - weight_output_loss: 0.8373 - bag_output_loss: 0.7026 - footwear_output_loss: 0.6021 - pose_output_loss: 0.3032 - emotion_output_loss: 0.7889 - gender_output_acc: 0.9040 - image_quality_output_acc: 0.7148 - age_output_acc: 0.4654 - weight_output_acc: 0.6873 - bag_output_acc: 0.6736 - footwear_output_acc: 0.7347 - pose_output_acc: 0.8986 - emotion_output_acc: 0.7320 - val_loss: 8.3468 - val_gender_output_loss: 0.9587 - val_image_quality_output_loss: 1.0344 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9997 - val_bag_output_loss: 1.0162 - val_footwear_output_loss: 1.0675 - val_pose_output_loss: 0.8801 - val_emotion_output_loss: 0.9495 - val_gender_output_acc: 0.7298 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.6966 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.32643\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.5734 - gender_output_loss: 0.2375 - image_quality_output_loss: 0.7152 - age_output_loss: 1.2657 - weight_output_loss: 0.8711 - bag_output_loss: 0.7286 - footwear_output_loss: 0.6283 - pose_output_loss: 0.3258 - emotion_output_loss: 0.8013 - gender_output_acc: 0.8931 - image_quality_output_acc: 0.7080 - age_output_acc: 0.4637 - weight_output_acc: 0.6770 - bag_output_acc: 0.6563 - footwear_output_acc: 0.7294 - pose_output_acc: 0.8919 - emotion_output_acc: 0.7312 - val_loss: 7.8316 - val_gender_output_loss: 0.5142 - val_image_quality_output_loss: 1.0690 - val_age_output_loss: 1.4558 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 1.0255 - val_footwear_output_loss: 0.9812 - val_pose_output_loss: 0.8356 - val_emotion_output_loss: 0.9581 - val_gender_output_acc: 0.7802 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5691 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.6699 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.32643\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.7297 - gender_output_loss: 0.2517 - image_quality_output_loss: 0.7417 - age_output_loss: 1.2925 - weight_output_loss: 0.8923 - bag_output_loss: 0.7511 - footwear_output_loss: 0.6331 - pose_output_loss: 0.3477 - emotion_output_loss: 0.8196 - gender_output_acc: 0.8865 - image_quality_output_acc: 0.6990 - age_output_acc: 0.4577 - weight_output_acc: 0.6710 - bag_output_acc: 0.6433 - footwear_output_acc: 0.7266 - pose_output_acc: 0.8820 - emotion_output_acc: 0.7263 - val_loss: 7.8790 - val_gender_output_loss: 0.508 - val_image_quality_output_loss: 1.0400 - val_age_output_loss: 1.4981 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.9155 - val_footwear_output_loss: 1.0165 - val_pose_output_loss: 0.9646 - val_emotion_output_loss: 0.9534 - val_gender_output_acc: 0.7631 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6280 - val_pose_output_acc: 0.6991 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 7.32643\n",
            "Epoch 42/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6050 - gender_output_loss: 0.2420 - image_quality_output_loss: 0.7199 - age_output_loss: 1.2774 - weight_output_loss: 0.8801 - bag_output_loss: 0.7282 - footwear_output_loss: 0.6152 - pose_output_loss: 0.3341 - emotion_output_loss: 0.8081 - gender_output_acc: 0.8898 - image_quality_output_acc: 0.7102 - age_output_acc: 0.4563 - weight_output_acc: 0.6712 - bag_output_acc: 0.6598 - footwear_output_acc: 0.7342 - pose_output_acc: 0.8889 - emotion_output_acc: 0.7311Epoch 42/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.6050 - gender_output_loss: 0.2422 - image_quality_output_loss: 0.7200 - age_output_loss: 1.2770 - weight_output_loss: 0.8803 - bag_output_loss: 0.7282 - footwear_output_loss: 0.6158 - pose_output_loss: 0.3342 - emotion_output_loss: 0.8073 - gender_output_acc: 0.8900 - image_quality_output_acc: 0.7102 - age_output_acc: 0.4565 - weight_output_acc: 0.6711 - bag_output_acc: 0.6597 - footwear_output_acc: 0.7339 - pose_output_acc: 0.8888 - emotion_output_acc: 0.7314 - val_loss: 8.2038 - val_gender_output_loss: 0.5051 - val_image_quality_output_loss: 1.2823 - val_age_output_loss: 1.4652 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9111 - val_footwear_output_loss: 1.2710 - val_pose_output_loss: 0.8369 - val_emotion_output_loss: 0.9553 - val_gender_output_acc: 0.7707 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3619 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5444 - val_footwear_output_acc: 0.6351 - val_pose_output_acc: 0.6920 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.32643\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 5.4407 - gender_output_loss: 0.2137 - image_quality_output_loss: 0.6776 - age_output_loss: 1.2537 - weight_output_loss: 0.8664 - bag_output_loss: 0.7150 - footwear_output_loss: 0.6125 - pose_output_loss: 0.2984 - emotion_output_loss: 0.8033 - gender_output_acc: 0.9055 - image_quality_output_acc: 0.7254 - age_output_acc: 0.4640 - weight_output_acc: 0.6760 - bag_output_acc: 0.6604 - footwear_output_acc: 0.7340 - pose_output_acc: 0.9015 - emotion_output_acc: 0.7315 - val_loss: 8.8396 - val_gender_output_loss: 1.0232 - val_image_quality_output_loss: 1.1870 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9981 - val_bag_output_loss: 0.9969 - val_footwear_output_loss: 1.3264 - val_pose_output_loss: 0.9154 - val_emotion_output_loss: 0.9519 - val_gender_output_acc: 0.7223 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.5680 - val_footwear_output_acc: 0.6134 - val_pose_output_acc: 0.6789 - val_emotion_output_acc: 0.6804\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 7.32643\n",
            "Epoch 44/50\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.2473 - gender_output_loss: 0.1967 - image_quality_output_loss: 0.6595 - age_output_loss: 1.2233 - weight_output_loss: 0.8450 - bag_output_loss: 0.6860 - footwear_output_loss: 0.5789 - pose_output_loss: 0.2715 - emotion_output_loss: 0.7865 - gender_output_acc: 0.9099 - image_quality_output_acc: 0.7330 - age_output_acc: 0.4774 - weight_output_acc: 0.6786 - bag_output_acc: 0.6763 - footwear_output_acc: 0.7400 - pose_output_acc: 0.9114 - emotion_output_acc: 0.7337 - val_loss: 8.3339 - val_gender_output_loss: 0.5821 - val_image_quality_output_loss: 1.1227 - val_age_output_loss: 1.4506 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.9767 - val_footwear_output_loss: 1.2513 - val_pose_output_loss: 1.0274 - val_emotion_output_loss: 0.9483 - val_gender_output_acc: 0.7717 - val_image_quality_output_acc: 0.5338 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5660 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.6986 - val_emotion_output_acc: 0.6809\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 7.32643\n",
            "Epoch 45/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.0601 - gender_output_loss: 0.1851 - image_quality_output_loss: 0.6427 - age_output_loss: 1.1876 - weight_output_loss: 0.8078 - bag_output_loss: 0.6650 - footwear_output_loss: 0.5465 - pose_output_loss: 0.2496 - emotion_output_loss: 0.7759 - gender_output_acc: 0.9132 - image_quality_output_acc: 0.7435 - age_output_acc: 0.4892 - weight_output_acc: 0.6933 - bag_output_acc: 0.6845 - footwear_output_acc: 0.7530 - pose_output_acc: 0.9173 - emotion_output_acc: 0.7342Epoch 45/50\n",
            "Epoch 00044: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 41s 114ms/step - loss: 5.0587 - gender_output_loss: 0.1851 - image_quality_output_loss: 0.6424 - age_output_loss: 1.1871 - weight_output_loss: 0.8070 - bag_output_loss: 0.6650 - footwear_output_loss: 0.5466 - pose_output_loss: 0.2495 - emotion_output_loss: 0.7760 - gender_output_acc: 0.9133 - image_quality_output_acc: 0.7437 - age_output_acc: 0.4892 - weight_output_acc: 0.6936 - bag_output_acc: 0.6842 - footwear_output_acc: 0.7531 - pose_output_acc: 0.9174 - emotion_output_acc: 0.7339 - val_loss: 8.4182 - val_gender_output_loss: 0.8367 - val_image_quality_output_loss: 1.2079 - val_age_output_loss: 1.4560 - val_weight_output_loss: 1.0112 - val_bag_output_loss: 0.9345 - val_footwear_output_loss: 1.1350 - val_pose_output_loss: 0.8853 - val_emotion_output_loss: 0.9516 - val_gender_output_acc: 0.7268 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.6845 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 7.32643\n",
            "Epoch 46/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.8824 - gender_output_loss: 0.1640 - image_quality_output_loss: 0.6160 - age_output_loss: 1.1602 - weight_output_loss: 0.7818 - bag_output_loss: 0.6486 - footwear_output_loss: 0.5178 - pose_output_loss: 0.2360 - emotion_output_loss: 0.7580 - gender_output_acc: 0.9230 - image_quality_output_acc: 0.7555 - age_output_acc: 0.4962 - weight_output_acc: 0.7042 - bag_output_acc: 0.6844 - footwear_output_acc: 0.7629 - pose_output_acc: 0.9196 - emotion_output_acc: 0.7394\n",
            "Epoch 00045: val_loss did not improve from 7.32643\n",
            "360/360 [==============================] - 41s 114ms/step - loss: 4.8835 - gender_output_loss: 0.1650 - image_quality_output_loss: 0.6160 - age_output_loss: 1.1599 - weight_output_loss: 0.7819 - bag_output_loss: 0.6483 - footwear_output_loss: 0.5181 - pose_output_loss: 0.2364 - emotion_output_loss: 0.7580 - gender_output_acc: 0.9225 - image_quality_output_acc: 0.7556 - age_output_acc: 0.4964 - weight_output_acc: 0.7041 - bag_output_acc: 0.6847 - footwear_output_acc: 0.7627 - pose_output_acc: 0.9195 - emotion_output_acc: 0.7393 - val_loss: 8.2407 - val_gender_output_loss: 0.7269 - val_image_quality_output_loss: 1.0796 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 1.0319 - val_footwear_output_loss: 1.2158 - val_pose_output_loss: 0.8269 - val_emotion_output_loss: 0.9428 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.5439 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7203 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 7.32643\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.7471 - gender_output_loss: 0.1617 - image_quality_output_loss: 0.6064 - age_output_loss: 1.1370 - weight_output_loss: 0.7558 - bag_output_loss: 0.6276 - footwear_output_loss: 0.4995 - pose_output_loss: 0.2143 - emotion_output_loss: 0.7448 - gender_output_acc: 0.9230 - image_quality_output_acc: 0.7599 - age_output_acc: 0.5070 - weight_output_acc: 0.7094 - bag_output_acc: 0.6985 - footwear_output_acc: 0.7671 - pose_output_acc: 0.9264 - emotion_output_acc: 0.7419 - val_loss: 8.1392 - val_gender_output_loss: 0.6312 - val_image_quality_output_loss: 1.1092 - val_age_output_loss: 1.4425 - val_weight_output_loss: 1.0012 - val_bag_output_loss: 0.9999 - val_footwear_output_loss: 1.1755 - val_pose_output_loss: 0.8353 - val_emotion_output_loss: 0.9445 - val_gender_output_acc: 0.7545 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5842 - val_footwear_output_acc: 0.6391 - val_pose_output_acc: 0.7213 - val_emotion_output_acc: 0.6835\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 7.32643\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 41s 114ms/step - loss: 4.6283 - gender_output_loss: 0.1454 - image_quality_output_loss: 0.5860 - age_output_loss: 1.1093 - weight_output_loss: 0.7442 - bag_output_loss: 0.6037 - footwear_output_loss: 0.4896 - pose_output_loss: 0.2141 - emotion_output_loss: 0.7360 - gender_output_acc: 0.9280 - image_quality_output_acc: 0.7713 - age_output_acc: 0.5217 - weight_output_acc: 0.7093 - bag_output_acc: 0.7045 - footwear_output_acc: 0.7770 - pose_output_acc: 0.9266 - emotion_output_acc: 0.7443 - val_loss: 8.2391 - val_gender_output_loss: 0.6510 - val_image_quality_output_loss: 1.1377 - val_age_output_loss: 1.4495 - val_weight_output_loss: 0.9888 - val_bag_output_loss: 1.0421 - val_footwear_output_loss: 1.2053 - val_pose_output_loss: 0.8213 - val_emotion_output_loss: 0.9436 - val_gender_output_acc: 0.7465 - val_image_quality_output_acc: 0.5378 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7253 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 7.32643\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.6010 - gender_output_loss: 0.1487 - image_quality_output_loss: 0.5893 - age_output_loss: 1.0969 - weight_output_loss: 0.7348 - bag_output_loss: 0.6109 - footwear_output_loss: 0.4827 - pose_output_loss: 0.2035 - emotion_output_loss: 0.7342 - gender_output_acc: 0.9296 - image_quality_output_acc: 0.7676 - age_output_acc: 0.5207 - weight_output_acc: 0.7148 - bag_output_acc: 0.7090 - footwear_output_acc: 0.7774 - pose_output_acc: 0.9280 - emotion_output_acc: 0.7434 - val_loss: 8.3765 - val_gender_output_loss: 0.7149 - val_image_quality_output_loss: 1.1272 - val_age_output_loss: 1.4409 - val_weight_output_loss: 0.9945 - val_bag_output_loss: 1.0264 - val_footwear_output_loss: 1.2702 - val_pose_output_loss: 0.8577 - val_emotion_output_loss: 0.9447 - val_gender_output_acc: 0.7571 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7162 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 7.32643\n",
            "Epoch 50/50\n",
            "360/360 [==============================] - 41s 115ms/step - loss: 4.6219 - gender_output_loss: 0.1456 - image_quality_output_loss: 0.5885 - age_output_loss: 1.1118 - weight_output_loss: 0.7393 - bag_output_loss: 0.6113 - footwear_output_loss: 0.4878 - pose_output_loss: 0.2047 - emotion_output_loss: 0.7329 - gender_output_acc: 0.9283 - image_quality_output_acc: 0.7682 - age_output_acc: 0.5160 - weight_output_acc: 0.7097 - bag_output_acc: 0.6984 - footwear_output_acc: 0.7772 - pose_output_acc: 0.9279 - emotion_output_acc: 0.7450 - val_loss: 8.5021 - val_gender_output_loss: 0.7039 - val_image_quality_output_loss: 1.2034 - val_age_output_loss: 1.4780 - val_weight_output_loss: 1.0139 - val_bag_output_loss: 1.0249 - val_footwear_output_loss: 1.2815 - val_pose_output_loss: 0.8522 - val_emotion_output_loss: 0.9443 - val_gender_output_acc: 0.7434 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7278 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 7.32643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad5c791710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoVUeE3AKqSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model):\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    accuracies = {}\n",
        "    losses = {}\n",
        "    for k, v in zip(model.metrics_names, results):\n",
        "        if k.endswith('acc'):\n",
        "            accuracies[k] = round(v * 100, 4) \n",
        "        else:\n",
        "            losses[k] = v\n",
        "    return accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmpOUgocna1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "359cf957-b042-4b7c-96a3-4970b72e4bbd"
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/bestmodel5.h5')\n",
        "results=model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names,results))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 19s 614ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.36693548387096775,\n",
              " 'age_output_loss': 1.4246500845878356,\n",
              " 'bag_output_acc': 0.579133064516129,\n",
              " 'bag_output_loss': 0.9019580041208575,\n",
              " 'emotion_output_acc': 0.6844758064516129,\n",
              " 'emotion_output_loss': 0.9425292707258656,\n",
              " 'footwear_output_acc': 0.6315524193548387,\n",
              " 'footwear_output_loss': 0.8983782625967457,\n",
              " 'gender_output_acc': 0.7948588709677419,\n",
              " 'gender_output_loss': 0.4983891094884565,\n",
              " 'image_quality_output_acc': 0.5539314516129032,\n",
              " 'image_quality_output_loss': 0.9668408939915318,\n",
              " 'loss': 7.32643078219506,\n",
              " 'pose_output_acc': 0.7167338709677419,\n",
              " 'pose_output_loss': 0.7224929486551592,\n",
              " 'weight_output_acc': 0.6376008064516129,\n",
              " 'weight_output_loss': 0.9711922272559135}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVmRGqdnkwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a1964236-99db-43c4-87f6-d2db405d867f"
      },
      "source": [
        "print(evaluate_model(model))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 11s 348ms/step\n",
            "{'gender_output_acc': 79.4859, 'image_quality_output_acc': 55.3931, 'age_output_acc': 36.6935, 'weight_output_acc': 63.7601, 'bag_output_acc': 57.9133, 'footwear_output_acc': 63.1552, 'pose_output_acc': 71.6734, 'emotion_output_acc': 68.4476}\n"
          ],
          "name": "stdout"
        }
      ]
    }
