{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5(updated).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "8747a469-3e31-4838-d97c-d4846e4449fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWQ8DU0OavsB",
        "colab_type": "code",
        "outputId": "74ebb72f-36ba-4c56-e8a9-5efff0f3ca0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "2eb00771-aa29-4a02-9acd-81c0905c48ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16,InceptionResNetV2,ResNet50\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import Flatten,GlobalAveragePooling2D\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "c52171a9-2d83-472a-d689-879ba8d818e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "3ea75206-6b30-4bda-843b-8e4a9e8ac21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "0d7322cf-2043-48e0-b6c9-2b982e1ffac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15,random_state=1)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "955cddd9-8f05-4622-9070-cfd463259a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resized/59.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>resized/2107.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5206</th>\n",
              "      <td>resized/5207.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>resized/1164.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>resized/13536.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "58        resized/59.jpg              0  ...                        1              0\n",
              "2106    resized/2107.jpg              1  ...                        1              0\n",
              "5206    resized/5207.jpg              0  ...                        1              0\n",
              "1163    resized/1164.jpg              1  ...                        1              0\n",
              "13534  resized/13536.jpg              1  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=ImageDataGenerator(\n",
        "                                                                           horizontal_flip=False, \n",
        "                                                                           vertical_flip=True\n",
        "                                                                           ))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "b01696d2-54a6-4946-ded4-8aeaab2d0a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGN6IC0bSjtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten,GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam,RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7X66PWRSBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "def resnet_v1(input_shape, depth):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "    Input\n",
        "    inputs = Input(shape=input_shape)\n",
        "  \n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    y = GlobalAveragePooling2D(data_format='channels_last')(x)\n",
        "    outputs = Dense(512,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    mode = Model(inputs=inputs, outputs=outputs)\n",
        "    return mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "0905bad5-c319-449e-bcc8-1470914cd7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "backbone = resnet_v1((224,224,3), depth=depth)\n",
        "neck = backbone.output\n",
        "\"\"\"from keras import regularizers\n",
        "neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = GlobalAveragePooling2D(data_format='channels_last')(neck)\n",
        "neck = Dense(64, input_dim=3,\n",
        "         #       kernel_regularizer=regularizers.l2(0.005),\n",
        "                #activity_regularizer=regularizers.l1(0.01),\n",
        "                activation=\"softmax\")(neck)\n",
        "\n",
        "\"\"\"\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "c423f5e4-64cb-4bbc-d227-9f1d003db4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "#opt = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(\n",
        "    optimizer=RMSprop(),\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nv8fdaSbkVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency.\n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored\n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or\n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "\n",
        "    # Example for CIFAR-10 w/ batch size 100:\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    # References\n",
        "\n",
        "      - [Cyclical Learning Rates for Training Neural Networks](\n",
        "      https://arxiv.org/abs/1506.01186)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            base_lr=0.001,\n",
        "            max_lr=0.006,\n",
        "            step_size=2000.,\n",
        "            mode='triangular',\n",
        "            gamma=1.,\n",
        "            scale_fn=None,\n",
        "            scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2',\n",
        "                        'exp_range']:\n",
        "            raise KeyError(\"mode must be one of 'triangular', \"\n",
        "                           \"'triangular2', or 'exp_range'\")\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** x\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr is not None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr is not None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size is not None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "        self.history.setdefault(\n",
        "            'lr', []).append(\n",
        "            K.get_value(\n",
        "                self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EveN99bbmCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_LR=1e-9\n",
        "MAX_LR=1e-3\n",
        "\n",
        "STEP_SIZE=8\n",
        "CLR_METHOD='triangular'\n",
        "NUM_EPOCHS=96\n",
        "\n",
        "\n",
        "clr=CyclicLR(\n",
        "            base_lr=MIN_LR,\n",
        "            max_lr=MAX_LR,\n",
        "            step_size=STEP_SIZE * (train_df.shape[0] // 32),\n",
        "            mode=CLR_METHOD\n",
        ")\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.5),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVa909EHMGth",
        "colab_type": "code",
        "outputId": "0a76e3d7-fec5-47eb-f840-53bb5baf70e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "filepath = '/content/drive/My Drive/bestmodel7.h5'\n",
        "ckpt =ModelCheckpoint(filepath, verbose=2, monitor='val_loss', save_best_only=True,save_weights_only=False, mode='auto')\n",
        "callback = [ckpt,clr,lr_reducer]\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6,\n",
        "    epochs=96,\n",
        "    steps_per_epoch=train_df.shape[0] // 32,\n",
        "    verbose=1,\n",
        "    callbacks=callback\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.0193 - gender_output_loss: 0.4470 - image_quality_output_loss: 0.8503 - age_output_loss: 1.3884 - weight_output_loss: 0.9591 - bag_output_loss: 0.8388 - footwear_output_loss: 0.7263 - pose_output_loss: 0.8902 - emotion_output_loss: 0.8737 - gender_output_acc: 0.7898 - image_quality_output_acc: 0.5988 - age_output_acc: 0.4058 - weight_output_acc: 0.6347 - bag_output_acc: 0.6229 - footwear_output_acc: 0.6871 - pose_output_acc: 0.6212 - emotion_output_acc: 0.7159Epoch 1/96\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 1/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 7.0178 - gender_output_loss: 0.4467 - image_quality_output_loss: 0.8502 - age_output_loss: 1.3887 - weight_output_loss: 0.9593 - bag_output_loss: 0.8384 - footwear_output_loss: 0.7263 - pose_output_loss: 0.8896 - emotion_output_loss: 0.8729 - gender_output_acc: 0.7899 - image_quality_output_acc: 0.5985 - age_output_acc: 0.4055 - weight_output_acc: 0.6345 - bag_output_acc: 0.6232 - footwear_output_acc: 0.6868 - pose_output_acc: 0.6215 - emotion_output_acc: 0.7162 - val_loss: 7.3989 - val_gender_output_loss: 0.5038 - val_image_quality_output_loss: 0.8728 - val_age_output_loss: 1.4255 - val_weight_output_loss: 0.9675 - val_bag_output_loss: 0.8857 - val_footwear_output_loss: 0.8367 - val_pose_output_loss: 0.9186 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.7641 - val_image_quality_output_acc: 0.5822 - val_age_output_acc: 0.3735 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6235 - val_pose_output_acc: 0.6139 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.39887, saving model to /content/drive/My Drive/bestmodel7.h5\n",
            "Epoch 2/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 7.0427 - gender_output_loss: 0.4572 - image_quality_output_loss: 0.8537 - age_output_loss: 1.3897 - weight_output_loss: 0.9610 - bag_output_loss: 0.8395 - footwear_output_loss: 0.7363 - pose_output_loss: 0.8854 - emotion_output_loss: 0.8743 - gender_output_acc: 0.7832 - image_quality_output_acc: 0.5957 - age_output_acc: 0.4035 - weight_output_acc: 0.6345 - bag_output_acc: 0.6268 - footwear_output_acc: 0.6814 - pose_output_acc: 0.6218 - emotion_output_acc: 0.7161 - val_loss: 7.8415 - val_gender_output_loss: 0.7941 - val_image_quality_output_loss: 0.9283 - val_age_output_loss: 1.4399 - val_weight_output_loss: 0.9683 - val_bag_output_loss: 0.9483 - val_footwear_output_loss: 0.8191 - val_pose_output_loss: 0.9330 - val_emotion_output_loss: 0.9650 - val_gender_output_acc: 0.6557 - val_image_quality_output_acc: 0.5696 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 7.39887\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.39887, saving model to /content/drive/My Drive/bestmodel7.h5\n",
            "Epoch 3/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.0586 - gender_output_loss: 0.4604 - image_quality_output_loss: 0.8554 - age_output_loss: 1.3908 - weight_output_loss: 0.9622 - bag_output_loss: 0.8421 - footwear_output_loss: 0.7414 - pose_output_loss: 0.8853 - emotion_output_loss: 0.8752 - gender_output_acc: 0.7800 - image_quality_output_acc: 0.5935 - age_output_acc: 0.4053 - weight_output_acc: 0.6349 - bag_output_acc: 0.6212 - footwear_output_acc: 0.6818 - pose_output_acc: 0.6218 - emotion_output_acc: 0.7158\n",
            "Epoch 00002: val_loss did not improve from 7.39887\n",
            "Epoch 3/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 7.0602 - gender_output_loss: 0.4609 - image_quality_output_loss: 0.8555 - age_output_loss: 1.3912 - weight_output_loss: 0.9626 - bag_output_loss: 0.8423 - footwear_output_loss: 0.7412 - pose_output_loss: 0.8856 - emotion_output_loss: 0.8751 - gender_output_acc: 0.7799 - image_quality_output_acc: 0.5934 - age_output_acc: 0.4050 - weight_output_acc: 0.6346 - bag_output_acc: 0.6211 - footwear_output_acc: 0.6819 - pose_output_acc: 0.6215 - emotion_output_acc: 0.7159 - val_loss: 7.9620 - val_gender_output_loss: 0.8135 - val_image_quality_output_loss: 0.9642 - val_age_output_loss: 1.4379 - val_weight_output_loss: 0.9676 - val_bag_output_loss: 0.8927 - val_footwear_output_loss: 0.9729 - val_pose_output_loss: 0.9061 - val_emotion_output_loss: 0.9612 - val_gender_output_acc: 0.5756 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.5413 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 7.39887\n",
            "Epoch 4/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 7.0717 - gender_output_loss: 0.4666 - image_quality_output_loss: 0.8599 - age_output_loss: 1.3911 - weight_output_loss: 0.9627 - bag_output_loss: 0.8427 - footwear_output_loss: 0.7445 - pose_output_loss: 0.8823 - emotion_output_loss: 0.8754 - gender_output_acc: 0.7753 - image_quality_output_acc: 0.5904 - age_output_acc: 0.4044 - weight_output_acc: 0.6347 - bag_output_acc: 0.6251 - footwear_output_acc: 0.6796 - pose_output_acc: 0.6225 - emotion_output_acc: 0.7162 - val_loss: 9.6556 - val_gender_output_loss: 1.0607 - val_image_quality_output_loss: 0.9891 - val_age_output_loss: 1.5315 - val_weight_output_loss: 1.0255 - val_bag_output_loss: 1.1759 - val_footwear_output_loss: 1.9020 - val_pose_output_loss: 0.9347 - val_emotion_output_loss: 0.9893 - val_gender_output_acc: 0.5675 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.2757 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3841 - val_pose_output_acc: 0.5938 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.39887\n",
            "Epoch 5/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.0850 - gender_output_loss: 0.4714 - image_quality_output_loss: 0.8614 - age_output_loss: 1.3919 - weight_output_loss: 0.9628 - bag_output_loss: 0.8456 - footwear_output_loss: 0.7502 - pose_output_loss: 0.8788 - emotion_output_loss: 0.8752 - gender_output_acc: 0.7731 - image_quality_output_acc: 0.5941 - age_output_acc: 0.4067 - weight_output_acc: 0.6346 - bag_output_acc: 0.6226 - footwear_output_acc: 0.6764 - pose_output_acc: 0.6221 - emotion_output_acc: 0.7163\n",
            "Epoch 00004: val_loss did not improve from 7.39887\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 7.0855 - gender_output_loss: 0.4713 - image_quality_output_loss: 0.8616 - age_output_loss: 1.3918 - weight_output_loss: 0.9623 - bag_output_loss: 0.8455 - footwear_output_loss: 0.7509 - pose_output_loss: 0.8789 - emotion_output_loss: 0.8756 - gender_output_acc: 0.7732 - image_quality_output_acc: 0.5943 - age_output_acc: 0.4067 - weight_output_acc: 0.6347 - bag_output_acc: 0.6225 - footwear_output_acc: 0.6760 - pose_output_acc: 0.6220 - emotion_output_acc: 0.7161 - val_loss: 9.1026 - val_gender_output_loss: 1.5484 - val_image_quality_output_loss: 0.9143 - val_age_output_loss: 1.4702 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9876 - val_footwear_output_loss: 0.9693 - val_pose_output_loss: 1.2053 - val_emotion_output_loss: 0.9733 - val_gender_output_acc: 0.5842 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5691 - val_footwear_output_acc: 0.5680 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.39887\n",
            "Epoch 6/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 7.0875 - gender_output_loss: 0.4705 - image_quality_output_loss: 0.8636 - age_output_loss: 1.3932 - weight_output_loss: 0.9631 - bag_output_loss: 0.8434 - footwear_output_loss: 0.7494 - pose_output_loss: 0.8767 - emotion_output_loss: 0.8778 - gender_output_acc: 0.7714 - image_quality_output_acc: 0.5872 - age_output_acc: 0.4044 - weight_output_acc: 0.6348 - bag_output_acc: 0.6234 - footwear_output_acc: 0.6766 - pose_output_acc: 0.6238 - emotion_output_acc: 0.7159 - val_loss: 9.3968 - val_gender_output_loss: 1.6354 - val_image_quality_output_loss: 1.1197 - val_age_output_loss: 1.5099 - val_weight_output_loss: 1.0383 - val_bag_output_loss: 1.1440 - val_footwear_output_loss: 0.9232 - val_pose_output_loss: 0.9698 - val_emotion_output_loss: 1.0054 - val_gender_output_acc: 0.5922 - val_image_quality_output_acc: 0.4642 - val_age_output_acc: 0.3488 - val_weight_output_acc: 0.6336 - val_bag_output_acc: 0.5524 - val_footwear_output_acc: 0.5927 - val_pose_output_acc: 0.6058 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.39887\n",
            "Epoch 7/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.0823 - gender_output_loss: 0.4700 - image_quality_output_loss: 0.8670 - age_output_loss: 1.3922 - weight_output_loss: 0.9632 - bag_output_loss: 0.8456 - footwear_output_loss: 0.7591 - pose_output_loss: 0.8574 - emotion_output_loss: 0.8748 - gender_output_acc: 0.7753 - image_quality_output_acc: 0.5877 - age_output_acc: 0.4064 - weight_output_acc: 0.6346 - bag_output_acc: 0.6206 - footwear_output_acc: 0.6721 - pose_output_acc: 0.6285 - emotion_output_acc: 0.7160Epoch 7/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 7.0820 - gender_output_loss: 0.4698 - image_quality_output_loss: 0.8666 - age_output_loss: 1.3921 - weight_output_loss: 0.9637 - bag_output_loss: 0.8459 - footwear_output_loss: 0.7588 - pose_output_loss: 0.8577 - emotion_output_loss: 0.8746 - gender_output_acc: 0.7755 - image_quality_output_acc: 0.5879 - age_output_acc: 0.4061 - weight_output_acc: 0.6344 - bag_output_acc: 0.6205 - footwear_output_acc: 0.6721 - pose_output_acc: 0.6282 - emotion_output_acc: 0.7162 - val_loss: 8.0554 - val_gender_output_loss: 0.6398 - val_image_quality_output_loss: 0.9379 - val_age_output_loss: 1.4357 - val_weight_output_loss: 1.0051 - val_bag_output_loss: 0.8899 - val_footwear_output_loss: 1.1711 - val_pose_output_loss: 0.9675 - val_emotion_output_loss: 0.9537 - val_gender_output_acc: 0.6779 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3674 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5691 - val_footwear_output_acc: 0.4808 - val_pose_output_acc: 0.4864 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 7.39887\n",
            "Epoch 8/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 7.0624 - gender_output_loss: 0.4648 - image_quality_output_loss: 0.8692 - age_output_loss: 1.3923 - weight_output_loss: 0.9652 - bag_output_loss: 0.8439 - footwear_output_loss: 0.7643 - pose_output_loss: 0.8308 - emotion_output_loss: 0.8750 - gender_output_acc: 0.7806 - image_quality_output_acc: 0.5879 - age_output_acc: 0.4029 - weight_output_acc: 0.6349 - bag_output_acc: 0.6231 - footwear_output_acc: 0.6668 - pose_output_acc: 0.6380 - emotion_output_acc: 0.7162 - val_loss: 7.8654 - val_gender_output_loss: 0.5776 - val_image_quality_output_loss: 1.1603 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8968 - val_footwear_output_loss: 0.8859 - val_pose_output_loss: 0.9113 - val_emotion_output_loss: 0.9535 - val_gender_output_acc: 0.7238 - val_image_quality_output_acc: 0.4592 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.6144 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.39887\n",
            "Epoch 8/96\n",
            "Epoch 9/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 7.0069 - gender_output_loss: 0.4587 - image_quality_output_loss: 0.8674 - age_output_loss: 1.3923 - weight_output_loss: 0.9657 - bag_output_loss: 0.8449 - footwear_output_loss: 0.7519 - pose_output_loss: 0.7895 - emotion_output_loss: 0.8748 - gender_output_acc: 0.7848 - image_quality_output_acc: 0.5896 - age_output_acc: 0.4052 - weight_output_acc: 0.6345 - bag_output_acc: 0.6194 - footwear_output_acc: 0.6718 - pose_output_acc: 0.6502 - emotion_output_acc: 0.7161 - val_loss: 8.0687 - val_gender_output_loss: 0.6581 - val_image_quality_output_loss: 1.1864 - val_age_output_loss: 1.4262 - val_weight_output_loss: 0.9739 - val_bag_output_loss: 0.8865 - val_footwear_output_loss: 0.9101 - val_pose_output_loss: 0.9946 - val_emotion_output_loss: 0.9692 - val_gender_output_acc: 0.7067 - val_image_quality_output_acc: 0.4163 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5988 - val_footwear_output_acc: 0.5791 - val_pose_output_acc: 0.6331 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.39887\n",
            "Epoch 9/96\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 7.39887\n",
            "Epoch 10/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.8891 - gender_output_loss: 0.4307 - image_quality_output_loss: 0.8601 - age_output_loss: 1.3858 - weight_output_loss: 0.9637 - bag_output_loss: 0.8420 - footwear_output_loss: 0.7435 - pose_output_loss: 0.7316 - emotion_output_loss: 0.8667 - gender_output_acc: 0.7998 - image_quality_output_acc: 0.5955 - age_output_acc: 0.4054 - weight_output_acc: 0.6342 - bag_output_acc: 0.6207 - footwear_output_acc: 0.6784 - pose_output_acc: 0.6777 - emotion_output_acc: 0.7162\n",
            "Epoch 00009: val_loss did not improve from 7.39887\n",
            "\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 6.8896 - gender_output_loss: 0.4304 - image_quality_output_loss: 0.8602 - age_output_loss: 1.3863 - weight_output_loss: 0.9637 - bag_output_loss: 0.8419 - footwear_output_loss: 0.7438 - pose_output_loss: 0.7316 - emotion_output_loss: 0.8665 - gender_output_acc: 0.7998 - image_quality_output_acc: 0.5955 - age_output_acc: 0.4049 - weight_output_acc: 0.6344 - bag_output_acc: 0.6208 - footwear_output_acc: 0.6784 - pose_output_acc: 0.6776 - emotion_output_acc: 0.7162 - val_loss: 7.6275 - val_gender_output_loss: 0.5064 - val_image_quality_output_loss: 0.9079 - val_age_output_loss: 1.4208 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.8966 - val_footwear_output_loss: 0.8356 - val_pose_output_loss: 1.0272 - val_emotion_output_loss: 0.9952 - val_gender_output_acc: 0.7429 - val_image_quality_output_acc: 0.5771 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5761 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.4173 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.39887\n",
            "Epoch 11/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.7830 - gender_output_loss: 0.4002 - image_quality_output_loss: 0.8549 - age_output_loss: 1.3839 - weight_output_loss: 0.9618 - bag_output_loss: 0.8357 - footwear_output_loss: 0.7318 - pose_output_loss: 0.6830 - emotion_output_loss: 0.8642 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5958 - age_output_acc: 0.4058 - weight_output_acc: 0.6348 - bag_output_acc: 0.6272 - footwear_output_acc: 0.6850 - pose_output_acc: 0.7085 - emotion_output_acc: 0.7163 - val_loss: 7.9698 - val_gender_output_loss: 0.6612 - val_image_quality_output_loss: 0.8885 - val_age_output_loss: 1.4301 - val_weight_output_loss: 0.9722 - val_bag_output_loss: 0.8968 - val_footwear_output_loss: 1.3764 - val_pose_output_loss: 0.7348 - val_emotion_output_loss: 0.9415 - val_gender_output_acc: 0.6673 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.4330 - val_pose_output_acc: 0.6885 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.39887\n",
            "Epoch 12/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.6769 - gender_output_loss: 0.3725 - image_quality_output_loss: 0.8488 - age_output_loss: 1.3809 - weight_output_loss: 0.9599 - bag_output_loss: 0.8292 - footwear_output_loss: 0.7189 - pose_output_loss: 0.6377 - emotion_output_loss: 0.8602 - gender_output_acc: 0.8349 - image_quality_output_acc: 0.5997 - age_output_acc: 0.4065 - weight_output_acc: 0.6345 - bag_output_acc: 0.6393 - footwear_output_acc: 0.6914 - pose_output_acc: 0.7332 - emotion_output_acc: 0.7162 - val_loss: 7.3540 - val_gender_output_loss: 0.4602 - val_image_quality_output_loss: 0.8771 - val_age_output_loss: 1.4259 - val_weight_output_loss: 0.9680 - val_bag_output_loss: 0.8915 - val_footwear_output_loss: 0.8878 - val_pose_output_loss: 0.8350 - val_emotion_output_loss: 0.9392 - val_gender_output_acc: 0.7858 - val_image_quality_output_acc: 0.5761 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.6195 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.39887 to 7.35405, saving model to /content/drive/My Drive/bestmodel7.h5\n",
            "Epoch 12/96\n",
            "Epoch 13/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.5636 - gender_output_loss: 0.3433 - image_quality_output_loss: 0.8392 - age_output_loss: 1.3759 - weight_output_loss: 0.9566 - bag_output_loss: 0.8248 - footwear_output_loss: 0.6964 - pose_output_loss: 0.6023 - emotion_output_loss: 0.8555 - gender_output_acc: 0.8492 - image_quality_output_acc: 0.6070 - age_output_acc: 0.4053 - weight_output_acc: 0.6347 - bag_output_acc: 0.6344 - footwear_output_acc: 0.7036 - pose_output_acc: 0.7498 - emotion_output_acc: 0.7161 - val_loss: 7.8807 - val_gender_output_loss: 0.7820 - val_image_quality_output_loss: 0.9610 - val_age_output_loss: 1.4207 - val_weight_output_loss: 0.9698 - val_bag_output_loss: 0.9644 - val_footwear_output_loss: 1.0037 - val_pose_output_loss: 0.7515 - val_emotion_output_loss: 0.9580 - val_gender_output_acc: 0.6855 - val_image_quality_output_acc: 0.5403 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5615 - val_footwear_output_acc: 0.5716 - val_pose_output_acc: 0.6623 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.35405\n",
            "Epoch 14/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.4498 - gender_output_loss: 0.3116 - image_quality_output_loss: 0.8314 - age_output_loss: 1.3691 - weight_output_loss: 0.9541 - bag_output_loss: 0.8163 - footwear_output_loss: 0.6819 - pose_output_loss: 0.5641 - emotion_output_loss: 0.8515 - gender_output_acc: 0.8668 - image_quality_output_acc: 0.6088 - age_output_acc: 0.4076 - weight_output_acc: 0.6341 - bag_output_acc: 0.6441 - footwear_output_acc: 0.7080 - pose_output_acc: 0.7669 - emotion_output_acc: 0.7163Epoch 14/96\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.35405\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.4492 - gender_output_loss: 0.3119 - image_quality_output_loss: 0.8314 - age_output_loss: 1.3690 - weight_output_loss: 0.9535 - bag_output_loss: 0.8158 - footwear_output_loss: 0.6819 - pose_output_loss: 0.5638 - emotion_output_loss: 0.8521 - gender_output_acc: 0.8668 - image_quality_output_acc: 0.6090 - age_output_acc: 0.4076 - weight_output_acc: 0.6343 - bag_output_acc: 0.6440 - footwear_output_acc: 0.7081 - pose_output_acc: 0.7669 - emotion_output_acc: 0.7160 - val_loss: 7.4883 - val_gender_output_loss: 0.6726 - val_image_quality_output_loss: 0.9098 - val_age_output_loss: 1.4447 - val_weight_output_loss: 0.9670 - val_bag_output_loss: 0.8922 - val_footwear_output_loss: 0.8364 - val_pose_output_loss: 0.7475 - val_emotion_output_loss: 0.9484 - val_gender_output_acc: 0.7112 - val_image_quality_output_acc: 0.5640 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5731 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.6542 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.35405\n",
            "Epoch 15/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.3440 - gender_output_loss: 0.2898 - image_quality_output_loss: 0.8198 - age_output_loss: 1.3648 - weight_output_loss: 0.9523 - bag_output_loss: 0.8092 - footwear_output_loss: 0.6595 - pose_output_loss: 0.5318 - emotion_output_loss: 0.8470 - gender_output_acc: 0.8803 - image_quality_output_acc: 0.6154 - age_output_acc: 0.4089 - weight_output_acc: 0.6354 - bag_output_acc: 0.6465 - footwear_output_acc: 0.7174 - pose_output_acc: 0.7878 - emotion_output_acc: 0.7166Epoch 15/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.3445 - gender_output_loss: 0.2897 - image_quality_output_loss: 0.8199 - age_output_loss: 1.3652 - weight_output_loss: 0.9515 - bag_output_loss: 0.8097 - footwear_output_loss: 0.6597 - pose_output_loss: 0.5315 - emotion_output_loss: 0.8477 - gender_output_acc: 0.8804 - image_quality_output_acc: 0.6154 - age_output_acc: 0.4086 - weight_output_acc: 0.6357 - bag_output_acc: 0.6462 - footwear_output_acc: 0.7173 - pose_output_acc: 0.7881 - emotion_output_acc: 0.7163 - val_loss: 8.3816 - val_gender_output_loss: 0.4554 - val_image_quality_output_loss: 0.9400 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.9340 - val_footwear_output_loss: 1.9597 - val_pose_output_loss: 0.6592 - val_emotion_output_loss: 0.9355 - val_gender_output_acc: 0.7807 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5741 - val_footwear_output_acc: 0.4017 - val_pose_output_acc: 0.7061 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.35405\n",
            "Epoch 16/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.2456 - gender_output_loss: 0.2577 - image_quality_output_loss: 0.8130 - age_output_loss: 1.3633 - weight_output_loss: 0.9480 - bag_output_loss: 0.8056 - footwear_output_loss: 0.6453 - pose_output_loss: 0.5004 - emotion_output_loss: 0.8427 - gender_output_acc: 0.8968 - image_quality_output_acc: 0.6201 - age_output_acc: 0.4085 - weight_output_acc: 0.6356 - bag_output_acc: 0.6510 - footwear_output_acc: 0.7252 - pose_output_acc: 0.8016 - emotion_output_acc: 0.7162 - val_loss: 7.0271 - val_gender_output_loss: 0.4422 - val_image_quality_output_loss: 0.8774 - val_age_output_loss: 1.4160 - val_weight_output_loss: 0.9623 - val_bag_output_loss: 0.8821 - val_footwear_output_loss: 0.8290 - val_pose_output_loss: 0.6105 - val_emotion_output_loss: 0.9381 - val_gender_output_acc: 0.8059 - val_image_quality_output_acc: 0.5811 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5978 - val_footwear_output_acc: 0.6477 - val_pose_output_acc: 0.7505 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "\n",
            "Epoch 16/96\n",
            "Epoch 00016: val_loss improved from 7.35405 to 7.02715, saving model to /content/drive/My Drive/bestmodel7.h5\n",
            "Epoch 17/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.2118 - gender_output_loss: 0.2491 - image_quality_output_loss: 0.8100 - age_output_loss: 1.3611 - weight_output_loss: 0.9465 - bag_output_loss: 0.8042 - footwear_output_loss: 0.6356 - pose_output_loss: 0.4939 - emotion_output_loss: 0.8420 - gender_output_acc: 0.8997 - image_quality_output_acc: 0.6239 - age_output_acc: 0.4114 - weight_output_acc: 0.6356 - bag_output_acc: 0.6510 - footwear_output_acc: 0.7303 - pose_output_acc: 0.8076 - emotion_output_acc: 0.7161 - val_loss: 7.3401 - val_gender_output_loss: 0.5680 - val_image_quality_output_loss: 0.9113 - val_age_output_loss: 1.4212 - val_weight_output_loss: 0.9619 - val_bag_output_loss: 0.9088 - val_footwear_output_loss: 0.8706 - val_pose_output_loss: 0.6661 - val_emotion_output_loss: 0.9627 - val_gender_output_acc: 0.7596 - val_image_quality_output_acc: 0.5746 - val_age_output_acc: 0.3826 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6361 - val_pose_output_acc: 0.7193 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.02715\n",
            "Epoch 18/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.2640 - gender_output_loss: 0.2653 - image_quality_output_loss: 0.8133 - age_output_loss: 1.3607 - weight_output_loss: 0.9469 - bag_output_loss: 0.8055 - footwear_output_loss: 0.6470 - pose_output_loss: 0.5114 - emotion_output_loss: 0.8444 - gender_output_acc: 0.8916 - image_quality_output_acc: 0.6206 - age_output_acc: 0.4093 - weight_output_acc: 0.6357 - bag_output_acc: 0.6497 - footwear_output_acc: 0.7254 - pose_output_acc: 0.7953 - emotion_output_acc: 0.7161 - val_loss: 7.7748 - val_gender_output_loss: 0.5182 - val_image_quality_output_loss: 0.9040 - val_age_output_loss: 1.4148 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8789 - val_footwear_output_loss: 1.0665 - val_pose_output_loss: 0.9462 - val_emotion_output_loss: 0.9997 - val_gender_output_acc: 0.7732 - val_image_quality_output_acc: 0.5741 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.5565 - val_pose_output_acc: 0.5348 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.02715\n",
            "Epoch 19/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.3006 - gender_output_loss: 0.2811 - image_quality_output_loss: 0.8190 - age_output_loss: 1.3611 - weight_output_loss: 0.9493 - bag_output_loss: 0.8071 - footwear_output_loss: 0.6575 - pose_output_loss: 0.5118 - emotion_output_loss: 0.8441 - gender_output_acc: 0.8813 - image_quality_output_acc: 0.6181 - age_output_acc: 0.4090 - weight_output_acc: 0.6352 - bag_output_acc: 0.6499 - footwear_output_acc: 0.7193 - pose_output_acc: 0.7960 - emotion_output_acc: 0.7164 - val_loss: 8.0151 - val_gender_output_loss: 0.7239 - val_image_quality_output_loss: 1.1369 - val_age_output_loss: 1.4315 - val_weight_output_loss: 0.9625 - val_bag_output_loss: 0.9059 - val_footwear_output_loss: 0.8470 - val_pose_output_loss: 0.9937 - val_emotion_output_loss: 0.9438 - val_gender_output_acc: 0.6830 - val_image_quality_output_acc: 0.4859 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5499 - val_footwear_output_acc: 0.6321 - val_pose_output_acc: 0.6648 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.02715\n",
            "Epoch 20/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.3256 - gender_output_loss: 0.2864 - image_quality_output_loss: 0.8189 - age_output_loss: 1.3630 - weight_output_loss: 0.9491 - bag_output_loss: 0.8101 - footwear_output_loss: 0.6638 - pose_output_loss: 0.5189 - emotion_output_loss: 0.8452 - gender_output_acc: 0.8800 - image_quality_output_acc: 0.6165 - age_output_acc: 0.4113 - weight_output_acc: 0.6373 - bag_output_acc: 0.6453 - footwear_output_acc: 0.7174 - pose_output_acc: 0.7905 - emotion_output_acc: 0.7160 - val_loss: 8.3705 - val_gender_output_loss: 1.1060 - val_image_quality_output_loss: 1.1342 - val_age_output_loss: 1.5146 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 0.9594 - val_footwear_output_loss: 0.8744 - val_pose_output_loss: 0.7699 - val_emotion_output_loss: 0.9698 - val_gender_output_acc: 0.5650 - val_image_quality_output_acc: 0.4531 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.4773 - val_footwear_output_acc: 0.5867 - val_pose_output_acc: 0.7248 - val_emotion_output_acc: 0.6724\n",
            "Epoch 20/96\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 7.02715\n",
            "Epoch 21/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.3526 - gender_output_loss: 0.2950 - image_quality_output_loss: 0.8216 - age_output_loss: 1.3613 - weight_output_loss: 0.9495 - bag_output_loss: 0.8109 - footwear_output_loss: 0.6688 - pose_output_loss: 0.5255 - emotion_output_loss: 0.8484 - gender_output_acc: 0.8746 - image_quality_output_acc: 0.6143 - age_output_acc: 0.4082 - weight_output_acc: 0.6349 - bag_output_acc: 0.6459 - footwear_output_acc: 0.7141 - pose_output_acc: 0.7866 - emotion_output_acc: 0.7158\n",
            "Epoch 00020: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.3523 - gender_output_loss: 0.2951 - image_quality_output_loss: 0.8214 - age_output_loss: 1.3613 - weight_output_loss: 0.9493 - bag_output_loss: 0.8115 - footwear_output_loss: 0.6692 - pose_output_loss: 0.5253 - emotion_output_loss: 0.8476 - gender_output_acc: 0.8746 - image_quality_output_acc: 0.6144 - age_output_acc: 0.4078 - weight_output_acc: 0.6350 - bag_output_acc: 0.6458 - footwear_output_acc: 0.7135 - pose_output_acc: 0.7867 - emotion_output_acc: 0.7162 - val_loss: 12.3277 - val_gender_output_loss: 0.7870 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.5375 - val_weight_output_loss: 1.0615 - val_bag_output_loss: 1.1958 - val_footwear_output_loss: 4.5375 - val_pose_output_loss: 1.1472 - val_emotion_output_loss: 1.0167 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3034 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5469 - val_footwear_output_acc: 0.3649 - val_pose_output_acc: 0.4446 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 7.02715\n",
            "Epoch 22/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.3909 - gender_output_loss: 0.3179 - image_quality_output_loss: 0.8258 - age_output_loss: 1.3603 - weight_output_loss: 0.9482 - bag_output_loss: 0.8145 - footwear_output_loss: 0.6681 - pose_output_loss: 0.5344 - emotion_output_loss: 0.8478 - gender_output_acc: 0.8631 - image_quality_output_acc: 0.6139 - age_output_acc: 0.4086 - weight_output_acc: 0.6357 - bag_output_acc: 0.6438 - footwear_output_acc: 0.7103 - pose_output_acc: 0.7785 - emotion_output_acc: 0.7166 - val_loss: 7.5314 - val_gender_output_loss: 0.5100 - val_image_quality_output_loss: 0.9137 - val_age_output_loss: 1.4164 - val_weight_output_loss: 0.9571 - val_bag_output_loss: 0.8904 - val_footwear_output_loss: 0.9364 - val_pose_output_loss: 0.8815 - val_emotion_output_loss: 0.9505 - val_gender_output_acc: 0.7581 - val_image_quality_output_acc: 0.5711 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5973 - val_footwear_output_acc: 0.5897 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.6845\n",
            "Epoch 22/96\n",
            "Epoch 00022: val_loss did not improve from 7.02715\n",
            "Epoch 23/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.4485 - gender_output_loss: 0.3335 - image_quality_output_loss: 0.8319 - age_output_loss: 1.3654 - weight_output_loss: 0.9486 - bag_output_loss: 0.8165 - footwear_output_loss: 0.6837 - pose_output_loss: 0.5458 - emotion_output_loss: 0.8458 - gender_output_acc: 0.8552 - image_quality_output_acc: 0.6060 - age_output_acc: 0.4069 - weight_output_acc: 0.6369 - bag_output_acc: 0.6448 - footwear_output_acc: 0.7054 - pose_output_acc: 0.7749 - emotion_output_acc: 0.7164 - val_loss: 8.6681 - val_gender_output_loss: 0.6732 - val_image_quality_output_loss: 1.0854 - val_age_output_loss: 1.4311 - val_weight_output_loss: 0.9913 - val_bag_output_loss: 0.8896 - val_footwear_output_loss: 1.5635 - val_pose_output_loss: 0.9157 - val_emotion_output_loss: 1.0389 - val_gender_output_acc: 0.6603 - val_image_quality_output_acc: 0.5060 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5847 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.5887 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 7.02715Epoch 23/96\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 7.02715\n",
            "Epoch 24/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.4799 - gender_output_loss: 0.3426 - image_quality_output_loss: 0.8366 - age_output_loss: 1.3655 - weight_output_loss: 0.9476 - bag_output_loss: 0.8174 - footwear_output_loss: 0.6895 - pose_output_loss: 0.5479 - emotion_output_loss: 0.8511 - gender_output_acc: 0.8485 - image_quality_output_acc: 0.6049 - age_output_acc: 0.4066 - weight_output_acc: 0.6349 - bag_output_acc: 0.6450 - footwear_output_acc: 0.6998 - pose_output_acc: 0.7749 - emotion_output_acc: 0.7162 - val_loss: 8.2624 - val_gender_output_loss: 0.5731 - val_image_quality_output_loss: 1.1975 - val_age_output_loss: 1.4449 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.9299 - val_footwear_output_loss: 0.8577 - val_pose_output_loss: 1.2323 - val_emotion_output_loss: 0.9485 - val_gender_output_acc: 0.7314 - val_image_quality_output_acc: 0.4375 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.4955 - val_emotion_output_acc: 0.6820\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.02715\n",
            "Epoch 25/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 6.4637 - gender_output_loss: 0.3399 - image_quality_output_loss: 0.8335 - age_output_loss: 1.3637 - weight_output_loss: 0.9469 - bag_output_loss: 0.8182 - footwear_output_loss: 0.6867 - pose_output_loss: 0.5408 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8510 - image_quality_output_acc: 0.6102 - age_output_acc: 0.4082 - weight_output_acc: 0.6362 - bag_output_acc: 0.6386 - footwear_output_acc: 0.7044 - pose_output_acc: 0.7807 - emotion_output_acc: 0.7161 - val_loss: 9.3343 - val_gender_output_loss: 2.0399 - val_image_quality_output_loss: 0.9606 - val_age_output_loss: 1.5480 - val_weight_output_loss: 0.9916 - val_bag_output_loss: 1.0990 - val_footwear_output_loss: 0.9286 - val_pose_output_loss: 0.6900 - val_emotion_output_loss: 0.9877 - val_gender_output_acc: 0.4456 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3644 - val_footwear_output_acc: 0.4884 - val_pose_output_acc: 0.7208 - val_emotion_output_acc: 0.6709\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 7.02715\n",
            "Epoch 26/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 6.3342 - gender_output_loss: 0.2997 - image_quality_output_loss: 0.8250 - age_output_loss: 1.3552 - weight_output_loss: 0.9427 - bag_output_loss: 0.8116 - footwear_output_loss: 0.6682 - pose_output_loss: 0.4966 - emotion_output_loss: 0.8445 - gender_output_acc: 0.8704 - image_quality_output_acc: 0.6140 - age_output_acc: 0.4126 - weight_output_acc: 0.6361 - bag_output_acc: 0.6464 - footwear_output_acc: 0.7082 - pose_output_acc: 0.7982 - emotion_output_acc: 0.7163 - val_loss: 7.6957 - val_gender_output_loss: 0.6441 - val_image_quality_output_loss: 0.9051 - val_age_output_loss: 1.4178 - val_weight_output_loss: 0.9628 - val_bag_output_loss: 0.9591 - val_footwear_output_loss: 0.8344 - val_pose_output_loss: 0.8969 - val_emotion_output_loss: 0.9834 - val_gender_output_acc: 0.7228 - val_image_quality_output_acc: 0.5746 - val_age_output_acc: 0.3826 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.6321 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 7.02715\n",
            "Epoch 27/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 6.2003 - gender_output_loss: 0.2699 - image_quality_output_loss: 0.8064 - age_output_loss: 1.3461 - weight_output_loss: 0.9382 - bag_output_loss: 0.8021 - footwear_output_loss: 0.6447 - pose_output_loss: 0.4611 - emotion_output_loss: 0.8389 - gender_output_acc: 0.8881 - image_quality_output_acc: 0.6262 - age_output_acc: 0.4111 - weight_output_acc: 0.6363 - bag_output_acc: 0.6484 - footwear_output_acc: 0.7253 - pose_output_acc: 0.8155 - emotion_output_acc: 0.7162 - val_loss: 8.2574 - val_gender_output_loss: 0.8265 - val_image_quality_output_loss: 1.3292 - val_age_output_loss: 1.4536 - val_weight_output_loss: 1.0071 - val_bag_output_loss: 0.9743 - val_footwear_output_loss: 0.9234 - val_pose_output_loss: 0.6634 - val_emotion_output_loss: 0.9860 - val_gender_output_acc: 0.6648 - val_image_quality_output_acc: 0.4068 - val_age_output_acc: 0.3679 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.4950 - val_footwear_output_acc: 0.5418 - val_pose_output_acc: 0.7122 - val_emotion_output_acc: 0.6689\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 7.02715\n",
            "Epoch 28/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 6.0738 - gender_output_loss: 0.2487 - image_quality_output_loss: 0.7961 - age_output_loss: 1.3373 - weight_output_loss: 0.9318 - bag_output_loss: 0.7954 - footwear_output_loss: 0.6212 - pose_output_loss: 0.4188 - emotion_output_loss: 0.8298 - gender_output_acc: 0.8957 - image_quality_output_acc: 0.6340 - age_output_acc: 0.4187 - weight_output_acc: 0.6386 - bag_output_acc: 0.6549 - footwear_output_acc: 0.7319 - pose_output_acc: 0.8337 - emotion_output_acc: 0.7164 - val_loss: 7.9945 - val_gender_output_loss: 0.5342 - val_image_quality_output_loss: 1.3450 - val_age_output_loss: 1.4248 - val_weight_output_loss: 0.9733 - val_bag_output_loss: 0.9058 - val_footwear_output_loss: 0.9127 - val_pose_output_loss: 0.8342 - val_emotion_output_loss: 0.9694 - val_gender_output_acc: 0.7525 - val_image_quality_output_acc: 0.4143 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5781 - val_footwear_output_acc: 0.5907 - val_pose_output_acc: 0.6462 - val_emotion_output_acc: 0.6694\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 28/96\n",
            "Epoch 29/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.9201 - gender_output_loss: 0.2096 - image_quality_output_loss: 0.7785 - age_output_loss: 1.3279 - weight_output_loss: 0.9238 - bag_output_loss: 0.7885 - footwear_output_loss: 0.5932 - pose_output_loss: 0.3798 - emotion_output_loss: 0.8236 - gender_output_acc: 0.9158 - image_quality_output_acc: 0.6382 - age_output_acc: 0.4179 - weight_output_acc: 0.6415 - bag_output_acc: 0.6594 - footwear_output_acc: 0.7497 - pose_output_acc: 0.8515 - emotion_output_acc: 0.7167Epoch 29/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 5.9230 - gender_output_loss: 0.2092 - image_quality_output_loss: 0.7780 - age_output_loss: 1.3279 - weight_output_loss: 0.9246 - bag_output_loss: 0.7887 - footwear_output_loss: 0.5937 - pose_output_loss: 0.3820 - emotion_output_loss: 0.8236 - gender_output_acc: 0.9161 - image_quality_output_acc: 0.6385 - age_output_acc: 0.4176 - weight_output_acc: 0.6410 - bag_output_acc: 0.6591 - footwear_output_acc: 0.7494 - pose_output_acc: 0.8508 - emotion_output_acc: 0.7167 - val_loss: 8.4398 - val_gender_output_loss: 0.6189 - val_image_quality_output_loss: 0.9302 - val_age_output_loss: 1.4168 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.8852 - val_footwear_output_loss: 0.8941 - val_pose_output_loss: 1.6773 - val_emotion_output_loss: 0.9344 - val_gender_output_acc: 0.7520 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5988 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.3765 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 7.02715\n",
            "Epoch 30/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.7850 - gender_output_loss: 0.1836 - image_quality_output_loss: 0.7585 - age_output_loss: 1.3205 - weight_output_loss: 0.9179 - bag_output_loss: 0.7809 - footwear_output_loss: 0.5631 - pose_output_loss: 0.3423 - emotion_output_loss: 0.8228 - gender_output_acc: 0.9275 - image_quality_output_acc: 0.6511 - age_output_acc: 0.4205 - weight_output_acc: 0.6397 - bag_output_acc: 0.6642 - footwear_output_acc: 0.7617 - pose_output_acc: 0.8689 - emotion_output_acc: 0.7162Epoch 30/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 5.7856 - gender_output_loss: 0.1841 - image_quality_output_loss: 0.7587 - age_output_loss: 1.3203 - weight_output_loss: 0.9180 - bag_output_loss: 0.7808 - footwear_output_loss: 0.5629 - pose_output_loss: 0.3428 - emotion_output_loss: 0.8225 - gender_output_acc: 0.9274 - image_quality_output_acc: 0.6512 - age_output_acc: 0.4201 - weight_output_acc: 0.6398 - bag_output_acc: 0.6641 - footwear_output_acc: 0.7616 - pose_output_acc: 0.8687 - emotion_output_acc: 0.7162 - val_loss: 7.8906 - val_gender_output_loss: 0.5915 - val_image_quality_output_loss: 0.9410 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.8894 - val_footwear_output_loss: 0.9431 - val_pose_output_loss: 1.0771 - val_emotion_output_loss: 0.9307 - val_gender_output_acc: 0.7888 - val_image_quality_output_acc: 0.5630 - val_age_output_acc: 0.3755 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6280 - val_pose_output_acc: 0.7041 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 7.02715\n",
            "Epoch 31/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 5.6344 - gender_output_loss: 0.1635 - image_quality_output_loss: 0.7371 - age_output_loss: 1.3066 - weight_output_loss: 0.9093 - bag_output_loss: 0.7746 - footwear_output_loss: 0.5374 - pose_output_loss: 0.2977 - emotion_output_loss: 0.8128 - gender_output_acc: 0.9364 - image_quality_output_acc: 0.6638 - age_output_acc: 0.4280 - weight_output_acc: 0.6444 - bag_output_acc: 0.6667 - footwear_output_acc: 0.7723 - pose_output_acc: 0.8919 - emotion_output_acc: 0.7167 - val_loss: 7.4899 - val_gender_output_loss: 0.5570 - val_image_quality_output_loss: 0.9404 - val_age_output_loss: 1.4393 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8839 - val_footwear_output_loss: 0.9580 - val_pose_output_loss: 0.6973 - val_emotion_output_loss: 0.9433 - val_gender_output_acc: 0.7878 - val_image_quality_output_acc: 0.5716 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6048 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.7601 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 7.02715\n",
            "Epoch 32/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.5030 - gender_output_loss: 0.1298 - image_quality_output_loss: 0.7173 - age_output_loss: 1.2979 - weight_output_loss: 0.9056 - bag_output_loss: 0.7654 - footwear_output_loss: 0.5163 - pose_output_loss: 0.2671 - emotion_output_loss: 0.8083 - gender_output_acc: 0.9527 - image_quality_output_acc: 0.6734 - age_output_acc: 0.4323 - weight_output_acc: 0.6457 - bag_output_acc: 0.6710 - footwear_output_acc: 0.7848 - pose_output_acc: 0.9029 - emotion_output_acc: 0.7174\n",
            "Epoch 00031: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 5.5023 - gender_output_loss: 0.1299 - image_quality_output_loss: 0.7171 - age_output_loss: 1.2980 - weight_output_loss: 0.9052 - bag_output_loss: 0.7653 - footwear_output_loss: 0.5159 - pose_output_loss: 0.2669 - emotion_output_loss: 0.8087 - gender_output_acc: 0.9527 - image_quality_output_acc: 0.6735 - age_output_acc: 0.4321 - weight_output_acc: 0.6457 - bag_output_acc: 0.6711 - footwear_output_acc: 0.7851 - pose_output_acc: 0.9030 - emotion_output_acc: 0.7173 - val_loss: 7.5039 - val_gender_output_loss: 0.5639 - val_image_quality_output_loss: 0.9507 - val_age_output_loss: 1.4391 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8938 - val_footwear_output_loss: 0.9590 - val_pose_output_loss: 0.6753 - val_emotion_output_loss: 0.9561 - val_gender_output_acc: 0.7873 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.3800 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.7384 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 7.02715\n",
            "Epoch 33/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 5.4571 - gender_output_loss: 0.1252 - image_quality_output_loss: 0.7052 - age_output_loss: 1.2963 - weight_output_loss: 0.9005 - bag_output_loss: 0.7654 - footwear_output_loss: 0.5067 - pose_output_loss: 0.2565 - emotion_output_loss: 0.8060 - gender_output_acc: 0.9530 - image_quality_output_acc: 0.6830 - age_output_acc: 0.4339 - weight_output_acc: 0.6486 - bag_output_acc: 0.6701 - footwear_output_acc: 0.7876 - pose_output_acc: 0.9089 - emotion_output_acc: 0.7181 - val_loss: 7.7799 - val_gender_output_loss: 0.6417 - val_image_quality_output_loss: 1.0040 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.8993 - val_footwear_output_loss: 0.9986 - val_pose_output_loss: 0.7866 - val_emotion_output_loss: 0.9392 - val_gender_output_acc: 0.7727 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6351 - val_bag_output_acc: 0.6043 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.7424 - val_emotion_output_acc: 0.6840\n",
            "Epoch 33/96\n",
            "360/360 [==============================]\n",
            "Epoch 00033: val_loss did not improve from 7.02715\n",
            "Epoch 34/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 5.5252 - gender_output_loss: 0.1378 - image_quality_output_loss: 0.7186 - age_output_loss: 1.2984 - weight_output_loss: 0.9037 - bag_output_loss: 0.7700 - footwear_output_loss: 0.5204 - pose_output_loss: 0.2742 - emotion_output_loss: 0.8070 - gender_output_acc: 0.9465 - image_quality_output_acc: 0.6732 - age_output_acc: 0.4319 - weight_output_acc: 0.6484 - bag_output_acc: 0.6691 - footwear_output_acc: 0.7787 - pose_output_acc: 0.8970 - emotion_output_acc: 0.7178 - val_loss: 8.0681 - val_gender_output_loss: 0.6449 - val_image_quality_output_loss: 1.1926 - val_age_output_loss: 1.4600 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8982 - val_footwear_output_loss: 1.0024 - val_pose_output_loss: 0.8387 - val_emotion_output_loss: 0.9518 - val_gender_output_acc: 0.7636 - val_image_quality_output_acc: 0.4864 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6265 - val_pose_output_acc: 0.7208 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.02715\n",
            "Epoch 35/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 5.5850 - gender_output_loss: 0.1594 - image_quality_output_loss: 0.7235 - age_output_loss: 1.2975 - weight_output_loss: 0.9058 - bag_output_loss: 0.7750 - footwear_output_loss: 0.5273 - pose_output_loss: 0.2917 - emotion_output_loss: 0.8097 - gender_output_acc: 0.9391 - image_quality_output_acc: 0.6673 - age_output_acc: 0.4308 - weight_output_acc: 0.6438 - bag_output_acc: 0.6643 - footwear_output_acc: 0.7767 - pose_output_acc: 0.8900 - emotion_output_acc: 0.7179 - val_loss: 8.2128 - val_gender_output_loss: 1.1149 - val_image_quality_output_loss: 1.0130 - val_age_output_loss: 1.4336 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.8905 - val_footwear_output_loss: 0.9593 - val_pose_output_loss: 0.7667 - val_emotion_output_loss: 0.9541 - val_gender_output_acc: 0.6704 - val_image_quality_output_acc: 0.5444 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7051 - val_emotion_output_acc: 0.6835\n",
            "Epoch 35/96\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.02715\n",
            "Epoch 36/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6605 - gender_output_loss: 0.1780 - image_quality_output_loss: 0.7303 - age_output_loss: 1.3041 - weight_output_loss: 0.9107 - bag_output_loss: 0.7751 - footwear_output_loss: 0.5408 - pose_output_loss: 0.3128 - emotion_output_loss: 0.8130 - gender_output_acc: 0.9304 - image_quality_output_acc: 0.6659 - age_output_acc: 0.4293 - weight_output_acc: 0.6460 - bag_output_acc: 0.6648 - footwear_output_acc: 0.7685 - pose_output_acc: 0.8803 - emotion_output_acc: 0.7174\n",
            "Epoch 00035: val_loss did not improve from 7.02715\n",
            "Epoch 36/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.6619 - gender_output_loss: 0.1782 - image_quality_output_loss: 0.7308 - age_output_loss: 1.3040 - weight_output_loss: 0.9112 - bag_output_loss: 0.7750 - footwear_output_loss: 0.5411 - pose_output_loss: 0.3128 - emotion_output_loss: 0.8131 - gender_output_acc: 0.9302 - image_quality_output_acc: 0.6655 - age_output_acc: 0.4295 - weight_output_acc: 0.6457 - bag_output_acc: 0.6650 - footwear_output_acc: 0.7685 - pose_output_acc: 0.8801 - emotion_output_acc: 0.7174 - val_loss: 8.4168 - val_gender_output_loss: 0.7330 - val_image_quality_output_loss: 1.2294 - val_age_output_loss: 1.4314 - val_weight_output_loss: 0.9679 - val_bag_output_loss: 0.9002 - val_footwear_output_loss: 1.0706 - val_pose_output_loss: 1.0408 - val_emotion_output_loss: 0.9474 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.4657 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6356 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.5988 - val_pose_output_acc: 0.7162 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 7.02715\n",
            "Epoch 37/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 5.7379 - gender_output_loss: 0.1927 - image_quality_output_loss: 0.7354 - age_output_loss: 1.3104 - weight_output_loss: 0.9123 - bag_output_loss: 0.7800 - footwear_output_loss: 0.5564 - pose_output_loss: 0.3350 - emotion_output_loss: 0.8185 - gender_output_acc: 0.9221 - image_quality_output_acc: 0.6604 - age_output_acc: 0.4268 - weight_output_acc: 0.6449 - bag_output_acc: 0.6617 - footwear_output_acc: 0.7617 - pose_output_acc: 0.8720 - emotion_output_acc: 0.7167 - val_loss: 8.7081 - val_gender_output_loss: 0.6030 - val_image_quality_output_loss: 1.1218 - val_age_output_loss: 1.4906 - val_weight_output_loss: 1.0023 - val_bag_output_loss: 0.9814 - val_footwear_output_loss: 1.0686 - val_pose_output_loss: 1.2401 - val_emotion_output_loss: 1.1023 - val_gender_output_acc: 0.7792 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6069 - val_pose_output_acc: 0.5801 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 7.02715\n",
            "Epoch 38/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.8236 - gender_output_loss: 0.2064 - image_quality_output_loss: 0.7448 - age_output_loss: 1.3149 - weight_output_loss: 0.9163 - bag_output_loss: 0.7833 - footwear_output_loss: 0.5804 - pose_output_loss: 0.3592 - emotion_output_loss: 0.8190 - gender_output_acc: 0.9145 - image_quality_output_acc: 0.6592 - age_output_acc: 0.4245 - weight_output_acc: 0.6424 - bag_output_acc: 0.6593 - footwear_output_acc: 0.7523 - pose_output_acc: 0.8583 - emotion_output_acc: 0.7161 - val_loss: 10.5270 - val_gender_output_loss: 0.9403 - val_image_quality_output_loss: 1.0163 - val_age_output_loss: 1.9253 - val_weight_output_loss: 1.0820 - val_bag_output_loss: 1.0083 - val_footwear_output_loss: 0.9870 - val_pose_output_loss: 2.2037 - val_emotion_output_loss: 1.2635 - val_gender_output_acc: 0.6613 - val_image_quality_output_acc: 0.5398 - val_age_output_acc: 0.3422 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5035 - val_footwear_output_acc: 0.5660 - val_pose_output_acc: 0.3337 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 7.02715\n",
            "Epoch 39/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 5.9250 - gender_output_loss: 0.2369 - image_quality_output_loss: 0.7594 - age_output_loss: 1.3199 - weight_output_loss: 0.9159 - bag_output_loss: 0.7904 - footwear_output_loss: 0.5951 - pose_output_loss: 0.3805 - emotion_output_loss: 0.8243 - gender_output_acc: 0.9023 - image_quality_output_acc: 0.6497 - age_output_acc: 0.4233 - weight_output_acc: 0.6416 - bag_output_acc: 0.6550 - footwear_output_acc: 0.7457 - pose_output_acc: 0.8522 - emotion_output_acc: 0.7171 - val_loss: 12.1110 - val_gender_output_loss: 0.6644 - val_image_quality_output_loss: 2.3440 - val_age_output_loss: 1.6065 - val_weight_output_loss: 1.2873 - val_bag_output_loss: 1.0146 - val_footwear_output_loss: 1.8465 - val_pose_output_loss: 2.0572 - val_emotion_output_loss: 1.1857 - val_gender_output_acc: 0.6895 - val_image_quality_output_acc: 0.3140 - val_age_output_acc: 0.2918 - val_weight_output_acc: 0.3906 - val_bag_output_acc: 0.4425 - val_footwear_output_acc: 0.2646 - val_pose_output_acc: 0.6038 - val_emotion_output_acc: 0.3866\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.02715\n",
            "Epoch 40/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 6.0242 - gender_output_loss: 0.2480 - image_quality_output_loss: 0.7729 - age_output_loss: 1.3230 - weight_output_loss: 0.9207 - bag_output_loss: 0.8001 - footwear_output_loss: 0.6194 - pose_output_loss: 0.4051 - emotion_output_loss: 0.8277 - gender_output_acc: 0.8953 - image_quality_output_acc: 0.6402 - age_output_acc: 0.4221 - weight_output_acc: 0.6417 - bag_output_acc: 0.6484 - footwear_output_acc: 0.7367 - pose_output_acc: 0.8385 - emotion_output_acc: 0.7170 - val_loss: 11.5254 - val_gender_output_loss: 2.2180 - val_image_quality_output_loss: 2.2650 - val_age_output_loss: 1.5658 - val_weight_output_loss: 1.1481 - val_bag_output_loss: 1.0593 - val_footwear_output_loss: 1.1448 - val_pose_output_loss: 0.9845 - val_emotion_output_loss: 1.0303 - val_gender_output_acc: 0.5847 - val_image_quality_output_acc: 0.2923 - val_age_output_acc: 0.3095 - val_weight_output_acc: 0.5282 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.5010 - val_pose_output_acc: 0.6391 - val_emotion_output_acc: 0.6245\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.02715\n",
            "Epoch 41/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 6.0082 - gender_output_loss: 0.2487 - image_quality_output_loss: 0.7720 - age_output_loss: 1.3226 - weight_output_loss: 0.9206 - bag_output_loss: 0.7949 - footwear_output_loss: 0.6111 - pose_output_loss: 0.4033 - emotion_output_loss: 0.8230 - gender_output_acc: 0.8978 - image_quality_output_acc: 0.6414 - age_output_acc: 0.4183 - weight_output_acc: 0.6413 - bag_output_acc: 0.6523 - footwear_output_acc: 0.7387 - pose_output_acc: 0.8384 - emotion_output_acc: 0.7159 - val_loss: 10.5191 - val_gender_output_loss: 1.4433 - val_image_quality_output_loss: 1.0171 - val_age_output_loss: 1.4526 - val_weight_output_loss: 1.0067 - val_bag_output_loss: 0.9278 - val_footwear_output_loss: 0.9935 - val_pose_output_loss: 2.5745 - val_emotion_output_loss: 0.9892 - val_gender_output_acc: 0.6467 - val_image_quality_output_acc: 0.5378 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5907 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6074 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.6809\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 7.02715\n",
            "Epoch 42/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.8627 - gender_output_loss: 0.2191 - image_quality_output_loss: 0.7530 - age_output_loss: 1.3094 - weight_output_loss: 0.9110 - bag_output_loss: 0.7895 - footwear_output_loss: 0.5839 - pose_output_loss: 0.3616 - emotion_output_loss: 0.8191 - gender_output_acc: 0.9083 - image_quality_output_acc: 0.6536 - age_output_acc: 0.4240 - weight_output_acc: 0.6416 - bag_output_acc: 0.6558 - footwear_output_acc: 0.7487 - pose_output_acc: 0.8569 - emotion_output_acc: 0.7164 - val_loss: 8.5519 - val_gender_output_loss: 0.6293 - val_image_quality_output_loss: 1.0087 - val_age_output_loss: 1.4877 - val_weight_output_loss: 1.0008 - val_bag_output_loss: 0.9151 - val_footwear_output_loss: 1.4774 - val_pose_output_loss: 0.9476 - val_emotion_output_loss: 0.9676 - val_gender_output_acc: 0.7530 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6336 - val_bag_output_acc: 0.5640 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.7026 - val_emotion_output_acc: 0.6794\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.02715\n",
            "Epoch 43/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.6952 - gender_output_loss: 0.1918 - image_quality_output_loss: 0.7235 - age_output_loss: 1.2943 - weight_output_loss: 0.9023 - bag_output_loss: 0.7791 - footwear_output_loss: 0.5584 - pose_output_loss: 0.3135 - emotion_output_loss: 0.8137 - gender_output_acc: 0.9197 - image_quality_output_acc: 0.6654 - age_output_acc: 0.4317 - weight_output_acc: 0.6462 - bag_output_acc: 0.6614 - footwear_output_acc: 0.7613 - pose_output_acc: 0.8787 - emotion_output_acc: 0.7173\n",
            "Epoch 00042: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.6959 - gender_output_loss: 0.1920 - image_quality_output_loss: 0.7232 - age_output_loss: 1.2942 - weight_output_loss: 0.9027 - bag_output_loss: 0.7797 - footwear_output_loss: 0.5582 - pose_output_loss: 0.3135 - emotion_output_loss: 0.8139 - gender_output_acc: 0.9196 - image_quality_output_acc: 0.6655 - age_output_acc: 0.4319 - weight_output_acc: 0.6460 - bag_output_acc: 0.6611 - footwear_output_acc: 0.7614 - pose_output_acc: 0.8787 - emotion_output_acc: 0.7170 - val_loss: 12.3002 - val_gender_output_loss: 3.8680 - val_image_quality_output_loss: 1.7152 - val_age_output_loss: 1.5058 - val_weight_output_loss: 1.0598 - val_bag_output_loss: 1.1059 - val_footwear_output_loss: 1.0434 - val_pose_output_loss: 0.8269 - val_emotion_output_loss: 1.0558 - val_gender_output_acc: 0.4395 - val_image_quality_output_acc: 0.3881 - val_age_output_acc: 0.3695 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.3609 - val_footwear_output_acc: 0.4672 - val_pose_output_acc: 0.7046 - val_emotion_output_acc: 0.5766\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 7.02715\n",
            "Epoch 44/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.5158 - gender_output_loss: 0.1671 - image_quality_output_loss: 0.6892 - age_output_loss: 1.2767 - weight_output_loss: 0.8840 - bag_output_loss: 0.7708 - footwear_output_loss: 0.5253 - pose_output_loss: 0.2762 - emotion_output_loss: 0.8065 - gender_output_acc: 0.9346 - image_quality_output_acc: 0.6901 - age_output_acc: 0.4369 - weight_output_acc: 0.6520 - bag_output_acc: 0.6633 - footwear_output_acc: 0.7745 - pose_output_acc: 0.8950 - emotion_output_acc: 0.7182 - val_loss: 8.2342 - val_gender_output_loss: 0.7791 - val_image_quality_output_loss: 1.0345 - val_age_output_loss: 1.4418 - val_weight_output_loss: 0.9905 - val_bag_output_loss: 0.9013 - val_footwear_output_loss: 0.9791 - val_pose_output_loss: 1.0083 - val_emotion_output_loss: 0.9791 - val_gender_output_acc: 0.7127 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3826 - val_weight_output_acc: 0.6109 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.5842 - val_pose_output_acc: 0.6154 - val_emotion_output_acc: 0.6784\n",
            "Epoch 44/96\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 7.02715\n",
            "Epoch 45/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.3497 - gender_output_loss: 0.1412 - image_quality_output_loss: 0.6527 - age_output_loss: 1.2554 - weight_output_loss: 0.8764 - bag_output_loss: 0.7646 - footwear_output_loss: 0.5037 - pose_output_loss: 0.2345 - emotion_output_loss: 0.8004 - gender_output_acc: 0.9444 - image_quality_output_acc: 0.7112 - age_output_acc: 0.4482 - weight_output_acc: 0.6552 - bag_output_acc: 0.6657 - footwear_output_acc: 0.7831 - pose_output_acc: 0.9137 - emotion_output_acc: 0.7175 - val_loss: 9.5172 - val_gender_output_loss: 0.8671 - val_image_quality_output_loss: 1.2786 - val_age_output_loss: 1.4741 - val_weight_output_loss: 1.0192 - val_bag_output_loss: 0.8930 - val_footwear_output_loss: 1.1633 - val_pose_output_loss: 1.7354 - val_emotion_output_loss: 0.9655 - val_gender_output_acc: 0.7616 - val_image_quality_output_acc: 0.4975 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.5817 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.6815 - val_emotion_output_acc: 0.6724\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 7.02715\n",
            "Epoch 46/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.1628 - gender_output_loss: 0.1122 - image_quality_output_loss: 0.6206 - age_output_loss: 1.2406 - weight_output_loss: 0.8586 - bag_output_loss: 0.7565 - footwear_output_loss: 0.4691 - pose_output_loss: 0.1914 - emotion_output_loss: 0.7928 - gender_output_acc: 0.9567 - image_quality_output_acc: 0.7248 - age_output_acc: 0.4562 - weight_output_acc: 0.6609 - bag_output_acc: 0.6726 - footwear_output_acc: 0.7933 - pose_output_acc: 0.9312 - emotion_output_acc: 0.7203Epoch 46/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.1633 - gender_output_loss: 0.1121 - image_quality_output_loss: 0.6209 - age_output_loss: 1.2406 - weight_output_loss: 0.8589 - bag_output_loss: 0.7563 - footwear_output_loss: 0.4690 - pose_output_loss: 0.1913 - emotion_output_loss: 0.7931 - gender_output_acc: 0.9567 - image_quality_output_acc: 0.7247 - age_output_acc: 0.4564 - weight_output_acc: 0.6608 - bag_output_acc: 0.6728 - footwear_output_acc: 0.7933 - pose_output_acc: 0.9313 - emotion_output_acc: 0.7201 - val_loss: 9.0158 - val_gender_output_loss: 0.7993 - val_image_quality_output_loss: 1.0858 - val_age_output_loss: 1.6543 - val_weight_output_loss: 1.1433 - val_bag_output_loss: 0.9116 - val_footwear_output_loss: 1.1007 - val_pose_output_loss: 1.2493 - val_emotion_output_loss: 0.9504 - val_gender_output_acc: 0.7641 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6084 - val_pose_output_acc: 0.7223 - val_emotion_output_acc: 0.6809\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 7.02715\n",
            "Epoch 47/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.9814 - gender_output_loss: 0.0926 - image_quality_output_loss: 0.5693 - age_output_loss: 1.2163 - weight_output_loss: 0.8448 - bag_output_loss: 0.7496 - footwear_output_loss: 0.4451 - pose_output_loss: 0.1568 - emotion_output_loss: 0.7860 - gender_output_acc: 0.9655 - image_quality_output_acc: 0.7534 - age_output_acc: 0.4654 - weight_output_acc: 0.6648 - bag_output_acc: 0.6755 - footwear_output_acc: 0.8111 - pose_output_acc: 0.9471 - emotion_output_acc: 0.7211 - val_loss: 9.3871 - val_gender_output_loss: 1.2591 - val_image_quality_output_loss: 1.2439 - val_age_output_loss: 1.6758 - val_weight_output_loss: 1.0295 - val_bag_output_loss: 0.9809 - val_footwear_output_loss: 1.1183 - val_pose_output_loss: 0.9408 - val_emotion_output_loss: 1.0179 - val_gender_output_acc: 0.6830 - val_image_quality_output_acc: 0.5081 - val_age_output_acc: 0.3432 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.5045 - val_footwear_output_acc: 0.5696 - val_pose_output_acc: 0.6925 - val_emotion_output_acc: 0.6648\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 7.02715\n",
            "Epoch 48/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.8145 - gender_output_loss: 0.0749 - image_quality_output_loss: 0.5252 - age_output_loss: 1.1953 - weight_output_loss: 0.8276 - bag_output_loss: 0.7390 - footwear_output_loss: 0.4248 - pose_output_loss: 0.1301 - emotion_output_loss: 0.7767 - gender_output_acc: 0.9719 - image_quality_output_acc: 0.7810 - age_output_acc: 0.4729 - weight_output_acc: 0.6704 - bag_output_acc: 0.6818 - footwear_output_acc: 0.8190 - pose_output_acc: 0.9581 - emotion_output_acc: 0.7214\n",
            "Epoch 00047: val_loss did not improve from 7.02715\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 48/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.8148 - gender_output_loss: 0.0749 - image_quality_output_loss: 0.5249 - age_output_loss: 1.1954 - weight_output_loss: 0.8281 - bag_output_loss: 0.7390 - footwear_output_loss: 0.4242 - pose_output_loss: 0.1303 - emotion_output_loss: 0.7773 - gender_output_acc: 0.9720 - image_quality_output_acc: 0.7812 - age_output_acc: 0.4727 - weight_output_acc: 0.6701 - bag_output_acc: 0.6820 - footwear_output_acc: 0.8194 - pose_output_acc: 0.9582 - emotion_output_acc: 0.7209 - val_loss: 8.3969 - val_gender_output_loss: 0.7128 - val_image_quality_output_loss: 1.1868 - val_age_output_loss: 1.4926 - val_weight_output_loss: 1.0129 - val_bag_output_loss: 0.9090 - val_footwear_output_loss: 1.1356 - val_pose_output_loss: 0.8594 - val_emotion_output_loss: 0.9669 - val_gender_output_acc: 0.7838 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.6129 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.7354 - val_emotion_output_acc: 0.6774\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 7.02715\n",
            "Epoch 49/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 4.7739 - gender_output_loss: 0.0736 - image_quality_output_loss: 0.5124 - age_output_loss: 1.1871 - weight_output_loss: 0.8262 - bag_output_loss: 0.7353 - footwear_output_loss: 0.4192 - pose_output_loss: 0.1197 - emotion_output_loss: 0.7798 - gender_output_acc: 0.9731 - image_quality_output_acc: 0.7898 - age_output_acc: 0.4790 - weight_output_acc: 0.6744 - bag_output_acc: 0.6804 - footwear_output_acc: 0.8175 - pose_output_acc: 0.9631 - emotion_output_acc: 0.7217 - val_loss: 9.3406 - val_gender_output_loss: 1.1967 - val_image_quality_output_loss: 1.2099 - val_age_output_loss: 1.6699 - val_weight_output_loss: 1.1614 - val_bag_output_loss: 0.9645 - val_footwear_output_loss: 1.1422 - val_pose_output_loss: 0.9006 - val_emotion_output_loss: 0.9747 - val_gender_output_acc: 0.7177 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.3362 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.6779\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 7.02715\n",
            "Epoch 50/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.8584 - gender_output_loss: 0.0844 - image_quality_output_loss: 0.5338 - age_output_loss: 1.1983 - weight_output_loss: 0.8305 - bag_output_loss: 0.7434 - footwear_output_loss: 0.4327 - pose_output_loss: 0.1344 - emotion_output_loss: 0.7802 - gender_output_acc: 0.9682 - image_quality_output_acc: 0.7730 - age_output_acc: 0.4741 - weight_output_acc: 0.6707 - bag_output_acc: 0.6792 - footwear_output_acc: 0.8138 - pose_output_acc: 0.9563 - emotion_output_acc: 0.7204 - val_loss: 9.0415 - val_gender_output_loss: 0.7423 - val_image_quality_output_loss: 1.3219 - val_age_output_loss: 1.4864 - val_weight_output_loss: 1.0281 - val_bag_output_loss: 0.9053 - val_footwear_output_loss: 1.1180 - val_pose_output_loss: 1.3692 - val_emotion_output_loss: 0.9498 - val_gender_output_acc: 0.7717 - val_image_quality_output_acc: 0.5151 - val_age_output_acc: 0.3624 - val_weight_output_acc: 0.6048 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.6018 - val_pose_output_acc: 0.6935 - val_emotion_output_acc: 0.6689\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 7.02715\n",
            "Epoch 51/96\n",
            "Epoch 00049: val_loss did not improve from 7.02715\n",
            "Epoch 50/96\n",
            "\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.9317 - gender_output_loss: 0.0999 - image_quality_output_loss: 0.5422 - age_output_loss: 1.2015 - weight_output_loss: 0.8364 - bag_output_loss: 0.7442 - footwear_output_loss: 0.4458 - pose_output_loss: 0.1576 - emotion_output_loss: 0.7834 - gender_output_acc: 0.9614 - image_quality_output_acc: 0.7670 - age_output_acc: 0.4732 - weight_output_acc: 0.6703 - bag_output_acc: 0.6778 - footwear_output_acc: 0.8040 - pose_output_acc: 0.9416 - emotion_output_acc: 0.7196\n",
            "Epoch 00050: val_loss did not improve from 7.02715\n",
            "\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.9323 - gender_output_loss: 0.0999 - image_quality_output_loss: 0.5420 - age_output_loss: 1.2016 - weight_output_loss: 0.8369 - bag_output_loss: 0.7448 - footwear_output_loss: 0.4457 - pose_output_loss: 0.1578 - emotion_output_loss: 0.7829 - gender_output_acc: 0.9614 - image_quality_output_acc: 0.7672 - age_output_acc: 0.4736 - weight_output_acc: 0.6702 - bag_output_acc: 0.6773 - footwear_output_acc: 0.8041 - pose_output_acc: 0.9414 - emotion_output_acc: 0.7198 - val_loss: 11.3155 - val_gender_output_loss: 1.1918 - val_image_quality_output_loss: 2.2465 - val_age_output_loss: 1.7502 - val_weight_output_loss: 1.1415 - val_bag_output_loss: 1.1655 - val_footwear_output_loss: 1.3397 - val_pose_output_loss: 1.2779 - val_emotion_output_loss: 1.0814 - val_gender_output_acc: 0.6815 - val_image_quality_output_acc: 0.3800 - val_age_output_acc: 0.3407 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.3997 - val_footwear_output_acc: 0.4904 - val_pose_output_acc: 0.6678 - val_emotion_output_acc: 0.6598\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 7.02715\n",
            "Epoch 52/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.0460 - gender_output_loss: 0.1243 - image_quality_output_loss: 0.5632 - age_output_loss: 1.2124 - weight_output_loss: 0.8439 - bag_output_loss: 0.7510 - footwear_output_loss: 0.4633 - pose_output_loss: 0.1806 - emotion_output_loss: 0.7861 - gender_output_acc: 0.9511 - image_quality_output_acc: 0.7530 - age_output_acc: 0.4679 - weight_output_acc: 0.6653 - bag_output_acc: 0.6733 - footwear_output_acc: 0.7982 - pose_output_acc: 0.9291 - emotion_output_acc: 0.7190 - val_loss: 10.2702 - val_gender_output_loss: 0.9862 - val_image_quality_output_loss: 1.3817 - val_age_output_loss: 1.6102 - val_weight_output_loss: 1.1355 - val_bag_output_loss: 0.9145 - val_footwear_output_loss: 1.3209 - val_pose_output_loss: 1.8346 - val_emotion_output_loss: 0.9648 - val_gender_output_acc: 0.7470 - val_image_quality_output_acc: 0.5146 - val_age_output_acc: 0.3412 - val_weight_output_acc: 0.4839 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.5973 - val_pose_output_acc: 0.6794 - val_emotion_output_acc: 0.6729\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 7.02715\n",
            "Epoch 53/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.1639 - gender_output_loss: 0.1371 - image_quality_output_loss: 0.5863 - age_output_loss: 1.2256 - weight_output_loss: 0.8503 - bag_output_loss: 0.7594 - footwear_output_loss: 0.4823 - pose_output_loss: 0.2091 - emotion_output_loss: 0.7914 - gender_output_acc: 0.9482 - image_quality_output_acc: 0.7422 - age_output_acc: 0.4622 - weight_output_acc: 0.6635 - bag_output_acc: 0.6701 - footwear_output_acc: 0.7922 - pose_output_acc: 0.9178 - emotion_output_acc: 0.7191 - val_loss: 9.9917 - val_gender_output_loss: 0.8561 - val_image_quality_output_loss: 1.4412 - val_age_output_loss: 1.5019 - val_weight_output_loss: 1.0341 - val_bag_output_loss: 0.9035 - val_footwear_output_loss: 1.2315 - val_pose_output_loss: 1.9279 - val_emotion_output_loss: 0.9719 - val_gender_output_acc: 0.7545 - val_image_quality_output_acc: 0.4677 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6159 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.5726 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.6714\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 7.02715\n",
            "Epoch 54/96\n",
            "Epoch 53/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.2902 - gender_output_loss: 0.1548 - image_quality_output_loss: 0.6085 - age_output_loss: 1.2365 - weight_output_loss: 0.8546 - bag_output_loss: 0.7658 - footwear_output_loss: 0.5064 - pose_output_loss: 0.2425 - emotion_output_loss: 0.7961 - gender_output_acc: 0.9387 - image_quality_output_acc: 0.7350 - age_output_acc: 0.4623 - weight_output_acc: 0.6589 - bag_output_acc: 0.6659 - footwear_output_acc: 0.7774 - pose_output_acc: 0.9102 - emotion_output_acc: 0.7199 - val_loss: 13.0899 - val_gender_output_loss: 4.7501 - val_image_quality_output_loss: 1.1732 - val_age_output_loss: 1.5276 - val_weight_output_loss: 1.1690 - val_bag_output_loss: 1.0356 - val_footwear_output_loss: 1.1621 - val_pose_output_loss: 1.1552 - val_emotion_output_loss: 0.9908 - val_gender_output_acc: 0.4385 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3493 - val_weight_output_acc: 0.4929 - val_bag_output_acc: 0.4022 - val_footwear_output_acc: 0.5282 - val_pose_output_acc: 0.7061 - val_emotion_output_acc: 0.6583\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 7.02715\n",
            "Epoch 54/96\n",
            "Epoch 55/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 5.4279 - gender_output_loss: 0.1760 - image_quality_output_loss: 0.6314 - age_output_loss: 1.2512 - weight_output_loss: 0.8633 - bag_output_loss: 0.7754 - footwear_output_loss: 0.5225 - pose_output_loss: 0.2732 - emotion_output_loss: 0.8068 - gender_output_acc: 0.9299 - image_quality_output_acc: 0.7242 - age_output_acc: 0.4518 - weight_output_acc: 0.6598 - bag_output_acc: 0.6622 - footwear_output_acc: 0.7754 - pose_output_acc: 0.8951 - emotion_output_acc: 0.7196 - val_loss: 9.8724 - val_gender_output_loss: 1.1654 - val_image_quality_output_loss: 1.7316 - val_age_output_loss: 1.5012 - val_weight_output_loss: 1.0207 - val_bag_output_loss: 1.1050 - val_footwear_output_loss: 1.2896 - val_pose_output_loss: 0.9673 - val_emotion_output_loss: 0.9613 - val_gender_output_acc: 0.6825 - val_image_quality_output_acc: 0.4032 - val_age_output_acc: 0.3639 - val_weight_output_acc: 0.6184 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.5635 - val_pose_output_acc: 0.6875 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 7.02715\n",
            "Epoch 56/96\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 5.5742 - gender_output_loss: 0.2052 - image_quality_output_loss: 0.6631 - age_output_loss: 1.2586 - weight_output_loss: 0.8734 - bag_output_loss: 0.7719 - footwear_output_loss: 0.5518 - pose_output_loss: 0.3106 - emotion_output_loss: 0.8069 - gender_output_acc: 0.9153 - image_quality_output_acc: 0.6987 - age_output_acc: 0.4532 - weight_output_acc: 0.6573 - bag_output_acc: 0.6626 - footwear_output_acc: 0.7611 - pose_output_acc: 0.8760 - emotion_output_acc: 0.7197 - val_loss: 9.3984 - val_gender_output_loss: 0.9383 - val_image_quality_output_loss: 1.0345 - val_age_output_loss: 1.5180 - val_weight_output_loss: 1.0502 - val_bag_output_loss: 1.0824 - val_footwear_output_loss: 1.0576 - val_pose_output_loss: 1.5901 - val_emotion_output_loss: 0.9919 - val_gender_output_acc: 0.7117 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.3377 - val_weight_output_acc: 0.5544 - val_bag_output_acc: 0.5756 - val_footwear_output_acc: 0.6169 - val_pose_output_acc: 0.5585 - val_emotion_output_acc: 0.6663\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 7.02715\n",
            "Epoch 57/96\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 5.5928 - gender_output_loss: 0.1969 - image_quality_output_loss: 0.6648 - age_output_loss: 1.2656 - weight_output_loss: 0.8780 - bag_output_loss: 0.7811 - footwear_output_loss: 0.5603 - pose_output_loss: 0.2971 - emotion_output_loss: 0.8110 - gender_output_acc: 0.9172 - image_quality_output_acc: 0.6989 - age_output_acc: 0.4493 - weight_output_acc: 0.6505 - bag_output_acc: 0.6608 - footwear_output_acc: 0.7539 - pose_output_acc: 0.8856 - emotion_output_acc: 0.7174 - val_loss: 9.0379 - val_gender_output_loss: 0.9136 - val_image_quality_output_loss: 1.0735 - val_age_output_loss: 1.4747 - val_weight_output_loss: 0.9946 - val_bag_output_loss: 1.0107 - val_footwear_output_loss: 1.4118 - val_pose_output_loss: 1.0605 - val_emotion_output_loss: 0.9582 - val_gender_output_acc: 0.7182 - val_image_quality_output_acc: 0.5348 - val_age_output_acc: 0.3382 - val_weight_output_acc: 0.6159 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5418 - val_pose_output_acc: 0.6421 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 7.02715\n",
            "Epoch 58/96\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 5.3879 - gender_output_loss: 0.1739 - image_quality_output_loss: 0.6258 - age_output_loss: 1.2371 - weight_output_loss: 0.8578 - bag_output_loss: 0.7683 - footwear_output_loss: 0.5319 - pose_output_loss: 0.2523 - emotion_output_loss: 0.7989 - gender_output_acc: 0.9309 - image_quality_output_acc: 0.7272 - age_output_acc: 0.4536 - weight_output_acc: 0.6588 - bag_output_acc: 0.6654 - footwear_output_acc: 0.7697 - pose_output_acc: 0.9037 - emotion_output_acc: 0.7190 - val_loss: 9.5873 - val_gender_output_loss: 1.5403 - val_image_quality_output_loss: 1.0841 - val_age_output_loss: 1.5614 - val_weight_output_loss: 1.0363 - val_bag_output_loss: 1.1665 - val_footwear_output_loss: 1.1987 - val_pose_output_loss: 0.8930 - val_emotion_output_loss: 0.9635 - val_gender_output_acc: 0.6457 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3357 - val_weight_output_acc: 0.5953 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.5882 - val_pose_output_acc: 0.6683 - val_emotion_output_acc: 0.6835\n",
            "Epoch 58/96\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 7.02715\n",
            "Epoch 59/96\n",
            "360/360 [==============================] - 91s 251ms/step - loss: 5.2084 - gender_output_loss: 0.1518 - image_quality_output_loss: 0.5863 - age_output_loss: 1.2143 - weight_output_loss: 0.8399 - bag_output_loss: 0.7644 - footwear_output_loss: 0.5062 - pose_output_loss: 0.2109 - emotion_output_loss: 0.7900 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.7406 - age_output_acc: 0.4651 - weight_output_acc: 0.6708 - bag_output_acc: 0.6645 - footwear_output_acc: 0.7773 - pose_output_acc: 0.9218 - emotion_output_acc: 0.7218 - val_loss: 10.1790 - val_gender_output_loss: 1.0030 - val_image_quality_output_loss: 1.5399 - val_age_output_loss: 1.8728 - val_weight_output_loss: 1.2753 - val_bag_output_loss: 0.9509 - val_footwear_output_loss: 1.2575 - val_pose_output_loss: 1.1340 - val_emotion_output_loss: 1.0000 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.4869 - val_age_output_acc: 0.3206 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.5741 - val_pose_output_acc: 0.7233 - val_emotion_output_acc: 0.6583\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 7.02715\n",
            "Epoch 60/96\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 5.0035 - gender_output_loss: 0.1306 - image_quality_output_loss: 0.5298 - age_output_loss: 1.1882 - weight_output_loss: 0.8212 - bag_output_loss: 0.7530 - footwear_output_loss: 0.4742 - pose_output_loss: 0.1761 - emotion_output_loss: 0.7840 - gender_output_acc: 0.9476 - image_quality_output_acc: 0.7727 - age_output_acc: 0.4777 - weight_output_acc: 0.6715 - bag_output_acc: 0.6689 - footwear_output_acc: 0.7941 - pose_output_acc: 0.9340 - emotion_output_acc: 0.7194 - val_loss: 9.6642 - val_gender_output_loss: 0.7330 - val_image_quality_output_loss: 1.2803 - val_age_output_loss: 1.9039 - val_weight_output_loss: 1.1980 - val_bag_output_loss: 0.9234 - val_footwear_output_loss: 1.0559 - val_pose_output_loss: 1.2853 - val_emotion_output_loss: 1.1376 - val_gender_output_acc: 0.7676 - val_image_quality_output_acc: 0.5141 - val_age_output_acc: 0.3276 - val_weight_output_acc: 0.6356 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6109 - val_pose_output_acc: 0.6502 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 7.02715\n",
            "Epoch 61/96\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 4.7799 - gender_output_loss: 0.1054 - image_quality_output_loss: 0.4771 - age_output_loss: 1.1528 - weight_output_loss: 0.7977 - bag_output_loss: 0.7433 - footwear_output_loss: 0.4397 - pose_output_loss: 0.1397 - emotion_output_loss: 0.7771 - gender_output_acc: 0.9606 - image_quality_output_acc: 0.7980 - age_output_acc: 0.4925 - weight_output_acc: 0.6831 - bag_output_acc: 0.6739 - footwear_output_acc: 0.8062 - pose_output_acc: 0.9481 - emotion_output_acc: 0.7246 - val_loss: 9.9512 - val_gender_output_loss: 0.8541 - val_image_quality_output_loss: 1.6920 - val_age_output_loss: 1.5614 - val_weight_output_loss: 1.0525 - val_bag_output_loss: 0.9323 - val_footwear_output_loss: 1.3263 - val_pose_output_loss: 1.3358 - val_emotion_output_loss: 1.0494 - val_gender_output_acc: 0.7641 - val_image_quality_output_acc: 0.5071 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.6028 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.5867 - val_pose_output_acc: 0.6935 - val_emotion_output_acc: 0.6840\n",
            "Epoch 61/96\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 7.02715\n",
            "Epoch 62/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.5693 - gender_output_loss: 0.0797 - image_quality_output_loss: 0.4153 - age_output_loss: 1.1186 - weight_output_loss: 0.7786 - bag_output_loss: 0.7365 - footwear_output_loss: 0.4126 - pose_output_loss: 0.1139 - emotion_output_loss: 0.7669 - gender_output_acc: 0.9696 - image_quality_output_acc: 0.8343 - age_output_acc: 0.5075 - weight_output_acc: 0.6903 - bag_output_acc: 0.6757 - footwear_output_acc: 0.8169 - pose_output_acc: 0.9604 - emotion_output_acc: 0.7230\n",
            "Epoch 00061: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 4.5690 - gender_output_loss: 0.0801 - image_quality_output_loss: 0.4155 - age_output_loss: 1.1184 - weight_output_loss: 0.7783 - bag_output_loss: 0.7361 - footwear_output_loss: 0.4130 - pose_output_loss: 0.1139 - emotion_output_loss: 0.7666 - gender_output_acc: 0.9695 - image_quality_output_acc: 0.8342 - age_output_acc: 0.5075 - weight_output_acc: 0.6904 - bag_output_acc: 0.6759 - footwear_output_acc: 0.8166 - pose_output_acc: 0.9604 - emotion_output_acc: 0.7232 - val_loss: 10.6084 - val_gender_output_loss: 1.2001 - val_image_quality_output_loss: 1.8493 - val_age_output_loss: 1.7185 - val_weight_output_loss: 1.2146 - val_bag_output_loss: 0.9887 - val_footwear_output_loss: 1.2863 - val_pose_output_loss: 1.2221 - val_emotion_output_loss: 0.9816 - val_gender_output_acc: 0.7429 - val_image_quality_output_acc: 0.4899 - val_age_output_acc: 0.3387 - val_weight_output_acc: 0.6179 - val_bag_output_acc: 0.6033 - val_footwear_output_acc: 0.6043 - val_pose_output_acc: 0.7152 - val_emotion_output_acc: 0.6416\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 7.02715\n",
            "Epoch 63/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.3645 - gender_output_loss: 0.0677 - image_quality_output_loss: 0.3472 - age_output_loss: 1.0860 - weight_output_loss: 0.7553 - bag_output_loss: 0.7226 - footwear_output_loss: 0.3886 - pose_output_loss: 0.0888 - emotion_output_loss: 0.7613 - gender_output_acc: 0.9751 - image_quality_output_acc: 0.8670 - age_output_acc: 0.5244 - weight_output_acc: 0.6976 - bag_output_acc: 0.6812 - footwear_output_acc: 0.8244 - pose_output_acc: 0.9714 - emotion_output_acc: 0.7240\n",
            "Epoch 00062: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 4.3646 - gender_output_loss: 0.0676 - image_quality_output_loss: 0.3470 - age_output_loss: 1.0861 - weight_output_loss: 0.7551 - bag_output_loss: 0.7231 - footwear_output_loss: 0.3883 - pose_output_loss: 0.0887 - emotion_output_loss: 0.7615 - gender_output_acc: 0.9752 - image_quality_output_acc: 0.8672 - age_output_acc: 0.5242 - weight_output_acc: 0.6978 - bag_output_acc: 0.6813 - footwear_output_acc: 0.8244 - pose_output_acc: 0.9714 - emotion_output_acc: 0.7239 - val_loss: 9.7319 - val_gender_output_loss: 0.8565 - val_image_quality_output_loss: 1.6870 - val_age_output_loss: 1.6200 - val_weight_output_loss: 1.1019 - val_bag_output_loss: 0.9189 - val_footwear_output_loss: 1.2618 - val_pose_output_loss: 1.1519 - val_emotion_output_loss: 0.9869 - val_gender_output_acc: 0.7742 - val_image_quality_output_acc: 0.5141 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5912 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6018 - val_pose_output_acc: 0.7258 - val_emotion_output_acc: 0.6724\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 7.02715\n",
            "Epoch 64/96\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 4.1859 - gender_output_loss: 0.0464 - image_quality_output_loss: 0.2956 - age_output_loss: 1.0533 - weight_output_loss: 0.7348 - bag_output_loss: 0.7153 - footwear_output_loss: 0.3716 - pose_output_loss: 0.0687 - emotion_output_loss: 0.7532 - gender_output_acc: 0.9829 - image_quality_output_acc: 0.8938 - age_output_acc: 0.5389 - weight_output_acc: 0.7051 - bag_output_acc: 0.6885 - footwear_output_acc: 0.8341 - pose_output_acc: 0.9786 - emotion_output_acc: 0.7267 - val_loss: 9.7280 - val_gender_output_loss: 0.8722 - val_image_quality_output_loss: 1.6466 - val_age_output_loss: 1.6801 - val_weight_output_loss: 1.1333 - val_bag_output_loss: 0.9345 - val_footwear_output_loss: 1.2719 - val_pose_output_loss: 1.0449 - val_emotion_output_loss: 0.9977 - val_gender_output_acc: 0.7732 - val_image_quality_output_acc: 0.5131 - val_age_output_acc: 0.3453 - val_weight_output_acc: 0.6074 - val_bag_output_acc: 0.5968 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.7303 - val_emotion_output_acc: 0.6774\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 7.02715\n",
            "Epoch 65/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.1405 - gender_output_loss: 0.0447 - image_quality_output_loss: 0.2801 - age_output_loss: 1.0435 - weight_output_loss: 0.7285 - bag_output_loss: 0.7111 - footwear_output_loss: 0.3711 - pose_output_loss: 0.0634 - emotion_output_loss: 0.7513 - gender_output_acc: 0.9848 - image_quality_output_acc: 0.8989 - age_output_acc: 0.5437 - weight_output_acc: 0.7032 - bag_output_acc: 0.6893 - footwear_output_acc: 0.8335 - pose_output_acc: 0.9819 - emotion_output_acc: 0.7261\n",
            "Epoch 00064: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 4.1404 - gender_output_loss: 0.0447 - image_quality_output_loss: 0.2800 - age_output_loss: 1.0430 - weight_output_loss: 0.7284 - bag_output_loss: 0.7111 - footwear_output_loss: 0.3714 - pose_output_loss: 0.0633 - emotion_output_loss: 0.7516 - gender_output_acc: 0.9847 - image_quality_output_acc: 0.8991 - age_output_acc: 0.5439 - weight_output_acc: 0.7034 - bag_output_acc: 0.6892 - footwear_output_acc: 0.8333 - pose_output_acc: 0.9819 - emotion_output_acc: 0.7260 - val_loss: 10.1751 - val_gender_output_loss: 1.0781 - val_image_quality_output_loss: 1.7150 - val_age_output_loss: 1.6855 - val_weight_output_loss: 1.1285 - val_bag_output_loss: 0.9559 - val_footwear_output_loss: 1.3037 - val_pose_output_loss: 1.1679 - val_emotion_output_loss: 0.9936 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.5116 - val_age_output_acc: 0.3392 - val_weight_output_acc: 0.5806 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6053 - val_pose_output_acc: 0.7268 - val_emotion_output_acc: 0.6749\n",
            "Epoch 65/96\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 7.02715\n",
            "Epoch 66/96\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 4.2352 - gender_output_loss: 0.0573 - image_quality_output_loss: 0.3008 - age_output_loss: 1.0610 - weight_output_loss: 0.7351 - bag_output_loss: 0.7179 - footwear_output_loss: 0.3828 - pose_output_loss: 0.0793 - emotion_output_loss: 0.7543 - gender_output_acc: 0.9790 - image_quality_output_acc: 0.8826 - age_output_acc: 0.5336 - weight_output_acc: 0.7003 - bag_output_acc: 0.6846 - footwear_output_acc: 0.8262 - pose_output_acc: 0.9729 - emotion_output_acc: 0.7256 - val_loss: 10.2737 - val_gender_output_loss: 0.9998 - val_image_quality_output_loss: 1.7325 - val_age_output_loss: 1.7231 - val_weight_output_loss: 1.1609 - val_bag_output_loss: 0.9675 - val_footwear_output_loss: 1.3439 - val_pose_output_loss: 1.2080 - val_emotion_output_loss: 0.9912 - val_gender_output_acc: 0.7712 - val_image_quality_output_acc: 0.5060 - val_age_output_acc: 0.3438 - val_weight_output_acc: 0.5746 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6033 - val_pose_output_acc: 0.7283 - val_emotion_output_acc: 0.6749\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 7.02715\n",
            " - 91s 254ms/step - loss: 4.1404 - gender_output_loss: 0.0447 - image_quality_output_loss: 0.2800 - age_output_loss: 1.0430 - weight_output_loss: 0.7284 - bag_output_loss: 0.7111 - footwear_output_loss: 0.3714 - pose_output_loss: 0.0633 - emotion_output_loss: 0.7516 - gender_output_acc: 0.9847 - image_quality_output_acc: 0.8991 - age_output_acc: 0.5439 - weight_output_acc: 0.7034 - bag_output_acc: 0.6892 - footwear_output_acc: 0.8333 - pose_output_acc: 0.9819 - emotion_output_acc: 0.7260 - val_loss: 10.1751 - val_gender_output_loss: 1.0781 - val_image_quality_output_loss: 1.7150 - val_age_output_loss: 1.6855 - val_weight_output_loss: 1.1285 - val_bag_output_loss: 0.9559 - val_footwear_output_loss: 1.3037 - val_pose_output_loss: 1.1679 - val_emotion_output_loss: 0.9936 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.5116 - val_age_output_acc: 0.3392 - val_weight_output_acc: 0.5806 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6053 - val_pose_output_acc: 0.7268 - val_emotion_output_acc: 0.6749\n",
            "Epoch 66/96\n",
            "Epoch 67/96\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 4.3197 - gender_output_loss: 0.0738 - image_quality_output_loss: 0.3140 - age_output_loss: 1.0722 - weight_output_loss: 0.7479 - bag_output_loss: 0.7179 - footwear_output_loss: 0.3940 - pose_output_loss: 0.0943 - emotion_output_loss: 0.7589 - gender_output_acc: 0.9717 - image_quality_output_acc: 0.8750 - age_output_acc: 0.5306 - weight_output_acc: 0.6974 - bag_output_acc: 0.6847 - footwear_output_acc: 0.8244 - pose_output_acc: 0.9655 - emotion_output_acc: 0.7279 - val_loss: 11.7272 - val_gender_output_loss: 1.2562 - val_image_quality_output_loss: 1.8747 - val_age_output_loss: 1.8230 - val_weight_output_loss: 1.2648 - val_bag_output_loss: 0.9381 - val_footwear_output_loss: 1.3984 - val_pose_output_loss: 2.0311 - val_emotion_output_loss: 0.9941 - val_gender_output_acc: 0.7500 - val_image_quality_output_acc: 0.5131 - val_age_output_acc: 0.3347 - val_weight_output_acc: 0.6184 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.6976 - val_emotion_output_acc: 0.6754\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 7.02715\n",
            "Epoch 68/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.4742 - gender_output_loss: 0.0951 - image_quality_output_loss: 0.3599 - age_output_loss: 1.0932 - weight_output_loss: 0.7570 - bag_output_loss: 0.7258 - footwear_output_loss: 0.4150 - pose_output_loss: 0.1209 - emotion_output_loss: 0.7602 - gender_output_acc: 0.9616 - image_quality_output_acc: 0.8498 - age_output_acc: 0.5188 - weight_output_acc: 0.6971 - bag_output_acc: 0.6814 - footwear_output_acc: 0.8155 - pose_output_acc: 0.9523 - emotion_output_acc: 0.7275\n",
            "\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 4.4732 - gender_output_loss: 0.0949 - image_quality_output_loss: 0.3597 - age_output_loss: 1.0930 - weight_output_loss: 0.7568 - bag_output_loss: 0.7262 - footwear_output_loss: 0.4147 - pose_output_loss: 0.1208 - emotion_output_loss: 0.7599 - gender_output_acc: 0.9616 - image_quality_output_acc: 0.8500 - age_output_acc: 0.5188 - weight_output_acc: 0.6973 - bag_output_acc: 0.6811 - footwear_output_acc: 0.8157 - pose_output_acc: 0.9523 - emotion_output_acc: 0.7274 - val_loss: 10.9648 - val_gender_output_loss: 0.9837 - val_image_quality_output_loss: 2.2019 - val_age_output_loss: 1.7353 - val_weight_output_loss: 1.1352 - val_bag_output_loss: 0.9316 - val_footwear_output_loss: 1.3037 - val_pose_output_loss: 1.4619 - val_emotion_output_loss: 1.0638 - val_gender_output_acc: 0.7631 - val_image_quality_output_acc: 0.4370 - val_age_output_acc: 0.3377 - val_weight_output_acc: 0.5993 - val_bag_output_acc: 0.5761 - val_footwear_output_acc: 0.6023 - val_pose_output_acc: 0.7011 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 7.02715\n",
            "Epoch 69/96\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 4.6343 - gender_output_loss: 0.1068 - image_quality_output_loss: 0.4070 - age_output_loss: 1.1077 - weight_output_loss: 0.7700 - bag_output_loss: 0.7368 - footwear_output_loss: 0.4380 - pose_output_loss: 0.1504 - emotion_output_loss: 0.7690 - gender_output_acc: 0.9581 - image_quality_output_acc: 0.8298 - age_output_acc: 0.5107 - weight_output_acc: 0.6903 - bag_output_acc: 0.6814 - footwear_output_acc: 0.8044 - pose_output_acc: 0.9464 - emotion_output_acc: 0.7241 - val_loss: 11.9252 - val_gender_output_loss: 1.2575 - val_image_quality_output_loss: 1.7953 - val_age_output_loss: 1.6952 - val_weight_output_loss: 1.2810 - val_bag_output_loss: 1.2689 - val_footwear_output_loss: 1.5230 - val_pose_output_loss: 1.8948 - val_emotion_output_loss: 1.0600 - val_gender_output_acc: 0.7480 - val_image_quality_output_acc: 0.4844 - val_age_output_acc: 0.2949 - val_weight_output_acc: 0.4834 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5897 - val_pose_output_acc: 0.6865 - val_emotion_output_acc: 0.6018\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 7.02715\n",
            "Epoch 70/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.7967 - gender_output_loss: 0.1267 - image_quality_output_loss: 0.4471 - age_output_loss: 1.1305 - weight_output_loss: 0.7854 - bag_output_loss: 0.7456 - footwear_output_loss: 0.4618 - pose_output_loss: 0.1706 - emotion_output_loss: 0.7782 - gender_output_acc: 0.9522 - image_quality_output_acc: 0.8123 - age_output_acc: 0.5062 - weight_output_acc: 0.6821 - bag_output_acc: 0.6705 - footwear_output_acc: 0.7966 - pose_output_acc: 0.9351 - emotion_output_acc: 0.7214Epoch 70/96\n",
            "Epoch 00069: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 4.7966 - gender_output_loss: 0.1265 - image_quality_output_loss: 0.4470 - age_output_loss: 1.1310 - weight_output_loss: 0.7851 - bag_output_loss: 0.7456 - footwear_output_loss: 0.4615 - pose_output_loss: 0.1707 - emotion_output_loss: 0.7784 - gender_output_acc: 0.9523 - image_quality_output_acc: 0.8124 - age_output_acc: 0.5064 - weight_output_acc: 0.6824 - bag_output_acc: 0.6704 - footwear_output_acc: 0.7967 - pose_output_acc: 0.9350 - emotion_output_acc: 0.7214 - val_loss: 11.3273 - val_gender_output_loss: 1.3050 - val_image_quality_output_loss: 2.3460 - val_age_output_loss: 1.8840 - val_weight_output_loss: 1.2102 - val_bag_output_loss: 0.9533 - val_footwear_output_loss: 1.2620 - val_pose_output_loss: 1.1952 - val_emotion_output_loss: 1.0194 - val_gender_output_acc: 0.6401 - val_image_quality_output_acc: 0.4214 - val_age_output_acc: 0.3261 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5111 - val_footwear_output_acc: 0.4950 - val_pose_output_acc: 0.6467 - val_emotion_output_acc: 0.6699\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 7.02715\n",
            "Epoch 71/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.9692 - gender_output_loss: 0.1539 - image_quality_output_loss: 0.4824 - age_output_loss: 1.1512 - weight_output_loss: 0.8021 - bag_output_loss: 0.7493 - footwear_output_loss: 0.4881 - pose_output_loss: 0.2042 - emotion_output_loss: 0.7837 - gender_output_acc: 0.9387 - image_quality_output_acc: 0.7976 - age_output_acc: 0.4976 - weight_output_acc: 0.6788 - bag_output_acc: 0.6684 - footwear_output_acc: 0.7852 - pose_output_acc: 0.9250 - emotion_output_acc: 0.7201Epoch 71/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.9696 - gender_output_loss: 0.1540 - image_quality_output_loss: 0.4823 - age_output_loss: 1.1509 - weight_output_loss: 0.8024 - bag_output_loss: 0.7493 - footwear_output_loss: 0.4880 - pose_output_loss: 0.2040 - emotion_output_loss: 0.7844 - gender_output_acc: 0.9385 - image_quality_output_acc: 0.7978 - age_output_acc: 0.4978 - weight_output_acc: 0.6786 - bag_output_acc: 0.6686 - footwear_output_acc: 0.7852 - pose_output_acc: 0.9251 - emotion_output_acc: 0.7198 - val_loss: 12.1786 - val_gender_output_loss: 0.8223 - val_image_quality_output_loss: 1.9760 - val_age_output_loss: 1.6057 - val_weight_output_loss: 1.0904 - val_bag_output_loss: 0.9593 - val_footwear_output_loss: 1.5781 - val_pose_output_loss: 2.8408 - val_emotion_output_loss: 1.1495 - val_gender_output_acc: 0.7389 - val_image_quality_output_acc: 0.4622 - val_age_output_acc: 0.3513 - val_weight_output_acc: 0.5524 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.5343 - val_pose_output_acc: 0.3735 - val_emotion_output_acc: 0.6739\n",
            "\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 7.02715\n",
            "Epoch 72/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.1556 - gender_output_loss: 0.1701 - image_quality_output_loss: 0.5323 - age_output_loss: 1.1754 - weight_output_loss: 0.8172 - bag_output_loss: 0.7575 - footwear_output_loss: 0.5151 - pose_output_loss: 0.2413 - emotion_output_loss: 0.7878 - gender_output_acc: 0.9333 - image_quality_output_acc: 0.7664 - age_output_acc: 0.4835 - weight_output_acc: 0.6747 - bag_output_acc: 0.6686 - footwear_output_acc: 0.7716 - pose_output_acc: 0.9103 - emotion_output_acc: 0.7195Epoch 72/96\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.1560 - gender_output_loss: 0.1707 - image_quality_output_loss: 0.5321 - age_output_loss: 1.1753 - weight_output_loss: 0.8170 - bag_output_loss: 0.7573 - footwear_output_loss: 0.5156 - pose_output_loss: 0.2413 - emotion_output_loss: 0.7879 - gender_output_acc: 0.9332 - image_quality_output_acc: 0.7662 - age_output_acc: 0.4835 - weight_output_acc: 0.6747 - bag_output_acc: 0.6687 - footwear_output_acc: 0.7715 - pose_output_acc: 0.9104 - emotion_output_acc: 0.7195 - val_loss: 10.5566 - val_gender_output_loss: 0.6345 - val_image_quality_output_loss: 2.7977 - val_age_output_loss: 1.7878 - val_weight_output_loss: 1.2750 - val_bag_output_loss: 0.9223 - val_footwear_output_loss: 0.9441 - val_pose_output_loss: 1.0109 - val_emotion_output_loss: 1.0226 - val_gender_output_acc: 0.7666 - val_image_quality_output_acc: 0.3664 - val_age_output_acc: 0.2933 - val_weight_output_acc: 0.4577 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.5706 - val_pose_output_acc: 0.6734 - val_emotion_output_acc: 0.6018\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 7.02715\n",
            "Epoch 73/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 5.1788 - gender_output_loss: 0.1726 - image_quality_output_loss: 0.5484 - age_output_loss: 1.1662 - weight_output_loss: 0.8241 - bag_output_loss: 0.7588 - footwear_output_loss: 0.5132 - pose_output_loss: 0.2394 - emotion_output_loss: 0.7919 - gender_output_acc: 0.9296 - image_quality_output_acc: 0.7607 - age_output_acc: 0.4865 - weight_output_acc: 0.6737 - bag_output_acc: 0.6635 - footwear_output_acc: 0.7756 - pose_output_acc: 0.9086 - emotion_output_acc: 0.7213 - val_loss: 16.0603 - val_gender_output_loss: 3.5109 - val_image_quality_output_loss: 1.4817 - val_age_output_loss: 1.8985 - val_weight_output_loss: 1.3767 - val_bag_output_loss: 1.4340 - val_footwear_output_loss: 1.9579 - val_pose_output_loss: 3.1344 - val_emotion_output_loss: 1.0997 - val_gender_output_acc: 0.5837 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3211 - val_weight_output_acc: 0.5267 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.5413 - val_pose_output_acc: 0.6442 - val_emotion_output_acc: 0.6129\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 5.1560 - gender_output_loss: 0.1707 - image_quality_output_loss: 0.5321 - age_output_loss: 1.1753 - weight_output_loss: 0.8170 - bag_output_loss: 0.7573 - footwear_output_loss: 0.5156 - pose_output_loss: 0.2413 - emotion_output_loss: 0.7879 - gender_output_acc: 0.9332 - image_quality_output_acc: 0.7662 - age_output_acc: 0.4835 - weight_output_acc: 0.6747 - bag_output_acc: 0.6687 - footwear_output_acc: 0.7715 - pose_output_acc: 0.9104 - emotion_output_acc: 0.7195 - val_loss: 10.5566 - val_gender_output_loss: 0.6345 - val_image_quality_output_loss: 2.7977 - val_age_output_loss: 1.7878 - val_weight_output_loss: 1.2750 - val_bag_output_loss: 0.9223 - val_footwear_output_loss: 0.9441 - val_pose_output_loss: 1.0109 - val_emotion_output_loss: 1.0226 - val_gender_output_acc: 0.7666 - val_image_quality_output_acc: 0.3664 - val_age_output_acc: 0.2933 - val_weight_output_acc: 0.4577 - val_bag_output_acc: 0.5570 - val_footwear_output_acc: 0.5706 - val_pose_output_acc: 0.6734 - val_emotion_output_acc: 0.6018\n",
            "Epoch 73/96\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 7.02715\n",
            "Epoch 74/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 4.9711 - gender_output_loss: 0.1467 - image_quality_output_loss: 0.4913 - age_output_loss: 1.1450 - weight_output_loss: 0.8023 - bag_output_loss: 0.7488 - footwear_output_loss: 0.4850 - pose_output_loss: 0.1996 - emotion_output_loss: 0.7842 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.7897 - age_output_acc: 0.4943 - weight_output_acc: 0.6826 - bag_output_acc: 0.6714 - footwear_output_acc: 0.7844 - pose_output_acc: 0.9235 - emotion_output_acc: 0.7222 - val_loss: 9.0521 - val_gender_output_loss: 0.6927 - val_image_quality_output_loss: 1.4789 - val_age_output_loss: 1.5871 - val_weight_output_loss: 1.0630 - val_bag_output_loss: 0.9316 - val_footwear_output_loss: 1.0564 - val_pose_output_loss: 1.0627 - val_emotion_output_loss: 1.0100 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5025 - val_age_output_acc: 0.3347 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5993 - val_footwear_output_acc: 0.6079 - val_pose_output_acc: 0.6426 - val_emotion_output_acc: 0.6850\n",
            "Epoch 74/96\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 7.02715\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00074: val_loss did not improve from 7.02715\n",
            "Epoch 75/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.7322 - gender_output_loss: 0.1236 - image_quality_output_loss: 0.4274 - age_output_loss: 1.1044 - weight_output_loss: 0.7772 - bag_output_loss: 0.7398 - footwear_output_loss: 0.4585 - pose_output_loss: 0.1574 - emotion_output_loss: 0.7730 - gender_output_acc: 0.9519 - image_quality_output_acc: 0.8205 - age_output_acc: 0.5194 - weight_output_acc: 0.6874 - bag_output_acc: 0.6694 - footwear_output_acc: 0.7982 - pose_output_acc: 0.9427 - emotion_output_acc: 0.7267\n",
            "Epoch 00074: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.7321 - gender_output_loss: 0.1237 - image_quality_output_loss: 0.4273 - age_output_loss: 1.1043 - weight_output_loss: 0.7768 - bag_output_loss: 0.7395 - footwear_output_loss: 0.4583 - pose_output_loss: 0.1579 - emotion_output_loss: 0.7734 - gender_output_acc: 0.9519 - image_quality_output_acc: 0.8206 - age_output_acc: 0.5198 - weight_output_acc: 0.6876 - bag_output_acc: 0.6697 - footwear_output_acc: 0.7983 - pose_output_acc: 0.9425 - emotion_output_acc: 0.7265 - val_loss: 13.0780 - val_gender_output_loss: 1.0325 - val_image_quality_output_loss: 2.0884 - val_age_output_loss: 2.6725 - val_weight_output_loss: 1.6589 - val_bag_output_loss: 1.1156 - val_footwear_output_loss: 1.1779 - val_pose_output_loss: 2.1454 - val_emotion_output_loss: 1.0152 - val_gender_output_acc: 0.7324 - val_image_quality_output_acc: 0.4380 - val_age_output_acc: 0.2767 - val_weight_output_acc: 0.6336 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6038 - val_pose_output_acc: 0.5060 - val_emotion_output_acc: 0.6794\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 7.02715\n",
            "Epoch 76/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.5122 - gender_output_loss: 0.1060 - image_quality_output_loss: 0.3517 - age_output_loss: 1.0730 - weight_output_loss: 0.7544 - bag_output_loss: 0.7253 - footwear_output_loss: 0.4258 - pose_output_loss: 0.1355 - emotion_output_loss: 0.7681 - gender_output_acc: 0.9577 - image_quality_output_acc: 0.8543 - age_output_acc: 0.5265 - weight_output_acc: 0.6978 - bag_output_acc: 0.6853 - footwear_output_acc: 0.8115 - pose_output_acc: 0.9490 - emotion_output_acc: 0.7244 - val_loss: 12.9442 - val_gender_output_loss: 1.1724 - val_image_quality_output_loss: 3.7724 - val_age_output_loss: 1.8045 - val_weight_output_loss: 1.1992 - val_bag_output_loss: 0.9758 - val_footwear_output_loss: 1.3248 - val_pose_output_loss: 1.5160 - val_emotion_output_loss: 1.0062 - val_gender_output_acc: 0.7374 - val_image_quality_output_acc: 0.3684 - val_age_output_acc: 0.3261 - val_weight_output_acc: 0.5333 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6069 - val_pose_output_acc: 0.7213 - val_emotion_output_acc: 0.6310\n",
            "Epoch 76/96\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 7.02715\n",
            "Epoch 77/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.3034 - gender_output_loss: 0.0816 - image_quality_output_loss: 0.3063 - age_output_loss: 1.0324 - weight_output_loss: 0.7270 - bag_output_loss: 0.7177 - footwear_output_loss: 0.3984 - pose_output_loss: 0.1093 - emotion_output_loss: 0.7577 - gender_output_acc: 0.9684 - image_quality_output_acc: 0.8745 - age_output_acc: 0.5523 - weight_output_acc: 0.7052 - bag_output_acc: 0.6854 - footwear_output_acc: 0.8213 - pose_output_acc: 0.9597 - emotion_output_acc: 0.7260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 77/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 4.3039 - gender_output_loss: 0.0814 - image_quality_output_loss: 0.3069 - age_output_loss: 1.0326 - weight_output_loss: 0.7270 - bag_output_loss: 0.7178 - footwear_output_loss: 0.3986 - pose_output_loss: 0.1091 - emotion_output_loss: 0.7575 - gender_output_acc: 0.9685 - image_quality_output_acc: 0.8740 - age_output_acc: 0.5522 - weight_output_acc: 0.7053 - bag_output_acc: 0.6853 - footwear_output_acc: 0.8212 - pose_output_acc: 0.9598 - emotion_output_acc: 0.7260 - val_loss: 11.2761 - val_gender_output_loss: 1.2515 - val_image_quality_output_loss: 2.2120 - val_age_output_loss: 1.7064 - val_weight_output_loss: 1.1467 - val_bag_output_loss: 0.9415 - val_footwear_output_loss: 1.3708 - val_pose_output_loss: 1.4830 - val_emotion_output_loss: 0.9909 - val_gender_output_acc: 0.7253 - val_image_quality_output_acc: 0.4899 - val_age_output_acc: 0.3463 - val_weight_output_acc: 0.5439 - val_bag_output_acc: 0.5343 - val_footwear_output_acc: 0.5771 - val_pose_output_acc: 0.7253 - val_emotion_output_acc: 0.6492\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 7.02715\n",
            "Epoch 78/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.0776 - gender_output_loss: 0.0668 - image_quality_output_loss: 0.2334 - age_output_loss: 0.9929 - weight_output_loss: 0.7097 - bag_output_loss: 0.7020 - footwear_output_loss: 0.3798 - pose_output_loss: 0.0741 - emotion_output_loss: 0.7457 - gender_output_acc: 0.9757 - image_quality_output_acc: 0.9105 - age_output_acc: 0.5687 - weight_output_acc: 0.7125 - bag_output_acc: 0.6899 - footwear_output_acc: 0.8269 - pose_output_acc: 0.9749 - emotion_output_acc: 0.7282\n",
            "Epoch 00077: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.0754 - gender_output_loss: 0.0668 - image_quality_output_loss: 0.2332 - age_output_loss: 0.9924 - weight_output_loss: 0.7090 - bag_output_loss: 0.7023 - footwear_output_loss: 0.3797 - pose_output_loss: 0.0739 - emotion_output_loss: 0.7449 - gender_output_acc: 0.9757 - image_quality_output_acc: 0.9106 - age_output_acc: 0.5689 - weight_output_acc: 0.7127 - bag_output_acc: 0.6897 - footwear_output_acc: 0.8269 - pose_output_acc: 0.9750 - emotion_output_acc: 0.7286 - val_loss: 11.3424 - val_gender_output_loss: 1.0435 - val_image_quality_output_loss: 1.9778 - val_age_output_loss: 2.0199 - val_weight_output_loss: 1.2921 - val_bag_output_loss: 0.9705 - val_footwear_output_loss: 1.4736 - val_pose_output_loss: 1.3548 - val_emotion_output_loss: 1.0369 - val_gender_output_acc: 0.7465 - val_image_quality_output_acc: 0.5131 - val_age_output_acc: 0.3422 - val_weight_output_acc: 0.6154 - val_bag_output_acc: 0.5186 - val_footwear_output_acc: 0.5791 - val_pose_output_acc: 0.7177 - val_emotion_output_acc: 0.6689\n",
            "Epoch 78/96\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 7.02715\n",
            "Epoch 79/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.9019 - gender_output_loss: 0.0563 - image_quality_output_loss: 0.1850 - age_output_loss: 0.9511 - weight_output_loss: 0.6861 - bag_output_loss: 0.6934 - footwear_output_loss: 0.3625 - pose_output_loss: 0.0598 - emotion_output_loss: 0.7348 - gender_output_acc: 0.9788 - image_quality_output_acc: 0.9352 - age_output_acc: 0.5880 - weight_output_acc: 0.7214 - bag_output_acc: 0.6948 - footwear_output_acc: 0.8367 - pose_output_acc: 0.9796 - emotion_output_acc: 0.7314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00078: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 3.9020 - gender_output_loss: 0.0561 - image_quality_output_loss: 0.1847 - age_output_loss: 0.9511 - weight_output_loss: 0.6859 - bag_output_loss: 0.6933 - footwear_output_loss: 0.3625 - pose_output_loss: 0.0600 - emotion_output_loss: 0.7353 - gender_output_acc: 0.9789 - image_quality_output_acc: 0.9354 - age_output_acc: 0.5881 - weight_output_acc: 0.7216 - bag_output_acc: 0.6945 - footwear_output_acc: 0.8365 - pose_output_acc: 0.9794 - emotion_output_acc: 0.7311 - val_loss: 10.8618 - val_gender_output_loss: 0.9295 - val_image_quality_output_loss: 2.0934 - val_age_output_loss: 1.8366 - val_weight_output_loss: 1.2092 - val_bag_output_loss: 0.9517 - val_footwear_output_loss: 1.4989 - val_pose_output_loss: 1.1776 - val_emotion_output_loss: 0.9921 - val_gender_output_acc: 0.7823 - val_image_quality_output_acc: 0.5121 - val_age_output_acc: 0.3201 - val_weight_output_acc: 0.5272 - val_bag_output_acc: 0.6043 - val_footwear_output_acc: 0.5932 - val_pose_output_acc: 0.7132 - val_emotion_output_acc: 0.6689\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 7.02715\n",
            "Epoch 80/96\n",
            "360/360 [==============================] - 90s 249ms/step - loss: 3.7266 - gender_output_loss: 0.0394 - image_quality_output_loss: 0.1308 - age_output_loss: 0.9177 - weight_output_loss: 0.6652 - bag_output_loss: 0.6835 - footwear_output_loss: 0.3454 - pose_output_loss: 0.0454 - emotion_output_loss: 0.7265 - gender_output_acc: 0.9856 - image_quality_output_acc: 0.9588 - age_output_acc: 0.6036 - weight_output_acc: 0.7311 - bag_output_acc: 0.7011 - footwear_output_acc: 0.8418 - pose_output_acc: 0.9854 - emotion_output_acc: 0.7338 - val_loss: 10.9315 - val_gender_output_loss: 0.9731 - val_image_quality_output_loss: 2.2106 - val_age_output_loss: 1.8571 - val_weight_output_loss: 1.2045 - val_bag_output_loss: 0.9371 - val_footwear_output_loss: 1.3593 - val_pose_output_loss: 1.1932 - val_emotion_output_loss: 1.0238 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5121 - val_age_output_acc: 0.3427 - val_weight_output_acc: 0.5801 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6053 - val_pose_output_acc: 0.7182 - val_emotion_output_acc: 0.6719\n",
            "\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 7.02715\n",
            "Epoch 81/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.6826 - gender_output_loss: 0.0343 - image_quality_output_loss: 0.1300 - age_output_loss: 0.9003 - weight_output_loss: 0.6642 - bag_output_loss: 0.6803 - footwear_output_loss: 0.3395 - pose_output_loss: 0.0414 - emotion_output_loss: 0.7199 - gender_output_acc: 0.9890 - image_quality_output_acc: 0.9601 - age_output_acc: 0.6132 - weight_output_acc: 0.7268 - bag_output_acc: 0.7028 - footwear_output_acc: 0.8474 - pose_output_acc: 0.9882 - emotion_output_acc: 0.7357\n",
            "Epoch 00080: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 3.6843 - gender_output_loss: 0.0343 - image_quality_output_loss: 0.1298 - age_output_loss: 0.9010 - weight_output_loss: 0.6645 - bag_output_loss: 0.6805 - footwear_output_loss: 0.3397 - pose_output_loss: 0.0414 - emotion_output_loss: 0.7205 - gender_output_acc: 0.9891 - image_quality_output_acc: 0.9602 - age_output_acc: 0.6129 - weight_output_acc: 0.7266 - bag_output_acc: 0.7027 - footwear_output_acc: 0.8471 - pose_output_acc: 0.9883 - emotion_output_acc: 0.7354 - val_loss: 11.2654 - val_gender_output_loss: 0.9720 - val_image_quality_output_loss: 2.1911 - val_age_output_loss: 1.8619 - val_weight_output_loss: 1.1987 - val_bag_output_loss: 0.9512 - val_footwear_output_loss: 1.3702 - val_pose_output_loss: 1.5122 - val_emotion_output_loss: 1.0356 - val_gender_output_acc: 0.7712 - val_image_quality_output_acc: 0.5116 - val_age_output_acc: 0.3448 - val_weight_output_acc: 0.5948 - val_bag_output_acc: 0.5993 - val_footwear_output_acc: 0.6018 - val_pose_output_acc: 0.6593 - val_emotion_output_acc: 0.6668\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 7.02715\n",
            "Epoch 82/96\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 3.7557 - gender_output_loss: 0.0467 - image_quality_output_loss: 0.1490 - age_output_loss: 0.9189 - weight_output_loss: 0.6679 - bag_output_loss: 0.6812 - footwear_output_loss: 0.3498 - pose_output_loss: 0.0473 - emotion_output_loss: 0.7224 - gender_output_acc: 0.9836 - image_quality_output_acc: 0.9470 - age_output_acc: 0.6042 - weight_output_acc: 0.7260 - bag_output_acc: 0.7038 - footwear_output_acc: 0.8411 - pose_output_acc: 0.9848 - emotion_output_acc: 0.7349 - val_loss: 12.0661 - val_gender_output_loss: 1.1738 - val_image_quality_output_loss: 2.9540 - val_age_output_loss: 1.8425 - val_weight_output_loss: 1.1852 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 1.4287 - val_pose_output_loss: 1.2673 - val_emotion_output_loss: 1.1156 - val_gender_output_acc: 0.7384 - val_image_quality_output_acc: 0.4410 - val_age_output_acc: 0.3458 - val_weight_output_acc: 0.5534 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5806 - val_pose_output_acc: 0.6794 - val_emotion_output_acc: 0.6809\n",
            "Epoch 82/96\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 7.02715\n",
            "Epoch 83/96\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 3.8627 - gender_output_loss: 0.0601 - image_quality_output_loss: 0.1773 - age_output_loss: 0.9346 - weight_output_loss: 0.6745 - bag_output_loss: 0.6844 - footwear_output_loss: 0.3645 - pose_output_loss: 0.0651 - emotion_output_loss: 0.7298 - gender_output_acc: 0.9768 - image_quality_output_acc: 0.9314 - age_output_acc: 0.6009 - weight_output_acc: 0.7248 - bag_output_acc: 0.6990 - footwear_output_acc: 0.8332 - pose_output_acc: 0.9775 - emotion_output_acc: 0.7326 - val_loss: 12.3581 - val_gender_output_loss: 0.9495 - val_image_quality_output_loss: 2.9203 - val_age_output_loss: 1.8592 - val_weight_output_loss: 1.1748 - val_bag_output_loss: 0.9638 - val_footwear_output_loss: 1.4276 - val_pose_output_loss: 1.9018 - val_emotion_output_loss: 0.9887 - val_gender_output_acc: 0.7823 - val_image_quality_output_acc: 0.4617 - val_age_output_acc: 0.3327 - val_weight_output_acc: 0.5549 - val_bag_output_acc: 0.6023 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6366 - val_emotion_output_acc: 0.6557\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 7.02715\n",
            "Epoch 84/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 3.9982 - gender_output_loss: 0.0762 - image_quality_output_loss: 0.2214 - age_output_loss: 0.9508 - weight_output_loss: 0.6906 - bag_output_loss: 0.6954 - footwear_output_loss: 0.3794 - pose_output_loss: 0.0801 - emotion_output_loss: 0.7316 - gender_output_acc: 0.9703 - image_quality_output_acc: 0.9155 - age_output_acc: 0.5885 - weight_output_acc: 0.7203 - bag_output_acc: 0.6931 - footwear_output_acc: 0.8274 - pose_output_acc: 0.9732 - emotion_output_acc: 0.7317 - val_loss: 13.8362 - val_gender_output_loss: 1.4329 - val_image_quality_output_loss: 2.1296 - val_age_output_loss: 2.1681 - val_weight_output_loss: 1.3026 - val_bag_output_loss: 1.4333 - val_footwear_output_loss: 1.3961 - val_pose_output_loss: 2.7328 - val_emotion_output_loss: 1.0676 - val_gender_output_acc: 0.7389 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.2767 - val_weight_output_acc: 0.4733 - val_bag_output_acc: 0.5635 - val_footwear_output_acc: 0.6099 - val_pose_output_acc: 0.5247 - val_emotion_output_acc: 0.5660\n",
            "\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 7.02715\n",
            "Epoch 85/96\n",
            "360/360 [==============================] - 88s 245ms/step - loss: 4.1815 - gender_output_loss: 0.0963 - image_quality_output_loss: 0.2695 - age_output_loss: 0.9835 - weight_output_loss: 0.7153 - bag_output_loss: 0.6997 - footwear_output_loss: 0.3964 - pose_output_loss: 0.1056 - emotion_output_loss: 0.7412 - gender_output_acc: 0.9645 - image_quality_output_acc: 0.8908 - age_output_acc: 0.5711 - weight_output_acc: 0.7102 - bag_output_acc: 0.6955 - footwear_output_acc: 0.8206 - pose_output_acc: 0.9606 - emotion_output_acc: 0.7298 - val_loss: 15.1361 - val_gender_output_loss: 1.3253 - val_image_quality_output_loss: 4.7012 - val_age_output_loss: 1.7122 - val_weight_output_loss: 1.1512 - val_bag_output_loss: 0.9025 - val_footwear_output_loss: 1.9860 - val_pose_output_loss: 2.1696 - val_emotion_output_loss: 1.0133 - val_gender_output_acc: 0.6784 - val_image_quality_output_acc: 0.3130 - val_age_output_acc: 0.3352 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5791 - val_footwear_output_acc: 0.4763 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 7.02715Epoch 86/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.4215 - gender_output_loss: 0.1168 - image_quality_output_loss: 0.3394 - age_output_loss: 1.0132 - weight_output_loss: 0.7322 - bag_output_loss: 0.7135 - footwear_output_loss: 0.4346 - pose_output_loss: 0.1468 - emotion_output_loss: 0.7489 - gender_output_acc: 0.9535 - image_quality_output_acc: 0.8616 - age_output_acc: 0.5642 - weight_output_acc: 0.7050 - bag_output_acc: 0.6931 - footwear_output_acc: 0.8091 - pose_output_acc: 0.9465 - emotion_output_acc: 0.7270\n",
            "Epoch 00085: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 4.4219 - gender_output_loss: 0.1171 - image_quality_output_loss: 0.3395 - age_output_loss: 1.0134 - weight_output_loss: 0.7318 - bag_output_loss: 0.7136 - footwear_output_loss: 0.4343 - pose_output_loss: 0.1468 - emotion_output_loss: 0.7494 - gender_output_acc: 0.9534 - image_quality_output_acc: 0.8615 - age_output_acc: 0.5645 - weight_output_acc: 0.7052 - bag_output_acc: 0.6931 - footwear_output_acc: 0.8092 - pose_output_acc: 0.9465 - emotion_output_acc: 0.7270 - val_loss: 13.9449 - val_gender_output_loss: 2.2055 - val_image_quality_output_loss: 2.5343 - val_age_output_loss: 2.3547 - val_weight_output_loss: 1.8932 - val_bag_output_loss: 0.9922 - val_footwear_output_loss: 1.2182 - val_pose_output_loss: 1.5657 - val_emotion_output_loss: 1.0035 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.4587 - val_age_output_acc: 0.2394 - val_weight_output_acc: 0.2394 - val_bag_output_acc: 0.4829 - val_footwear_output_acc: 0.5801 - val_pose_output_acc: 0.6502 - val_emotion_output_acc: 0.6169\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 7.02715\n",
            "Epoch 87/96\n",
            "360/360 [==============================] - 89s 248ms/step - loss: 4.5882 - gender_output_loss: 0.1223 - image_quality_output_loss: 0.3837 - age_output_loss: 1.0468 - weight_output_loss: 0.7523 - bag_output_loss: 0.7200 - footwear_output_loss: 0.4502 - pose_output_loss: 0.1712 - emotion_output_loss: 0.7624 - gender_output_acc: 0.9522 - image_quality_output_acc: 0.8420 - age_output_acc: 0.5425 - weight_output_acc: 0.6965 - bag_output_acc: 0.6866 - footwear_output_acc: 0.7964 - pose_output_acc: 0.9369 - emotion_output_acc: 0.7246 - val_loss: 11.8253 - val_gender_output_loss: 0.8414 - val_image_quality_output_loss: 1.9030 - val_age_output_loss: 2.1796 - val_weight_output_loss: 1.3137 - val_bag_output_loss: 1.0048 - val_footwear_output_loss: 1.7341 - val_pose_output_loss: 1.6619 - val_emotion_output_loss: 1.0053 - val_gender_output_acc: 0.7535 - val_image_quality_output_acc: 0.4985 - val_age_output_acc: 0.3221 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.5449 - val_pose_output_acc: 0.6678 - val_emotion_output_acc: 0.6356\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 7.02715\n",
            "Epoch 88/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 4.7898 - gender_output_loss: 0.1462 - image_quality_output_loss: 0.4272 - age_output_loss: 1.0738 - weight_output_loss: 0.7806 - bag_output_loss: 0.7338 - footwear_output_loss: 0.4706 - pose_output_loss: 0.2039 - emotion_output_loss: 0.7698 - gender_output_acc: 0.9438 - image_quality_output_acc: 0.8181 - age_output_acc: 0.5333 - weight_output_acc: 0.6842 - bag_output_acc: 0.6825 - footwear_output_acc: 0.7908 - pose_output_acc: 0.9243 - emotion_output_acc: 0.7216 - val_loss: 14.7999 - val_gender_output_loss: 2.6797 - val_image_quality_output_loss: 2.6988 - val_age_output_loss: 2.7535 - val_weight_output_loss: 1.4528 - val_bag_output_loss: 1.2788 - val_footwear_output_loss: 1.1853 - val_pose_output_loss: 1.5915 - val_emotion_output_loss: 0.9727 - val_gender_output_acc: 0.5988 - val_image_quality_output_acc: 0.3800 - val_age_output_acc: 0.1764 - val_weight_output_acc: 0.3362 - val_bag_output_acc: 0.5746 - val_footwear_output_acc: 0.5998 - val_pose_output_acc: 0.6663 - val_emotion_output_acc: 0.6799\n",
            "Epoch 88/96\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00087: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 7.02715\n",
            "Epoch 89/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 4.8390 - gender_output_loss: 0.1515 - image_quality_output_loss: 0.4306 - age_output_loss: 1.0859 - weight_output_loss: 0.7766 - bag_output_loss: 0.7380 - footwear_output_loss: 0.4814 - pose_output_loss: 0.2133 - emotion_output_loss: 0.7722 - gender_output_acc: 0.9391 - image_quality_output_acc: 0.8158 - age_output_acc: 0.5319 - weight_output_acc: 0.6859 - bag_output_acc: 0.6780 - footwear_output_acc: 0.7876 - pose_output_acc: 0.9165 - emotion_output_acc: 0.7246 - val_loss: 12.7430 - val_gender_output_loss: 1.2066 - val_image_quality_output_loss: 4.5240 - val_age_output_loss: 1.5962 - val_weight_output_loss: 1.0604 - val_bag_output_loss: 0.9014 - val_footwear_output_loss: 1.1501 - val_pose_output_loss: 1.1605 - val_emotion_output_loss: 0.9522 - val_gender_output_acc: 0.6603 - val_image_quality_output_acc: 0.2253 - val_age_output_acc: 0.3614 - val_weight_output_acc: 0.5771 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.5504 - val_pose_output_acc: 0.6628 - val_emotion_output_acc: 0.6809\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 7.02715\n",
            "Epoch 90/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.6204 - gender_output_loss: 0.1319 - image_quality_output_loss: 0.3796 - age_output_loss: 1.0506 - weight_output_loss: 0.7606 - bag_output_loss: 0.7265 - footwear_output_loss: 0.4507 - pose_output_loss: 0.1657 - emotion_output_loss: 0.7614 - gender_output_acc: 0.9481 - image_quality_output_acc: 0.8395 - age_output_acc: 0.5449 - weight_output_acc: 0.6943 - bag_output_acc: 0.6818 - footwear_output_acc: 0.8007 - pose_output_acc: 0.9394 - emotion_output_acc: 0.7235\n",
            "Epoch 00089: val_loss did not improve from 7.02715\n",
            "Epoch 90/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 4.6207 - gender_output_loss: 0.1317 - image_quality_output_loss: 0.3796 - age_output_loss: 1.0505 - weight_output_loss: 0.7609 - bag_output_loss: 0.7265 - footwear_output_loss: 0.4512 - pose_output_loss: 0.1657 - emotion_output_loss: 0.7612 - gender_output_acc: 0.9482 - image_quality_output_acc: 0.8396 - age_output_acc: 0.5448 - weight_output_acc: 0.6941 - bag_output_acc: 0.6818 - footwear_output_acc: 0.8007 - pose_output_acc: 0.9395 - emotion_output_acc: 0.7235 - val_loss: 13.1948 - val_gender_output_loss: 2.2732 - val_image_quality_output_loss: 2.7145 - val_age_output_loss: 1.6967 - val_weight_output_loss: 1.1448 - val_bag_output_loss: 0.9105 - val_footwear_output_loss: 1.8076 - val_pose_output_loss: 1.4195 - val_emotion_output_loss: 1.0331 - val_gender_output_acc: 0.5983 - val_image_quality_output_acc: 0.4496 - val_age_output_acc: 0.3438 - val_weight_output_acc: 0.5685 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5020 - val_pose_output_acc: 0.7208 - val_emotion_output_acc: 0.6043\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 7.02715\n",
            "Epoch 91/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 4.4045 - gender_output_loss: 0.1068 - image_quality_output_loss: 0.3302 - age_output_loss: 1.0016 - weight_output_loss: 0.7409 - bag_output_loss: 0.7177 - footwear_output_loss: 0.4287 - pose_output_loss: 0.1306 - emotion_output_loss: 0.7519 - gender_output_acc: 0.9589 - image_quality_output_acc: 0.8635 - age_output_acc: 0.5567 - weight_output_acc: 0.7016 - bag_output_acc: 0.6843 - footwear_output_acc: 0.8080 - pose_output_acc: 0.9516 - emotion_output_acc: 0.7293 - val_loss: 12.4780 - val_gender_output_loss: 1.6654 - val_image_quality_output_loss: 1.7934 - val_age_output_loss: 2.1021 - val_weight_output_loss: 1.2077 - val_bag_output_loss: 1.0997 - val_footwear_output_loss: 1.6795 - val_pose_output_loss: 1.7658 - val_emotion_output_loss: 0.9675 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5176 - val_age_output_acc: 0.2676 - val_weight_output_acc: 0.4662 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.5554 - val_pose_output_acc: 0.6754 - val_emotion_output_acc: 0.6588\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00091: val_loss did not improve from 7.02715\n",
            "Epoch 92/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.1594 - gender_output_loss: 0.0871 - image_quality_output_loss: 0.2528 - age_output_loss: 0.9570 - weight_output_loss: 0.7161 - bag_output_loss: 0.6982 - footwear_output_loss: 0.4043 - pose_output_loss: 0.1036 - emotion_output_loss: 0.7429 - gender_output_acc: 0.9672 - image_quality_output_acc: 0.9006 - age_output_acc: 0.5835 - weight_output_acc: 0.7104 - bag_output_acc: 0.6938 - footwear_output_acc: 0.8164 - pose_output_acc: 0.9604 - emotion_output_acc: 0.7302Epoch 92/96\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 7.02715\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 4.1595 - gender_output_loss: 0.0871 - image_quality_output_loss: 0.2526 - age_output_loss: 0.9568 - weight_output_loss: 0.7162 - bag_output_loss: 0.6981 - footwear_output_loss: 0.4045 - pose_output_loss: 0.1037 - emotion_output_loss: 0.7431 - gender_output_acc: 0.9672 - image_quality_output_acc: 0.9007 - age_output_acc: 0.5837 - weight_output_acc: 0.7104 - bag_output_acc: 0.6938 - footwear_output_acc: 0.8160 - pose_output_acc: 0.9604 - emotion_output_acc: 0.7299 - val_loss: 12.6226 - val_gender_output_loss: 2.0806 - val_image_quality_output_loss: 2.1051 - val_age_output_loss: 1.8428 - val_weight_output_loss: 1.2450 - val_bag_output_loss: 0.9660 - val_footwear_output_loss: 1.6005 - val_pose_output_loss: 1.4786 - val_emotion_output_loss: 1.1061 - val_gender_output_acc: 0.6164 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3473 - val_weight_output_acc: 0.4859 - val_bag_output_acc: 0.5212 - val_footwear_output_acc: 0.5544 - val_pose_output_acc: 0.6870 - val_emotion_output_acc: 0.6729\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 7.02715\n",
            "Epoch 93/96\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 3.9484 - gender_output_loss: 0.0784 - image_quality_output_loss: 0.1999 - age_output_loss: 0.9099 - weight_output_loss: 0.6993 - bag_output_loss: 0.6837 - footwear_output_loss: 0.3730 - pose_output_loss: 0.0782 - emotion_output_loss: 0.7280 - gender_output_acc: 0.9708 - image_quality_output_acc: 0.9255 - age_output_acc: 0.6030 - weight_output_acc: 0.7192 - bag_output_acc: 0.7059 - footwear_output_acc: 0.8279 - pose_output_acc: 0.9718 - emotion_output_acc: 0.7331 - val_loss: 13.8540 - val_gender_output_loss: 1.2329 - val_image_quality_output_loss: 3.0537 - val_age_output_loss: 2.2559 - val_weight_output_loss: 1.3079 - val_bag_output_loss: 1.0316 - val_footwear_output_loss: 1.5017 - val_pose_output_loss: 2.0580 - val_emotion_output_loss: 1.2142 - val_gender_output_acc: 0.7455 - val_image_quality_output_acc: 0.4506 - val_age_output_acc: 0.3337 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.5575 - val_footwear_output_acc: 0.6079 - val_pose_output_acc: 0.6835 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 7.02715\n",
            "Epoch 94/96\n",
            "360/360 [==============================] - 88s 246ms/step - loss: 3.7543 - gender_output_loss: 0.0519 - image_quality_output_loss: 0.1591 - age_output_loss: 0.8660 - weight_output_loss: 0.6808 - bag_output_loss: 0.6695 - footwear_output_loss: 0.3519 - pose_output_loss: 0.0656 - emotion_output_loss: 0.7112 - gender_output_acc: 0.9800 - image_quality_output_acc: 0.9422 - age_output_acc: 0.6281 - weight_output_acc: 0.7244 - bag_output_acc: 0.7122 - footwear_output_acc: 0.8369 - pose_output_acc: 0.9773 - emotion_output_acc: 0.7399 - val_loss: 12.6697 - val_gender_output_loss: 1.2019 - val_image_quality_output_loss: 2.2590 - val_age_output_loss: 2.3600 - val_weight_output_loss: 1.4840 - val_bag_output_loss: 1.1692 - val_footwear_output_loss: 1.5516 - val_pose_output_loss: 1.3955 - val_emotion_output_loss: 1.0505 - val_gender_output_acc: 0.7445 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.6028 - val_footwear_output_acc: 0.5953 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.6729\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 7.02715\n",
            "Epoch 95/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 3.5688 - gender_output_loss: 0.0471 - image_quality_output_loss: 0.1124 - age_output_loss: 0.8130 - weight_output_loss: 0.6604 - bag_output_loss: 0.6564 - footwear_output_loss: 0.3374 - pose_output_loss: 0.0466 - emotion_output_loss: 0.6976 - gender_output_acc: 0.9826 - image_quality_output_acc: 0.9625 - age_output_acc: 0.6564 - weight_output_acc: 0.7354 - bag_output_acc: 0.7184 - footwear_output_acc: 0.8436 - pose_output_acc: 0.9845 - emotion_output_acc: 0.7431 - val_loss: 13.6280 - val_gender_output_loss: 1.1956 - val_image_quality_output_loss: 3.1883 - val_age_output_loss: 2.5583 - val_weight_output_loss: 1.4823 - val_bag_output_loss: 1.0674 - val_footwear_output_loss: 1.4931 - val_pose_output_loss: 1.3753 - val_emotion_output_loss: 1.0699 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.4788 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6018 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 7.02715\n",
            "Epoch 96/96\n",
            "360/360 [==============================] - 89s 247ms/step - loss: 3.5688 - gender_output_loss: 0.0471 - image_quality_output_loss: 0.1124 - age_output_loss: 0.8130 - weight_output_loss: 0.6604 - bag_output_loss: 0.6564 - footwear_output_loss: 0.3374 - pose_output_loss: 0.0466 - emotion_output_loss: 0.6976 - gender_output_acc: 0.9826 - image_quality_output_acc: 0.9625 - age_output_acc: 0.6564 - weight_output_acc: 0.7354 - bag_output_acc: 0.7184 - footwear_output_acc: 0.8436 - pose_output_acc: 0.9845 - emotion_output_acc: 0.7431 - val_loss: 13.6280 - val_gender_output_loss: 1.1956 - val_image_quality_output_loss: 3.1883 - val_age_output_loss: 2.5583 - val_weight_output_loss: 1.4823 - val_bag_output_loss: 1.0674 - val_footwear_output_loss: 1.4931 - val_pose_output_loss: 1.3753 - val_emotion_output_loss: 1.0699 - val_gender_output_acc: 0.7555 - val_image_quality_output_acc: 0.4788 - val_age_output_acc: 0.3231 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6018 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.6401\n",
            "360/360 [==============================] - 89s 246ms/step - loss: 3.4115 - gender_output_loss: 0.0371 - image_quality_output_loss: 0.0811 - age_output_loss: 0.7827 - weight_output_loss: 0.6409 - bag_output_loss: 0.6408 - footwear_output_loss: 0.3182 - pose_output_loss: 0.0334 - emotion_output_loss: 0.6797 - gender_output_acc: 0.9866 - image_quality_output_acc: 0.9765 - age_output_acc: 0.6722 - weight_output_acc: 0.7349 - bag_output_acc: 0.7286 - footwear_output_acc: 0.8516 - pose_output_acc: 0.9891 - emotion_output_acc: 0.7495 - val_loss: 12.1935 - val_gender_output_loss: 1.0548 - val_image_quality_output_loss: 2.5179 - val_age_output_loss: 2.2151 - val_weight_output_loss: 1.2900 - val_bag_output_loss: 0.9815 - val_footwear_output_loss: 1.4797 - val_pose_output_loss: 1.3943 - val_emotion_output_loss: 1.0626 - val_gender_output_acc: 0.7676 - val_image_quality_output_acc: 0.5045 - val_age_output_acc: 0.3402 - val_weight_output_acc: 0.5938 - val_bag_output_acc: 0.5736 - val_footwear_output_acc: 0.6084 - val_pose_output_acc: 0.7107 - val_emotion_output_acc: 0.6578\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 7.02715\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 7.02715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc43f2c470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoVUeE3AKqSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model):\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    accuracies = {}\n",
        "    losses = {}\n",
        "    for k, v in zip(model.metrics_names, results):\n",
        "        if k.endswith('acc'):\n",
        "            accuracies[k] = round(v * 100, 4) \n",
        "        else:\n",
        "            losses[k] = v\n",
        "    return accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG8MIK5BzptP",
        "colab_type": "code",
        "outputId": "348da30a-2e13-4681-f2e7-4a7241da4d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "results=model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names,results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 5s 151ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3402217741935484,\n",
              " 'age_output_loss': 2.2151069218112576,\n",
              " 'bag_output_acc': 0.5735887096774194,\n",
              " 'bag_output_loss': 0.981515351803072,\n",
              " 'emotion_output_acc': 0.6577620967741935,\n",
              " 'emotion_output_loss': 1.0625840348582114,\n",
              " 'footwear_output_acc': 0.608366935483871,\n",
              " 'footwear_output_loss': 1.4797117787022744,\n",
              " 'gender_output_acc': 0.7676411290322581,\n",
              " 'gender_output_loss': 1.0548176400123104,\n",
              " 'image_quality_output_acc': 0.5045362903225806,\n",
              " 'image_quality_output_loss': 2.5178961753845215,\n",
              " 'loss': 12.193536943004977,\n",
              " 'pose_output_acc': 0.7106854838709677,\n",
              " 'pose_output_loss': 1.3942807009143214,\n",
              " 'weight_output_acc': 0.59375,\n",
              " 'weight_output_loss': 1.2900181355014924}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9WZKwojEM5",
        "colab_type": "code",
        "outputId": "0778dff2-9c02-420b-fe85-889f66dcdeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/bestmodel7.h5')\n",
        "results=model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names,results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 5s 150ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3780241935483871,\n",
              " 'age_output_loss': 1.4160407435509466,\n",
              " 'bag_output_acc': 0.5977822580645161,\n",
              " 'bag_output_loss': 0.8821480312655049,\n",
              " 'emotion_output_acc': 0.6844758064516129,\n",
              " 'emotion_output_loss': 0.938050635399357,\n",
              " 'footwear_output_acc': 0.6476814516129032,\n",
              " 'footwear_output_loss': 0.8289902748600129,\n",
              " 'gender_output_acc': 0.8059475806451613,\n",
              " 'gender_output_loss': 0.4421830744512619,\n",
              " 'image_quality_output_acc': 0.5811491935483871,\n",
              " 'image_quality_output_loss': 0.8773847895283853,\n",
              " 'loss': 7.027149154293921,\n",
              " 'pose_output_acc': 0.7505040322580645,\n",
              " 'pose_output_loss': 0.6104686894724446,\n",
              " 'weight_output_acc': 0.6381048387096774,\n",
              " 'weight_output_loss': 0.9623317526232812}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}
