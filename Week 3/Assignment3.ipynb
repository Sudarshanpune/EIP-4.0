from keras import backend as K
import time
import matplotlib.pyplot as plt
import numpy as np
% matplotlib inline
np.random.seed(2017) 
from keras.models import Sequential
from keras.layers.convolutional import Convolution2D, MaxPooling2D,SeparableConv2D, AveragePooling2D,DepthwiseConv2D
from keras.layers import Activation, Flatten, Dense, Dropout
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from keras.layers import LeakyReLU, GlobalAveragePooling2D
Using TensorFlow backend.
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.
We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.

In [0]:
from keras.datasets import cifar10
(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()
num_train, img_channels, img_rows, img_cols =  train_features.shape
num_test, _, _, _ =  test_features.shape
num_classes = len(np.unique(train_labels))
In [3]:
class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']
fig = plt.figure(figsize=(8,3))
for i in range(num_classes):
    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])
    idx = np.where(train_labels[:]==i)[0]
    features_idx = train_features[idx,::]
    img_num = np.random.randint(features_idx.shape[0])
    im = features_idx[img_num]
    ax.set_title(class_names[i])
    plt.imshow(im)
plt.show()

In [0]:
def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # summarize history for accuracy
    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])
    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)
    axs[0].legend(['train', 'val'], loc='best')
    # summarize history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    plt.show()
In [0]:
def accuracy(test_x, test_y, model):
    result = model.predict(test_x)
    predicted_class = np.argmax(result, axis=1)
    true_class = np.argmax(test_y, axis=1)
    num_correct = np.sum(predicted_class == true_class) 
    accuracy = float(num_correct)/result.shape[0]
    return (accuracy * 100)
In [0]:
train_features = train_features.astype('float32')/255
test_features = test_features.astype('float32')/255
# convert class labels to binary class labels
train_labels = np_utils.to_categorical(train_labels, num_classes)
test_labels = np_utils.to_categorical(test_labels, num_classes)
In [0]:
# Define the model
model = Sequential()
model.add(Convolution2D(48, 3, 3, border_mode='same', input_shape=(32, 32, 3)))
model.add(Activation('relu'))
model.add(Convolution2D(48, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(96, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(96, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(192, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(192, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), input_shape=(32, 32, 3..., padding="same")`
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3))`
  after removing the cwd from sys.path.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), padding="same")`
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3))`
  # Remove the CWD from sys.path while we load stuff.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), padding="same")`
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3))`
  app.launch_new_instance()
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

In [0]:
model.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 48)        1344      
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 48)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 30, 30, 48)        20784     
_________________________________________________________________
activation_2 (Activation)    (None, 30, 30, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 15, 15, 48)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 15, 15, 48)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 15, 15, 96)        41568     
_________________________________________________________________
activation_3 (Activation)    (None, 15, 15, 96)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 13, 13, 96)        83040     
_________________________________________________________________
activation_4 (Activation)    (None, 13, 13, 96)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 6, 6, 96)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 6, 6, 96)          0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 6, 6, 192)         166080    
_________________________________________________________________
activation_5 (Activation)    (None, 6, 6, 192)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 4, 4, 192)         331968    
_________________________________________________________________
activation_6 (Activation)    (None, 4, 4, 192)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 2, 192)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 2, 2, 192)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 768)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               393728    
_________________________________________________________________
activation_7 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 256)               131328    
_________________________________________________________________
activation_8 (Activation)    (None, 256)               0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                2570      
=================================================================
Total params: 1,172,410
Trainable params: 1,172,410
Non-trainable params: 0
_________________________________________________________________
In [0]:
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(zoom_range=0.0, 
                             horizontal_flip=False)


# train the model
start = time.time()
# Train the model
model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
                                 samples_per_epoch = train_features.shape[0], nb_epoch = 50, 
                                 validation_data = (test_features, test_labels), verbose=1)
end = time.time()
print ("Model took %0.2f seconds to train"%(end - start))
# plot model history
plot_model_history(model_info)
# compute test accuracy
print ("Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model))
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.
  if sys.path[0] == '':
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`
  if sys.path[0] == '':
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

Epoch 1/50
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

390/390 [==============================] - 29s 73ms/step - loss: 1.9062 - acc: 0.2542 - val_loss: 1.5915 - val_acc: 0.4022
Epoch 2/50
390/390 [==============================] - 20s 52ms/step - loss: 1.4447 - acc: 0.4701 - val_loss: 1.2901 - val_acc: 0.5244
Epoch 3/50
390/390 [==============================] - 21s 53ms/step - loss: 1.1623 - acc: 0.5875 - val_loss: 0.9837 - val_acc: 0.6510
Epoch 4/50
390/390 [==============================] - 20s 52ms/step - loss: 1.0037 - acc: 0.6485 - val_loss: 0.9160 - val_acc: 0.6843
Epoch 5/50
390/390 [==============================] - 20s 52ms/step - loss: 0.8959 - acc: 0.6922 - val_loss: 0.7716 - val_acc: 0.7317
Epoch 6/50
390/390 [==============================] - 20s 51ms/step - loss: 0.8202 - acc: 0.7192 - val_loss: 0.7513 - val_acc: 0.7410
Epoch 7/50
390/390 [==============================] - 20s 52ms/step - loss: 0.7757 - acc: 0.7350 - val_loss: 0.7213 - val_acc: 0.7551
Epoch 8/50
390/390 [==============================] - 20s 52ms/step - loss: 0.7324 - acc: 0.7509 - val_loss: 0.6943 - val_acc: 0.7631
Epoch 9/50
390/390 [==============================] - 20s 51ms/step - loss: 0.6888 - acc: 0.7669 - val_loss: 0.6406 - val_acc: 0.7856
Epoch 10/50
390/390 [==============================] - 20s 51ms/step - loss: 0.6558 - acc: 0.7763 - val_loss: 0.6419 - val_acc: 0.7811
Epoch 11/50
390/390 [==============================] - 20s 52ms/step - loss: 0.6322 - acc: 0.7865 - val_loss: 0.6365 - val_acc: 0.7826
Epoch 12/50
390/390 [==============================] - 20s 52ms/step - loss: 0.6118 - acc: 0.7944 - val_loss: 0.6186 - val_acc: 0.7963
Epoch 13/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5902 - acc: 0.8010 - val_loss: 0.5970 - val_acc: 0.7998
Epoch 14/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5662 - acc: 0.8063 - val_loss: 0.6118 - val_acc: 0.7989
Epoch 15/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5557 - acc: 0.8128 - val_loss: 0.5936 - val_acc: 0.8040
Epoch 16/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5425 - acc: 0.8154 - val_loss: 0.5972 - val_acc: 0.8056
Epoch 17/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5269 - acc: 0.8225 - val_loss: 0.5862 - val_acc: 0.8008
Epoch 18/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5128 - acc: 0.8267 - val_loss: 0.5854 - val_acc: 0.8085
Epoch 19/50
390/390 [==============================] - 20s 52ms/step - loss: 0.5004 - acc: 0.8295 - val_loss: 0.5633 - val_acc: 0.8099
Epoch 20/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4907 - acc: 0.8325 - val_loss: 0.5885 - val_acc: 0.8101
Epoch 21/50
390/390 [==============================] - 21s 53ms/step - loss: 0.4797 - acc: 0.8366 - val_loss: 0.5760 - val_acc: 0.8123
Epoch 22/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4664 - acc: 0.8395 - val_loss: 0.5791 - val_acc: 0.8146
Epoch 23/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4581 - acc: 0.8458 - val_loss: 0.5873 - val_acc: 0.8140
Epoch 24/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4548 - acc: 0.8452 - val_loss: 0.5674 - val_acc: 0.8169
Epoch 25/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4421 - acc: 0.8498 - val_loss: 0.5707 - val_acc: 0.8149
Epoch 26/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4362 - acc: 0.8513 - val_loss: 0.5698 - val_acc: 0.8184
Epoch 27/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4334 - acc: 0.8542 - val_loss: 0.5781 - val_acc: 0.8155
Epoch 28/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4214 - acc: 0.8559 - val_loss: 0.5927 - val_acc: 0.8114
Epoch 29/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4193 - acc: 0.8583 - val_loss: 0.5652 - val_acc: 0.8221
Epoch 30/50
390/390 [==============================] - 20s 52ms/step - loss: 0.4187 - acc: 0.8590 - val_loss: 0.5720 - val_acc: 0.8194
Epoch 31/50
390/390 [==============================] - 20s 51ms/step - loss: 0.4091 - acc: 0.8615 - val_loss: 0.5636 - val_acc: 0.8204
Epoch 32/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3959 - acc: 0.8644 - val_loss: 0.5568 - val_acc: 0.8248
Epoch 33/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3894 - acc: 0.8674 - val_loss: 0.5781 - val_acc: 0.8208
Epoch 34/50
390/390 [==============================] - 20s 52ms/step - loss: 0.3980 - acc: 0.8657 - val_loss: 0.5683 - val_acc: 0.8239
Epoch 35/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3851 - acc: 0.8693 - val_loss: 0.5712 - val_acc: 0.8202
Epoch 36/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3834 - acc: 0.8687 - val_loss: 0.5444 - val_acc: 0.8286
Epoch 37/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3848 - acc: 0.8693 - val_loss: 0.5689 - val_acc: 0.8231
Epoch 38/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3810 - acc: 0.8711 - val_loss: 0.5699 - val_acc: 0.8247
Epoch 39/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3622 - acc: 0.8775 - val_loss: 0.5445 - val_acc: 0.8300
Epoch 40/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3687 - acc: 0.8749 - val_loss: 0.5759 - val_acc: 0.8216
Epoch 41/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3615 - acc: 0.8798 - val_loss: 0.5664 - val_acc: 0.8235
Epoch 42/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3669 - acc: 0.8774 - val_loss: 0.5581 - val_acc: 0.8278
Epoch 43/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3540 - acc: 0.8807 - val_loss: 0.5653 - val_acc: 0.8282
Epoch 44/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3468 - acc: 0.8820 - val_loss: 0.5445 - val_acc: 0.8363
Epoch 45/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3476 - acc: 0.8835 - val_loss: 0.5565 - val_acc: 0.8272
Epoch 46/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3505 - acc: 0.8809 - val_loss: 0.5762 - val_acc: 0.8200
Epoch 47/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3494 - acc: 0.8828 - val_loss: 0.5495 - val_acc: 0.8324
Epoch 48/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3450 - acc: 0.8842 - val_loss: 0.5463 - val_acc: 0.8365
Epoch 49/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3327 - acc: 0.8880 - val_loss: 0.5434 - val_acc: 0.8323
Epoch 50/50
390/390 [==============================] - 20s 51ms/step - loss: 0.3337 - acc: 0.8892 - val_loss: 0.5557 - val_acc: 0.8253
Model took 1013.59 seconds to train

Accuracy on test data is: 82.53
Assignment

In [7]:
model = Sequential()
model.add(SeparableConv2D(48, kernel_size=(3,3), input_shape=(32, 32, 3))) #30X30X48 #RF=3X3
model.add(BatchNormalization())
#model.add(Convolution2D(48, 3, 3, , input_shape=(32, 32, 3)))
model.add(Activation('relu'))

model.add(SeparableConv2D(48, kernel_size=(3,3))) #28X28X48 #RF=5X5
model.add(BatchNormalization())
#model.add(Convolution2D(48, 3, 3))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2))) #14X14X48 #RF=6X6
model.add(Dropout(0.25))

model.add(SeparableConv2D(96, kernel_size=(3,3), border_mode='same')) #14X14X96 #RF=10X10
model.add(BatchNormalization())
#model.add(Convolution2D(96, 3, 3, border_mode='same'))
model.add(Activation('relu'))

model.add(SeparableConv2D(96, kernel_size=(3,3))) #12X12X96 #RF=14X14
model.add(BatchNormalization())
#model.add(Convolution2D(96, 3, 3))
model.add(Activation('relu'))


model.add(MaxPooling2D(pool_size=(2, 2))) #6X6X96 #RF=16X16
model.add(Dropout(0.25))

model.add(SeparableConv2D(192, kernel_size=(3,3), border_mode='same')) #6X6X192 #RF=24X24
model.add(BatchNormalization())
#model.add(Convolution2D(192, 3, 3, border_mode='same'))
model.add(Activation('relu'))

model.add(SeparableConv2D(192, kernel_size=(3,3))) #4X4X192 #RF=32X32
model.add(BatchNormalization())
#model.add(Convolution2D(192, 3, 3))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2, 2))) #2X2X192 #RF=36X36
model.add(Dropout(0.25))

model.add(SeparableConv2D(num_classes,kernel_size=(2,2))) #1X1X10 #RF=44X44
model.add(Flatten())

model.add(Activation('softmax'))
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(96, kernel_size=(3, 3), padding="same")`
  from ipykernel import kernelapp as app
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(192, kernel_size=(3, 3), padding="same")`
In [8]:
model.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
separable_conv2d_1 (Separabl (None, 30, 30, 48)        219       
_________________________________________________________________
batch_normalization_1 (Batch (None, 30, 30, 48)        192       
_________________________________________________________________
activation_1 (Activation)    (None, 30, 30, 48)        0         
_________________________________________________________________
separable_conv2d_2 (Separabl (None, 28, 28, 48)        2784      
_________________________________________________________________
batch_normalization_2 (Batch (None, 28, 28, 48)        192       
_________________________________________________________________
activation_2 (Activation)    (None, 28, 28, 48)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 48)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 14, 14, 48)        0         
_________________________________________________________________
separable_conv2d_3 (Separabl (None, 14, 14, 96)        5136      
_________________________________________________________________
batch_normalization_3 (Batch (None, 14, 14, 96)        384       
_________________________________________________________________
activation_3 (Activation)    (None, 14, 14, 96)        0         
_________________________________________________________________
separable_conv2d_4 (Separabl (None, 12, 12, 96)        10176     
_________________________________________________________________
batch_normalization_4 (Batch (None, 12, 12, 96)        384       
_________________________________________________________________
activation_4 (Activation)    (None, 12, 12, 96)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 6, 6, 96)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 6, 6, 96)          0         
_________________________________________________________________
separable_conv2d_5 (Separabl (None, 6, 6, 192)         19488     
_________________________________________________________________
batch_normalization_5 (Batch (None, 6, 6, 192)         768       
_________________________________________________________________
activation_5 (Activation)    (None, 6, 6, 192)         0         
_________________________________________________________________
separable_conv2d_6 (Separabl (None, 4, 4, 192)         38784     
_________________________________________________________________
batch_normalization_6 (Batch (None, 4, 4, 192)         768       
_________________________________________________________________
activation_6 (Activation)    (None, 4, 4, 192)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 2, 192)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 2, 2, 192)         0         
_________________________________________________________________
separable_conv2d_7 (Separabl (None, 1, 1, 10)          2698      
_________________________________________________________________
flatten_1 (Flatten)          (None, 10)                0         
_________________________________________________________________
activation_7 (Activation)    (None, 10)                0         
=================================================================
Total params: 81,973
Trainable params: 80,629
Non-trainable params: 1,344
_________________________________________________________________
In [9]:
from keras.optimizers import RMSprop, Adam
model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

In [10]:
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(zoom_range=0.0,horizontal_flip=True)

# train the model
start = time.time()
# Train the model
model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
                                 samples_per_epoch = train_features.shape[0], nb_epoch = 50, 
                                 validation_data = (test_features, test_labels), verbose=1)
end = time.time()
print ("Model took %0.2f seconds to train"%(end - start))
# plot model history
plot_model_history(model_info)
# compute test accuracy
print ("Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model))
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.
  # Remove the CWD from sys.path while we load stuff.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`
  # Remove the CWD from sys.path while we load stuff.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

Epoch 1/50
390/390 [==============================] - 30s 77ms/step - loss: 1.4969 - acc: 0.4584 - val_loss: 1.3550 - val_acc: 0.5192
Epoch 2/50
390/390 [==============================] - 27s 68ms/step - loss: 1.1446 - acc: 0.5919 - val_loss: 1.0737 - val_acc: 0.6171
Epoch 3/50
390/390 [==============================] - 27s 69ms/step - loss: 1.0193 - acc: 0.6380 - val_loss: 0.9340 - val_acc: 0.6674
Epoch 4/50
390/390 [==============================] - 27s 68ms/step - loss: 0.9389 - acc: 0.6660 - val_loss: 1.0707 - val_acc: 0.6322
Epoch 5/50
390/390 [==============================] - 27s 68ms/step - loss: 0.8717 - acc: 0.6929 - val_loss: 0.9505 - val_acc: 0.6730
Epoch 6/50
390/390 [==============================] - 27s 69ms/step - loss: 0.8197 - acc: 0.7115 - val_loss: 0.8282 - val_acc: 0.7151
Epoch 7/50
390/390 [==============================] - 27s 69ms/step - loss: 0.7810 - acc: 0.7253 - val_loss: 0.8656 - val_acc: 0.7018
Epoch 8/50
390/390 [==============================] - 27s 69ms/step - loss: 0.7455 - acc: 0.7377 - val_loss: 0.7635 - val_acc: 0.7338
Epoch 9/50
390/390 [==============================] - 27s 69ms/step - loss: 0.7193 - acc: 0.7476 - val_loss: 0.6917 - val_acc: 0.7619
Epoch 10/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6950 - acc: 0.7552 - val_loss: 0.6756 - val_acc: 0.7650
Epoch 11/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6754 - acc: 0.7632 - val_loss: 0.7502 - val_acc: 0.7462
Epoch 12/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6566 - acc: 0.7702 - val_loss: 0.6869 - val_acc: 0.7586
Epoch 13/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6413 - acc: 0.7749 - val_loss: 0.6770 - val_acc: 0.7673
Epoch 14/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6285 - acc: 0.7779 - val_loss: 0.6898 - val_acc: 0.7621
Epoch 15/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6092 - acc: 0.7877 - val_loss: 0.6384 - val_acc: 0.7820
Epoch 16/50
390/390 [==============================] - 27s 69ms/step - loss: 0.6042 - acc: 0.7880 - val_loss: 0.6063 - val_acc: 0.7894
Epoch 17/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5857 - acc: 0.7942 - val_loss: 0.6491 - val_acc: 0.7798
Epoch 18/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5866 - acc: 0.7947 - val_loss: 0.6532 - val_acc: 0.7800
Epoch 19/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5653 - acc: 0.8026 - val_loss: 0.6244 - val_acc: 0.7882
Epoch 20/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5571 - acc: 0.8059 - val_loss: 0.6256 - val_acc: 0.7889
Epoch 21/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5466 - acc: 0.8084 - val_loss: 0.7123 - val_acc: 0.7629
Epoch 22/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5433 - acc: 0.8102 - val_loss: 0.5354 - val_acc: 0.8140
Epoch 23/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5337 - acc: 0.8120 - val_loss: 0.6248 - val_acc: 0.7889
Epoch 24/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5229 - acc: 0.8167 - val_loss: 0.6716 - val_acc: 0.7757
Epoch 25/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5186 - acc: 0.8170 - val_loss: 0.6699 - val_acc: 0.7712
Epoch 26/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5173 - acc: 0.8177 - val_loss: 0.5303 - val_acc: 0.8157
Epoch 27/50
390/390 [==============================] - 27s 69ms/step - loss: 0.5114 - acc: 0.8211 - val_loss: 0.5293 - val_acc: 0.8217
Epoch 28/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4995 - acc: 0.8243 - val_loss: 0.6009 - val_acc: 0.7966
Epoch 29/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4940 - acc: 0.8277 - val_loss: 0.5733 - val_acc: 0.8062
Epoch 30/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4925 - acc: 0.8263 - val_loss: 0.5542 - val_acc: 0.8134
Epoch 31/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4856 - acc: 0.8301 - val_loss: 0.5840 - val_acc: 0.8068
Epoch 32/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4776 - acc: 0.8339 - val_loss: 0.5462 - val_acc: 0.8127
Epoch 33/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4785 - acc: 0.8339 - val_loss: 0.5543 - val_acc: 0.8103
Epoch 34/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4710 - acc: 0.8334 - val_loss: 0.5275 - val_acc: 0.8230
Epoch 35/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4668 - acc: 0.8366 - val_loss: 0.5336 - val_acc: 0.8224
Epoch 36/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4666 - acc: 0.8377 - val_loss: 0.5190 - val_acc: 0.8244
Epoch 37/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4600 - acc: 0.8398 - val_loss: 0.5344 - val_acc: 0.8187
Epoch 38/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4551 - acc: 0.8407 - val_loss: 0.5338 - val_acc: 0.8213
Epoch 39/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4566 - acc: 0.8395 - val_loss: 0.5269 - val_acc: 0.8249
Epoch 40/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4477 - acc: 0.8435 - val_loss: 0.5432 - val_acc: 0.8198
Epoch 41/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4453 - acc: 0.8434 - val_loss: 0.5281 - val_acc: 0.8202
Epoch 42/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4420 - acc: 0.8439 - val_loss: 0.4743 - val_acc: 0.8390
Epoch 43/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4343 - acc: 0.8459 - val_loss: 0.5318 - val_acc: 0.8165
Epoch 44/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4349 - acc: 0.8476 - val_loss: 0.5344 - val_acc: 0.8253
Epoch 45/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4314 - acc: 0.8484 - val_loss: 0.5168 - val_acc: 0.8240
Epoch 46/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4261 - acc: 0.8488 - val_loss: 0.5871 - val_acc: 0.8065
Epoch 47/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4299 - acc: 0.8486 - val_loss: 0.4812 - val_acc: 0.8366
Epoch 48/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4214 - acc: 0.8529 - val_loss: 0.4879 - val_acc: 0.8390
Epoch 49/50
390/390 [==============================] - 27s 68ms/step - loss: 0.4221 - acc: 0.8517 - val_loss: 0.5315 - val_acc: 0.8265
Epoch 50/50
390/390 [==============================] - 27s 69ms/step - loss: 0.4187 - acc: 0.8529 - val_loss: 0.4905 - val_acc: 0.8330
Model took 1343.49 seconds to train

Accuracy on test data is: 83.30
